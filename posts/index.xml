<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on DAXPY</title><link>https://daxpy.xyz/posts/</link><description>Recent content in Posts on DAXPY</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 28 Apr 2024 21:30:00 +0530</lastBuildDate><atom:link href="https://daxpy.xyz/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>XGBoost</title><link>https://daxpy.xyz/posts/xgboost/</link><pubDate>Sun, 28 Apr 2024 21:30:00 +0530</pubDate><guid>https://daxpy.xyz/posts/xgboost/</guid><description>&lt;h1 id="xgboost"&gt;XGBoost&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Extreme Gradient Boosting&lt;/strong&gt; or XGBoost is a technique that has become quite useful for solving prediction problems. XGBoost is also quite interesting academically; for it combines quite few techniques together to give us one robust method. The technique is composed from gradient boosting, decision trees, matching pursuit and gradient descent in function space among others. In this post, we will explore and derive the inner workings of XGBoost.&lt;/p&gt;
&lt;h2 id="the-regression-problem"&gt;The Regression Problem&lt;/h2&gt;
&lt;p&gt;We are given a set of samples from population $\{(y_i,x_i)\}_{i=1}^{N}$ which we call a dataset. $y$&amp;rsquo;s are scalars and $x$ are vectors.&lt;/p&gt;</description></item><item><title>Sampling in a Sphere</title><link>https://daxpy.xyz/posts/sampling-in-a-sphere/</link><pubDate>Sun, 17 Mar 2024 21:44:15 +0530</pubDate><guid>https://daxpy.xyz/posts/sampling-in-a-sphere/</guid><description>&lt;h1 id="sampling-in-a-sphere"&gt;Sampling in a Sphere&lt;/h1&gt;
&lt;p&gt;Understanding how to generate a uniform sample of points inside a sphere takes us through a few interesting topics. So let begin with the end in mind.&lt;/p&gt;
&lt;p&gt;The following algorithm generates a uniform sample of points inside sphere in n dimensions.&lt;/p&gt;
&lt;p&gt;$$u_1, \ldots, u_{n+2} \sim \mathcal{N}(0,1)$$
$$x_1, \ldots, x_n = \frac{(u_1,\ldots, u_n)}{\sqrt{u_1^2+\ldots+u_{n+2}^{2}}}$$&lt;/p&gt;
&lt;p&gt;There is a lot to unpack here.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why are we sampling from a normal distribution to get a uniform distribution?&lt;/li&gt;
&lt;li&gt;Why $n+2$ ?&lt;/li&gt;
&lt;li&gt;Why are we dropping 2 coordinates?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="what-is-inside-a-sphere"&gt;What is inside a Sphere?&lt;/h2&gt;
&lt;p&gt;A sphere is defined as a set of points which are equidistant from a point known as center. For this article, we mostly deal with unit spheres. A unit n-sphere with center at origin can be defined as&lt;/p&gt;</description></item><item><title>Normalisation Layers</title><link>https://daxpy.xyz/posts/normalisation/</link><pubDate>Tue, 24 Oct 2023 18:33:10 +0530</pubDate><guid>https://daxpy.xyz/posts/normalisation/</guid><description>&lt;h1 id="normalisation"&gt;Normalisation&lt;/h1&gt;
&lt;p&gt;Regulating the magnitude of activations inside a neural network is crucial for an effective training regime. We may get stuck in local minima or worse yet, the training may diverge otherwise. For this, we make use of normalisation.&lt;/p&gt;
&lt;p&gt;Normalisation comes in two flavours. Weight normalisation and Layer normalisation. We briefly touch on some fundamental techniques in both.&lt;/p&gt;
&lt;h2 id="weight-normalisation"&gt;Weight normalisation&lt;/h2&gt;
&lt;p&gt;In weight normalisation, we focus on the magnitude of the parameters of the network; preventing them from uncontrollable growth or collapse. The basic technique appeared in &lt;a href="https://arxiv.org/pdf/1602.07868.pdf" title="Salimans, Tim and Kingma, Durk P &amp;quot;Weight normalization: A simple reparameterization to accelerate training of deep neural networks&amp;quot; In Advances in neural information processing systems 29, (2016)"&gt;Salimans et al. (2016)&lt;/a&gt; which describes a simple scenario.&lt;/p&gt;</description></item><item><title>Variational Inference</title><link>https://daxpy.xyz/posts/variational-inference/</link><pubDate>Sun, 16 Apr 2023 12:15:00 +0530</pubDate><guid>https://daxpy.xyz/posts/variational-inference/</guid><description>&lt;h1 id="variational-inference"&gt;Variational Inference&lt;/h1&gt;
&lt;p&gt;We have some data from a population and we suspect that the it is generated by some underlying process. Estimating the process which generates the data allows us to understand its fundamental properties.&lt;/p&gt;
&lt;p&gt;Concretely, $p(x)$ is the distribution of the data and $z$ are its latent variables, the process which generates the data is $p(x|z)$.
Estimating the generation process is computing the true posterior $p(z|x)$. This is the (posterior) inference problem.&lt;/p&gt;</description></item><item><title>Finding Median</title><link>https://daxpy.xyz/posts/finding-median/</link><pubDate>Tue, 01 Mar 2022 06:59:51 +0530</pubDate><guid>https://daxpy.xyz/posts/finding-median/</guid><description>&lt;h1 id="finding-median"&gt;Finding Median&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Here is my &lt;a href="https://www.dictionary.com/e/slang/eli5/"&gt;ELI5&lt;/a&gt; definition of a median.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Median is the middle number when numbers are sorted&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There is only a single middle number when the size list is odd. But if the size is even, there are 2 middle numbers. Then we take an average of those 2 numbers to be the median.&lt;/p&gt;
&lt;p&gt;Median is useful when your data doesn&amp;rsquo;t behave. Medians are part of &amp;ldquo;robust statistics&amp;rdquo; because they are not affected by outliers. Both $[1,2,100]$ and $[1,2,3]$ have 2 as their median while their means differ widely. You can see why medians are not affected by noise.&lt;/p&gt;</description></item><item><title>Binary Search</title><link>https://daxpy.xyz/posts/binary-search-problems/</link><pubDate>Fri, 28 Jan 2022 06:59:51 +0530</pubDate><guid>https://daxpy.xyz/posts/binary-search-problems/</guid><description>&lt;h1 id="binary-search"&gt;Binary Search&lt;/h1&gt;
&lt;p&gt;Binary search is more than just a search algorithm for sorted arrays. It&amp;rsquo;s an algorithm which keeps showing up as optimal solutions in unlikely places. This note is a very limited exploration of what binary search can do.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s begin by talking about vanilla binary search.&lt;/p&gt;
&lt;h2 id="binary-search-1"&gt;Binary Search&lt;/h2&gt;
&lt;p&gt;We are given a sorted array of numbers and a target. Binary search is the most optimal way of finding position of target in the array if present.&lt;/p&gt;</description></item><item><title>Differentiable Computations</title><link>https://daxpy.xyz/posts/differentiable-computations/</link><pubDate>Sun, 18 Feb 2018 00:00:00 +0000</pubDate><guid>https://daxpy.xyz/posts/differentiable-computations/</guid><description>&lt;h1 id="differentiable-computations"&gt;Differentiable Computations&lt;/h1&gt;
&lt;p&gt;Auto-grad or automatic gradient computation is a nice feature found in many computational frameworks. Specify the computation in forward direction and the framework computes backward gradients. Let&amp;rsquo;s talk about the generic method to do this.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say we have to compute the result of &amp;lsquo;something&amp;rsquo;. It may be a
nasty heat equation or some logic driven steps to get from input to
output. Abstracting the steps involved gives us a sequence of equations
$$\begin{aligned}
z_i = f_i(z_{a(i)})\end{aligned}$$&lt;/p&gt;</description></item></channel></rss>