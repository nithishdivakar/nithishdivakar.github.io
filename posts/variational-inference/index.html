<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><title>Variational Inference | DAXPY</title><link rel=stylesheet href=/css/main.min.9e0d91ee5e43768adbd3e495fb3273f760195e4e09cae07eb8461013ded5303d.css integrity="sha256-ng2R7l5Ddorb0+SV+zJz92AZXk4JyuB+uEYQE97VMD0=" crossorigin=anonymous><script src=/js/main.23cd0c7d837263b9eaeb96ee2d9ccfa2969daa3fa00fa1c1fe8701a9b87251a1.js integrity="sha256-I80MfYNyY7nq65buLZzPopadqj+gD6HB/ocBqbhyUaE=" crossorigin=anonymous></script></head><body><header><nav><ul><li><a class=text_secondary href=https://daxpy.xyz/>DAXPY</a></li><li>|</li><li><a class=text_secondary href=https://daxpy.xyz/posts/>posts</a></li><li><a class=text_secondary href=https://daxpy.xyz/notes/>notes</a></li><li><a class=text_secondary href=https://daxpy.xyz/links/>links</a></li><li><a class=text_secondary href=https://daxpy.xyz/stories/>stories</a></li><li><a class=text_secondary href=/about>about</a></li></ul></nav></header><main><section><article><h1 id=variational-inference>Variational Inference</h1><p>We have some data from a population and we suspect that the it is generated by some underlying process. Estimating the process which generates the data allows us to understand its fundamental properties.</p><p>Concretely, $p(x)$ is the distribution of the data and $z$ are its latent variables, the process which generates the data is $p(x|z)$.
Estimating the generation process is computing the true posterior $p(z|x)$. This is the (posterior) inference problem.</p><p>From Bayes rule, we have
$$p(z|x) = \frac{p(x,z)}{p(x)}$$
with $p(x) = \int_z p(x,z)$. But this is often intractable as the quantities on RHS are non-trivial to estimate.</p><p>Variational inference is the technique which helps in converting this estimation problem into an optimization problem by approximating the posterior $p(z|x)$ with a family of simpler distributions $q_v(z)$. The best approximation is found by minimizing the <a href=https://en.wikipedia.org/wiki/Divergence_(statistics) title=Divergence>divergence</a> between $q$ and $p$.</p><p>$$ \min_v D_{KL}[q_v(z), p(z|x)] $$</p><p>$q_v$ is a family of distribution and a specific member is selected by $v$. In this context, $v$&rsquo;s are called <em>variational parameters</em>.</p><p>There are many different <a href=https://en.wikipedia.org/wiki/Divergence_(statistics) title=Divergence>divergence</a> measures. But we are going to stick with <a href=https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence title="KL Divergence">KL divergence</a> as it has a straight forward definition. Note that it is asymmetric.</p><p>$$D_{KL}(p,q) = \int_{\Omega} p \log\frac{p}{q}$$</p><p>Although this sounds promising and simple in theory, in practise we run into some difficulties. Optimising <a href=https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence title="KL Divergence">KL divergence</a> is practically difficult. The gradient is readily available for simpler distributions and in practise $p$ can be very complex. This will force us to use very simple $q$ and have a bad approximations.</p><p>Let see if we can optimise KL divergence indirectly.</p><p>Consider this. $\log p(x)$ is called <em>evidence</em> or <em>log-evidence</em> of $x$ and is considered a constant. Since $\int_z q_v(z)=1$ we can write</p><p>$$\log p(x) = \log p(x) \int_z q_v(z)$$</p><p>Since $p(x)$ is invariant to the w.r.to $z$, we have,</p><p>$$\log p(x) = \int_z q_v(z) \log p(x)$$</p><p>A bit of clever substitutions and wrangling later,</p><p>$$ \log p(x) =\int_z q_v(z) \log \frac{p(x,z)}{q_v(z)} - \int_z q_v(z) \log \frac{p(z|x)}{q_v(z)}$$</p><p>Thus we have the following system.</p><p>$$\log p(x) = L(q,p) - D_{KL}(q,p)$$</p><p>$\log p(x)$ is constant and KL divergence is always $\geq 0$. The $L(q,p)$ is thus lower bound on the approximation between $p$ and $q$. It is hence called <em><a href=https://en.wikipedia.org/wiki/Evidence_lower_bound title="Evidence Lower Bound">Evidence Lower Bound</a></em> or ELBO.</p><p>Since a constant is equal to difference between 2 variables, maximising ELBO decreases the KL divergence making approximation of $q$ to $p$ better. Now we have an alternate way to find the best approximation.</p><h2 id=evidence-lower-bound>Evidence Lower Bound</h2><p>One easy way to understand effect of optimising evidence lower bound is to think of it as a combination of 2 entropies.
$$L(q,p) = H(q) - H(q,p)$$
While maximizing $L$, the entropy term is pushing $q$ to spread everywhere while the negative cross entropy term is pushing $q$ to concentrate on regions where $p$ has high density.
However, for using as a training objective, we need its gradient.</p><h2 id=black-box-variational-inference>Black Box Variational Inference</h2><p>ELBO has the following equivalent form.</p><p>$$L(q,p) = \mathbb{E}_{q_v(z)} \left [ \log p(x,z) - \log q_v(z) \right ]$$</p><p><a href=http://www.cs.columbia.edu/~blei/papers/RanganathGerrishBlei2014.pdf title='Rajesh Ranganath, Sean Gerrish, and David Blei. "Black box variational inference." Artificial intelligence and statistics. PMLR, 2014.'>Ranganath et al. (2014)</a> gives the following form of gradient.
$$\nabla_v L(v) = \mathbb{E}_{q_v(z)} \left [ \nabla_v \log q_v(z)\, (\log p(x,z) - \log q_v(z) )\right ]$$</p><p>Gradient of the log of probability distribution is called score function and this gradient is known as <strong>score gradient</strong>. When the gradient is in form of an expectation of a random variable, we take monte-carlo samples and to get <a href=https://en.wikipedia.org/wiki/Stochastic_approximation title="Stochastic Approximation">approximate gradient</a>.</p><p>Also note that the gradient assumes no knowledge of the model except that we can evaluate the quantities in the equation. More of less, a <em>Black Box Variational Inference</em>.</p><p>Unfortunately, the approximation of the gradient has high variance. Fortunately, <a href=http://www.cs.columbia.edu/~blei/papers/RanganathGerrishBlei2014.pdf title='Rajesh Ranganath, Sean Gerrish, and David Blei. "Black box variational inference." Artificial intelligence and statistics. PMLR, 2014.'>Ranganath et al. (2014)</a> also describes a few variance reduction techniques.</p><h2 id=reprameterisation-gradient>Reprameterisation gradient</h2><p>Score gradient allows us to use complex distributions to approximate posterior distribution. But it is difficult to sample from complex distributions. <a href=https://arxiv.org/abs/1312.6114 title='Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." arXiv preprint arXiv:1312.6114 (2013).'>Reparametrisation trick</a> allows us to create complex distributions from simple ones.</p><p>Say we are able to write $z = t(\epsilon, v)$ with
$\epsilon \sim s(\epsilon)$, a simple distribution which we can sample from.
What we have done is to bound all &ldquo;randomness&rdquo; to $s(\epsilon)$ and made $q_v(z)$ &ldquo;non-random&rdquo;.</p><p>With this, we have a simpler gradient for elbo.</p><p>$$\nabla_v L(v) = \mathbb{E}_{s(\epsilon)}\big[ \nabla_z \left[ \log p(x,z) - \log q_v(z) \right] \nabla_v t(\epsilon, v) \big]$$</p><p>To compute approximate gradient, take samples $\epsilon \sim s(\epsilon)$, compute $z = t(\epsilon, v)$ and then evaluate the reparameterization gradient.
$\log p(x,z) - \log q_v(z)$ is the model and $\nabla_z(\cdots)$ can be evaluated using auto-differentiation. See <a href=http://www.cs.columbia.edu/~blei/papers/RuizTitsiasBlei2016b.pdf title='Ruiz, Francisco R., Titsias RC AUEB, and David Blei. "The generalized reparameterization gradient." Advances in neural information processing systems 29 (2016).'>Ruiz et al. (2016)</a> for more discussion on reprameterisation gradient.</p><h2 id=amortised-variational-inference>Amortised Variational Inference</h2><p>Variational Inference is still not scalable. We still have to fit the variational parameter for each observations essentially minimising KL divergence between each $q_v(z)$ and $p(z|x_i)$. This is not scalable and will lead to over fitting.</p><p>We can get around this by constrianing the variational parameters to be a function of the observations; a learnable function.</p><p>$$v = g_\phi(x)$$</p><p>In <a href=https://www.tuananhle.co.uk/notes/amortized-inference.html title="Amortized Inference by Tuan Anh Le">Amortised Inference</a>, the variational parameters are output of a network which takes the samples from true distribution as input. The parameters of the network can be learned by optimising reparameterisation gradient.</p><p>$$\nabla_\phi L(\phi) = \mathbb{E}_{s(\epsilon)}\big[ \nabla_z \left[ \log p(x,z) - \log q_v(z) \right] \nabla_\phi t(\epsilon, g_\phi(x)) \big]$$</p><h2 id=variational-auto-encoders>Variational Auto Encoders</h2><p><a href=https://arxiv.org/abs/1312.6114 title='Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." arXiv preprint arXiv:1312.6114 (2013).'>Variational auto encoders</a> are a generative model which learns to generate data from its true distribution. It has an architecture simlar to a <a href=https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf title='Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P. A., & Bottou, L. (2010). "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion". Journal of machine learning research, 11(12).'>denoising auto encoder</a> and uses variational inference to learn the distribution.</p><p>Variational auto encoders make a constraint that the posterior is approximated by an gaussian distribution with diagonal covariances. As a result, the latent representation will have linearly independent dimensions.</p><p>The encoder is a differentiable network which is used to approximate posterior distribution $p(z|x)$. The network is trained to predict the parameters of the approximating distribution from a data point.</p><p>$$\mu, \sigma = g_{\phi}(x)$$</p><p>then, the posterior is approximated by the distribution
$$ q_{\mu, \sigma}(z)= \mathcal{N}(z; \mu, diag(\sigma^2))$$</p><p>The latent vector is obtained by sampling from $q_{\mu, \sigma}(z)$.</p><p>The decoder is also a differentiable network which is trained to predict a sample from the latent vector.
$$\hat{x} = f_{\theta}(z)$$</p><p>Different samples generate different predictions thus generating samples from $p(x|z)$. Some designs explicitly adds &ldquo;intelligent noise&rdquo; to aid in directed distribution. $\hat{x} = f_{\theta}(z+\epsilon)$</p><p>The forward inference of variational auto encoder defined in the <a href=https://arxiv.org/abs/1312.6114 title='Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." arXiv preprint arXiv:1312.6114 (2013).'>Kingma et al. (2013)</a> has the following form.</p><p>$$(\mu, \log \sigma) = g_\phi(x) $$
$$q_{\mu,\sigma}(z|x) = \mathcal{N} (z; \mu, diag(\sigma^2))$$
$$z \sim q_{\mu,\sigma}(z|x)$$
$$ \hat{x} = f_{\theta}(z)$$</p><p>Both the encoder and decoder are neural networks and $\phi$ and $\theta$ are their parameters. Both the networks are trained end to end to miniminse the following loss.
$$L
= C\|x - f(z) \|^2 + D_{KL}(\mathcal{N}(\mu, diag(\sigma^2), \mathcal{N}(0,I))$$</p><h2 id=references>References</h2><reference><small><ul><li><p><a href=https://arxiv.org/abs/1601.00670 title='Blei, David M., Alp Kucukelbir, and Jon D. McAuliffe. "Variational inference: A review for statisticians." Journal of the American statistical Association 112.518 (2017): 859-877.'>Blei et al. (2017)</a>: Blei, David M., Alp Kucukelbir, and Jon D. McAuliffe. &ldquo;<em>Variational inference: A review for statisticians.</em>&rdquo; Journal of the American statistical Association 112.518 (2017): 859-877.</p></li><li><p><a href=https://arxiv.org/abs/1312.6114 title='Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." arXiv preprint arXiv:1312.6114 (2013).'>Kingma et al. (2013)</a>: Kingma, Diederik P., and Max Welling. &ldquo;<em>Auto-encoding variational bayes.</em>&rdquo; arXiv preprint arXiv:1312.6114 (2013).</p></li><li><p><a href=https://arxiv.org/pdf/1906.02691.pdf title='Diederik P. Kingma and Max Welling (2019), "An Introduction to Variational Autoencoders", Foundations and Trends in Machine Learning: Vol. xx, No.xx, pp 1–18. DOI: 10.1561/XXXXXXXXX.'>Kingma et al. (2019)</a>: Diederik P. Kingma and Max Welling (2019), &ldquo;<em>An Introduction to Variational Autoencoders</em>&rdquo;, Foundations and Trends in Machine Learning: Vol. xx, No.xx, pp 1–18. DOI: 10.1561/XXXXXXXXX.</p></li><li><p><a href=http://www.cs.columbia.edu/~blei/papers/RanganathGerrishBlei2014.pdf title='Rajesh Ranganath, Sean Gerrish, and David Blei. "Black box variational inference." Artificial intelligence and statistics. PMLR, 2014.'>Ranganath et al. (2014)</a>: Rajesh Ranganath, Sean Gerrish, and David Blei. &ldquo;<em>Black box variational inference.</em>&rdquo; Artificial intelligence and statistics. PMLR, 2014.</p></li><li><p><a href=https://arxiv.org/pdf/1401.4082.pdf title='Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. "Stochastic backpropagation and approximate inference in deep generative models." International conference on machine learning. PMLR, 2014.'>Rezende et al. (2014)</a>: Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. &ldquo;<em>Stochastic backpropagation and approximate inference in deep generative models.</em>&rdquo; International conference on machine learning. PMLR, 2014.</p></li><li><p><a href=http://www.cs.columbia.edu/~blei/papers/RuizTitsiasBlei2016b.pdf title='Ruiz, Francisco R., Titsias RC AUEB, and David Blei. "The generalized reparameterization gradient." Advances in neural information processing systems 29 (2016).'>Ruiz et al. (2016)</a>: Ruiz, Francisco R., Titsias RC AUEB, and David Blei. &ldquo;<em>The generalized reparameterization gradient.</em>&rdquo; Advances in neural information processing systems 29 (2016).</p></li><li><p><a href=https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf title='Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P. A., & Bottou, L. (2010). "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion". Journal of machine learning research, 11(12).'>Vincent et al. (2010)</a>: Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P. A., & Bottou, L. (2010). &ldquo;<em>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</em>&rdquo;. Journal of machine learning research, 11(12).</p></li><li><p><em><a href=https://www.tuananhle.co.uk/notes/amortized-inference.html title="Amortized Inference by Tuan Anh Le">Amortized Inference</a></em> by Tuan Anh Le<br><small><em><code>https://www.tuananhle.co.uk/notes/amortized-inference.html</code></em></small></p></li><li><p><em><a href=https://erdogdu.github.io/csc412/notes/lec11-1.pdf title="Amortized Inference and Variational Auto Encoders">Amortized Inference and Variational Auto Encoders</a></em><br><small><em><code>https://erdogdu.github.io/csc412/notes/lec11-1.pdf</code></em></small></p></li><li><p><em><a href=https://en.wikipedia.org/wiki/Divergence_(statistics) title=Divergence>Divergence</a></em><br><small><em><code>https://en.wikipedia.org/wiki/Divergence_(statistics)</code></em></small></p></li><li><p><em><a href=https://en.wikipedia.org/wiki/Evidence_lower_bound title="Evidence Lower Bound">Evidence Lower Bound</a></em><br><small><em><code>https://en.wikipedia.org/wiki/Evidence_lower_bound</code></em></small></p></li><li><p><em><a href=https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence title="KL Divergence">KL Divergence</a></em><br><small><em><code>https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence</code></em></small></p></li><li><p><em><a href=https://en.wikipedia.org/wiki/Stochastic_approximation title="Stochastic Approximation">Stochastic Approximation</a></em><br><small><em><code>https://en.wikipedia.org/wiki/Stochastic_approximation</code></em></small></p></li><li><p><em><a href=https://ermongroup.github.io/cs228-notes/extras/vae title="The variational auto-encoder">The variational auto-encoder</a></em><br><small><em><code>https://ermongroup.github.io/cs228-notes/extras/vae</code></em></small></p></li><li><p><em><a href=https://www.depthfirstlearning.com/2021/VI-with-NFs title="Variational Inference with Normalizing Flows">Variational Inference with Normalizing Flows</a></em><br><small><em><code>https://www.depthfirstlearning.com/2021/VI-with-NFs</code></em></small></p></li><li><p><em><a href=https://ermongroup.github.io/cs228-notes/inference/variational title="Variational inference">Variational inference</a></em><br><small><em><code>https://ermongroup.github.io/cs228-notes/inference/variational</code></em></small></p></li><li><p><em><a href="https://www.youtube.com/watch?v=Dv86zdWjJKQ" title="Variational Inference: Foundations and Innovations by David Blei">Variational Inference: Foundations and Innovations by David Blei</a></em></p></li></ul></small></reference></article><br><br><br><i>tags:</i>
<a class=gray style=padding-right:5px href=/tags#machine-learning><i>#machine-learning</i></a>
<a class=gray style=padding-right:5px href=/tags#optimisation><i>#optimisation</i></a>
<a class=gray style=padding-right:5px href=/tags#probability><i>#probability</i></a></section></main><footer><p>...</p></footer></body></html>