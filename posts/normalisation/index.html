<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><title>Normalisation Layers | DAXPY</title><link rel=stylesheet href=/css/main.min.893c9340c957513598581780adb1395d937fd4885954ab6c67536bd4c77301a6.css integrity="sha256-iTyTQMlXUTWYWBeArbE5XZN/1IhZVKtsZ1Nr1MdzAaY=" crossorigin=anonymous><script src=/js/main.23cd0c7d837263b9eaeb96ee2d9ccfa2969daa3fa00fa1c1fe8701a9b87251a1.js integrity="sha256-I80MfYNyY7nq65buLZzPopadqj+gD6HB/ocBqbhyUaE=" crossorigin=anonymous></script></head><body><header><nav><ul><li><a class=text_secondary href=https://daxpy.xyz/>DAXPY</a></li><li>|</li><li><a href=https://daxpy.xyz/posts/>Posts</a></li><li><a href=https://daxpy.xyz/notes/>Notes</a></li><li><a href=https://daxpy.xyz/links/>Links</a></li><li><a href=https://daxpy.xyz/stories/>Stories</a></li><li><a href=/about>About</a></li></ul></nav></header><main><section><article><h1 id=normalisation>Normalisation</h1><p>Regulating the magnitude of activations inside a neural network is crucial for an effective training regime. We may get stuck in local minima or worse yet, the training may diverge otherwise. For this, we make use of normalisation.</p><p>Normalisation comes in two flavours. Weight normalisation and Layer normalisation. We briefly touch on some fundamental techniques in both.</p><h2 id=weight-normalisation>Weight normalisation</h2><p>In weight normalisation, we focus on the magnitude of the parameters of the network; preventing them from uncontrollable growth or collapse. The basic technique appeared in <a href=https://arxiv.org/pdf/1602.07868.pdf title='Salimans, Tim and Kingma, Durk P "Weight normalization: A simple reparameterization to accelerate training of deep neural networks" In Advances in neural information processing systems 29, (2016)'>Salimans et al. (2016)</a> which describes a simple scenario.</p><p>Given a network layer</p><p>$$y = \phi(w \cdot x+b)$$</p><p>We constrain the weight by reparameterising it as</p><p>$$w = g\frac{v}{\|v\|}$$</p><p>This constraints the norm of $w$ to be always $g$, the growth parameter.</p><h3 id=spectral-normalization>Spectral Normalization</h3><p>If we consider the parameter of the layer/network as matrix, then we can device normalisation schemes using matrix norms. One such technique is spectral norm. It was first described in <a href=https://arxiv.org/pdf/1802.05957.pdf title='Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi "Spectral normalization for generative adversarial networks" In arXiv preprint arXiv:1802.05957 , (2018)'>Miyato et al. (2018)</a>.</p><p>Given the weight matrix, we normalise it by its spectral norm</p><p>$$W \gets \frac{W}{\sigma(W)}$$</p><p><a href=https://en.wikipedia.org/wiki/Matrix_norm title="Matrix Norm">Spectral norm</a> is an induced vector norm defined as</p><p>$$\sigma(A) = max_{h \neq 0}\frac{\|Ah\|_ 2}{\|h\|_ 2}$$</p><p>To recall, this is the formulation of the largest singular value of $A$ and hence, spectral norm stabilizes the <a href=https://en.wikipedia.org/wiki/Lipschitz_continuity title="Lipschitz Continuity">Lipschitz constant</a> of the weight matrix.</p><p>Spectral norm had found its initial use in <a href=https://arxiv.org/pdf/1802.05957.pdf title='Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi "Spectral normalization for generative adversarial networks" In arXiv preprint arXiv:1802.05957 , (2018)'>Generative Adversarial Networks (GANs)</a> for stabilising training by preventing mode collapse.</p><h2 id=normalisation-layers>Normalisation Layers</h2><p>Normalisation layers are like any other layers in a neural network which takes applied transformations on its input. But their primary purpose is to stablise the activations. They are places in the computational graph such that the further layers recieve a more stable inputs due to their action.</p><h3 id=batch-normalisation>Batch Normalisation</h3><p>The key idea behind batch norm is to normalise each feature of a sample. Ideally, if $x$ is the input feature, then the best normalisation is achieved if we do
$$x_f = \frac{x_f - \mathbb{E}[x_f]}{\sqrt{\operatorname{Var}[x_f]}}$$</p><p>But we don&rsquo;t have access to the exact population mean and variance. So we approximate it at run time with the mean and variance of the features in the mini-batch. Hence the name &ldquo;batch norm&rdquo;.</p><p>Batch norm was introduced by <a href=http://proceedings.mlr.press/v37/ioffe15.pdf title='Ioffe, Sergey and Szegedy, Christian "Batch normalization: Accelerating deep network training by reducing internal covariate shift" In  , (2015)'>Ioffe et al. (2015)</a> and it has has 4 learnable parameters, each of size $F$, the number of features.</p><p>$$y \gets \operatorname{BatchNorm}(x; \{\bar{\mu}, \bar{\sigma}^2, \gamma, \beta \})$$
After (and during) training, the layer converges its output to $\mathcal{N}(\beta, \gamma^2I)$. The layer does the following transforms during the forward pass of the training.</p><p>$$y_{b,f} = \gamma_{f}\left( \frac{x_{b,f}-\mu_{f}}{\sqrt{\sigma^2_{f}-\epsilon}} \right) + \beta_{f}$$
$$\mu_{f} \gets \frac{1}{B}\sum_b x_{b,f}$$
$$\sigma^2_{f} \gets \frac{1}{B} \sum_t (x_{b,f}-\mu_{f})^2$$</p><p>During inference batch statistics are not available. So batch norm keeps track of running average of the batch statistics during training in $\bar{\mu}$ and $\bar{\sigma}^2$. These are used during inference as mean and variance.</p><p>$$\bar{\mu}_{f} \gets \operatorname{running-mean}(\mu_{f})$$
$$\bar{\sigma}_{f}^2 \gets \operatorname{running-mean}(\sigma^2_{f})$$</p><p>Each of the features are normalised independently using batch statistics. The $\epsilon$ is constant added for numerical stability.</p><p><strong>Why does batch norm work?</strong></p><p><em>Internal co-variate shift</em> was the original reasoning for batch norm discussed in <a href=http://proceedings.mlr.press/v37/ioffe15.pdf title='Ioffe, Sergey and Szegedy, Christian "Batch normalization: Accelerating deep network training by reducing internal covariate shift" In  , (2015)'>Ioffe et al. (2015)</a>. During training, when the lower layer&rsquo;s parameter changes, the output distribution changes. This leads to the following layers needing to constantly readjust its parameters.</p><p>There has been some evidence against the internal covariate shift theory. See <a href=https://arxiv.org/pdf/1805.11604.pdf title='Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander "How does batch normalization help optimization?" In Advances in neural information processing systems 31, (2018)'>Santurkar et al. (2018)</a></p><h3 id=layer-normalisation>Layer Normalisation</h3><p>While batch norm normalises each features independently, Layer norm normalises the total activation of a sample. Since we do not need to keep track of batch statistics, it only has 2 scalar parameters.</p><p>$$y \gets \operatorname{LayerNorm}(x; \{\gamma, \beta \})$$</p><p>Layer norm was introduces in <a href=https://arxiv.org/abs/1607.06450 title='Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E "Layer normalization" In arXiv preprint arXiv:1607.06450 , (2016)'>Lei Ba et al. (2016)</a> with the following definitions.</p><p>$$y_{b,f} = \gamma \left(\frac{x_{b,f}-\mu_{b}}{\sqrt{\sigma^2_{b}-\epsilon}}\right) + \beta$$</p><p>$$\mu_{b} = \frac{1}{F}\sum_f x_{b,f}$$
$$\sigma^2_{b} = \frac{1}{F} \sum_f (x_{b,f}-\mu_f)^2$$</p><p>Layer normalization is used in recurrent neural networks (RNNs) like architectures to address vanishing gradient problems.</p><blockquote><p>In a standard RNN, there is a tendency for the average magnitude of the summed inputs to the recurrent units to either grow or shrink at every time-step, leading to exploding or vanishing gradients. In a layer normalized RNN, the normalization terms make it invariant to re-scaling all of the summed inputs to a layer, which results in much more stable hidden-to-hidden dynamics. ~ <em>from <a href=https://arxiv.org/abs/1607.06450 title='Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E "Layer normalization" In arXiv preprint arXiv:1607.06450 , (2016)'>Lei Ba et al. (2016)</a></em></p></blockquote><p>Since layer norm does not have any contribution from different samples of a same batch, it does the same computation during inference as well. For the same reason, it is also distributed training friendly as gradients depend only on the single sample.</p><h2 id=rmsnorm>RMSNorm</h2><p><a href=https://arxiv.org/pdf/1910.07467.pdf title='Zhang, Biao and Sennrich, Rico "Root mean square layer normalization" In Advances in Neural Information Processing Systems 32, (2019)'>Root Mean Square Layer Normalization</a> is a variant of Layer norm. The difference is that instead of normalising the data by mean centering and dividing by variance, the data is simply divided by RMS.</p><p>$$y \gets \operatorname{RMSNorm}(x; \{\gamma\})$$</p><p>RMS measures the quadratic mean of inputs.</p><p>$$\operatorname{RMS}(x_b) = \sqrt{\frac{1}{F}\sum_f x_{b,f}^2}$$</p><p>Then RMS norm is computed as</p><p>$$y_{b,f} = \gamma_f \frac{x_{b,f}}{\operatorname{RMS}(x_b)}$$</p><p>The main difference to LayerNorm is that RMSNorm is not re-centered and thus does not show similar linearity property for variable shifting.</p><p>RMSNorm forces the summed inputs into a âˆšn-scaled unit sphere. By doing so, the output distribution remains same regardless of the scaling of input. RMSNorm is invariant to the scaling of its inputs. It is not invariant to all re-centering operations.</p><h2 id=normalisation-layers-for-data-with-channels>Normalisation Layers for data with channels</h2><h3 id=instance-normalization>Instance Normalization</h3><p>For data with multiple channels, applying a normalisation technique like layer normalisation might result in one of the channels being saturated. For such scenario, we use <a href=https://arxiv.org/abs/1607.08022 title='Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor "Instance normalization: The missing ingredient for fast stylization" In arXiv preprint arXiv:1607.08022 , (2016)'>Instance Normalization</a>.</p><p>The technique is similar to layer normalization, but the mean and variance statistics are calculated for each channels and instance independently. Instance normalization is commonly used in style transfer and image generation tasks.</p><h3 id=group-normalization>Group Normalization</h3><p><a href=https://arxiv.org/abs/1803.08494 title='Wu, Yuxin and He, Kaiming "Group normalization" In Proceedings of the European conference on computer vision (ECCV) pp.3--19, (2018)'>Group Normalization</a> is a variant of Instance normalisation which attempts to strike a balance between Layer norm and Instance norm. The channels of a layer are divided into groups, and mean and variance statistics are computed for each group separately.</p><h2 id=references>References</h2><reference><small><ul><li><p><a href=http://proceedings.mlr.press/v37/ioffe15.pdf title='Ioffe, Sergey and Szegedy, Christian "Batch normalization: Accelerating deep network training by reducing internal covariate shift" In  , (2015)'>Ioffe et al. (2015)</a>: Ioffe, Sergey and Szegedy, Christian &ldquo;<em>Batch normalization: Accelerating deep network training by reducing internal covariate shift</em>&rdquo; In , (2015)</p></li><li><p><a href=https://arxiv.org/abs/1607.06450 title='Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E "Layer normalization" In arXiv preprint arXiv:1607.06450 , (2016)'>Lei Ba et al. (2016)</a>: Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E &ldquo;<em>Layer normalization</em>&rdquo; In arXiv preprint arXiv:1607.06450 , (2016)</p></li><li><p><a href=https://arxiv.org/pdf/1802.05957.pdf title='Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi "Spectral normalization for generative adversarial networks" In arXiv preprint arXiv:1802.05957 , (2018)'>Miyato et al. (2018)</a>: Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi &ldquo;<em>Spectral normalization for generative adversarial networks</em>&rdquo; In arXiv preprint arXiv:1802.05957 , (2018)</p></li><li><p><a href=https://arxiv.org/pdf/1602.07868.pdf title='Salimans, Tim and Kingma, Durk P "Weight normalization: A simple reparameterization to accelerate training of deep neural networks" In Advances in neural information processing systems 29, (2016)'>Salimans et al. (2016)</a>: Salimans, Tim and Kingma, Durk P &ldquo;<em>Weight normalization: A simple reparameterization to accelerate training of deep neural networks</em>&rdquo; In Advances in neural information processing systems 29, (2016)</p></li><li><p><a href=https://arxiv.org/pdf/1805.11604.pdf title='Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander "How does batch normalization help optimization?" In Advances in neural information processing systems 31, (2018)'>Santurkar et al. (2018)</a>: Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander &ldquo;<em>How does batch normalization help optimization?</em>&rdquo; In Advances in neural information processing systems 31, (2018)</p></li><li><p><a href=https://arxiv.org/abs/1607.08022 title='Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor "Instance normalization: The missing ingredient for fast stylization" In arXiv preprint arXiv:1607.08022 , (2016)'>Ulyanov et al. (2016)</a>: Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor &ldquo;<em>Instance normalization: The missing ingredient for fast stylization</em>&rdquo; In arXiv preprint arXiv:1607.08022 , (2016)</p></li><li><p><a href=https://arxiv.org/pdf/1902.08129 title='Yang, Greg and Pennington, Jeffrey and Rao, Vinay and Sohl-Dickstein, Jascha and Schoenholz, Samuel S "A mean field theory of batch normalization" In arXiv preprint arXiv:1902.08129 , (2019)'>Yang et al. (2019)</a>: Yang, Greg and Pennington, Jeffrey and Rao, Vinay and Sohl-Dickstein, Jascha and Schoenholz, Samuel S &ldquo;<em>A mean field theory of batch normalization</em>&rdquo; In arXiv preprint arXiv:1902.08129 , (2019)</p></li><li><p><a href=https://arxiv.org/pdf/1910.07467.pdf title='Zhang, Biao and Sennrich, Rico "Root mean square layer normalization" In Advances in Neural Information Processing Systems 32, (2019)'>Zhang et al. (2019)</a>: Zhang, Biao and Sennrich, Rico &ldquo;<em>Root mean square layer normalization</em>&rdquo; In Advances in Neural Information Processing Systems 32, (2019)</p></li><li><p><a href=https://arxiv.org/abs/1803.08494 title='Wu, Yuxin and He, Kaiming "Group normalization" In Proceedings of the European conference on computer vision (ECCV) pp.3--19, (2018)'>Wu et al. (2018)</a>: Wu, Yuxin and He, Kaiming &ldquo;<em>Group normalization</em>&rdquo; In Proceedings of the European conference on computer vision (ECCV) pp. 3&ndash;19, (2018)</p></li><li><p><em><a href=https://en.wikipedia.org/wiki/Lipschitz_continuity title="Lipschitz Continuity">Lipschitz Continuity</a></em><br><small><em><code>https://en.wikipedia.org/wiki/Lipschitz_continuity</code></em> [accessed - Oct 2023]</small></p></li><li><p><em><a href=https://en.wikipedia.org/wiki/Matrix_norm title="Matrix Norm">Matrix Norm</a></em><br><small><em><code>https://en.wikipedia.org/wiki/Matrix_norm</code></em> [accessed - Oct 2023]</small></p></li></ul></small></reference></article><br><br><br><i>tags:</i>
<a class=gray style=padding-right:5px href=/tags#machine-learning><i>#machine-learning</i></a></section></main><footer><p>Found something useful here? pass it on; or tell me about it!<nav><ul><li><a href="https://drive.google.com/drive/folders/18LU6PKnxy8rDrAXhpiPXV2yOYlO_I8cF?usp=sharing">Resume</a></li><li><a href=https://daxpy.xyz/omnia_mea>Coding notes</a></li><li><a href=https://www.linkedin.com/in/ndivakar/>Linkedin</a></li></ul></nav><small><a href=#top>â†‘ Top of page</a></small></p></footer></body></html>