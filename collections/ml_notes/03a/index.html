<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><title>Implementing Self Attention | DAXPY</title><link rel=stylesheet href=/css/main.min.d700ad2998f190f21436ef39c94129166ea316da7e0014005c9c2ced3b288a04.css integrity="sha256-1wCtKZjxkPIUNu85yUEpFm6jFtp+ABQAXJws7TsoigQ=" crossorigin=anonymous><script src=/js/main.23cd0c7d837263b9eaeb96ee2d9ccfa2969daa3fa00fa1c1fe8701a9b87251a1.js integrity="sha256-I80MfYNyY7nq65buLZzPopadqj+gD6HB/ocBqbhyUaE=" crossorigin=anonymous></script></head><body><header><nav><ul><li><a class=text_secondary href=https://daxpy.xyz/>DAXPY</a></li><li>|</li><li><a href=https://daxpy.xyz/posts/>Posts</a></li><li><a href=https://daxpy.xyz/notes/>Notes</a></li><li><a href=https://daxpy.xyz/collections/>Collections</a></li><li><a href=https://daxpy.xyz/links/>Links</a></li><li><a href=https://daxpy.xyz/stories/>Stories</a></li><li><a href=/about>About</a></li></ul></nav></header><main><section><div style=text-align:right;height:5px;margin-top:10px class=italic>this page
⟩ <a href=/collections/ml_notes/>ml</a>
⟩ <a href=/collections/>collections</a>
⟩ <a href=/>home</a></div><article><h2 id=implementing-self-attention>Implementing Self Attention</h2><p>b is batch, t is tokens and d is token embedding size.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>scaled_dot_attention</span>(
</span></span><span style=display:flex><span>    Q: Tensor[b, t, d],
</span></span><span style=display:flex><span>    K: Tensor[b, t, d],
</span></span><span style=display:flex><span>    V: Tensor[b, t, d]
</span></span><span style=display:flex><span>) <span style=color:#f92672>-&gt;</span> Tensor[b, t, d]:
</span></span><span style=display:flex><span>    dot: Tensor[b, t, t] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#39;b i d , b j d -&gt; b i j&#39;</span>, Q, K) <span style=color:#f92672>*</span> sqrt(d)
</span></span><span style=display:flex><span>    attention: Tensor[b, t, t] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>softmax(dot, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    out: Tensor[b, t, d] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#39;t i j , t j d -&gt; t i d&#39;</span>, attention, V)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> out
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>self_attention</span>(X: Tensor[b, t, c]) <span style=color:#f92672>-&gt;</span> Tensor[b, t, d]:
</span></span><span style=display:flex><span>    Wq, Wk, Wv <span style=color:#f92672>=</span> <span style=color:#f92672>...</span> <span style=color:#75715e># define weight matrices</span>
</span></span><span style=display:flex><span>    Q: Tensor[b, t, d] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#39;b i c, c d -&gt; b i d&#39;</span>, X, Wq)
</span></span><span style=display:flex><span>    K: Tensor[b, t, d] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#39;b i c, c d -&gt; b i d&#39;</span>, X, Wk)
</span></span><span style=display:flex><span>    V: Tensor[b, t, d] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#39;b i c, c d -&gt; b i d&#39;</span>, X, Wv)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> scaled_dot_attention(Q, K, V)
</span></span></code></pre></div></article><br><br><br><i>tags:</i>
<a class=gray style=padding-right:5px href=/tags#self%20attention><i>#self attention</i></a>
<a class=gray style=padding-right:5px href=/tags#transformers><i>#transformers</i></a></section></main><footer><p>Found something useful here? pass it on; or tell me about it!<nav><ul><li><a href="https://drive.google.com/drive/folders/18LU6PKnxy8rDrAXhpiPXV2yOYlO_I8cF?usp=sharing">Resume</a></li><li><a href=https://daxpy.xyz/collections/>Collections</a></li><li><a href=https://www.linkedin.com/in/ndivakar/>Linkedin</a></li></ul></nav><small><a href=#top>↑ Top of page</a></small></p></footer></body></html>