<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><title>ML | DAXPY</title><link rel=stylesheet href=/css/main.min.94a2540136d9a839e3ae21136b6ed35c457b231d8bd1a157c7089710a8682c84.css integrity="sha256-lKJUATbZqDnjriETa27TXEV7Ix2L0aFXxwiXEKhoLIQ=" crossorigin=anonymous><script src=/js/main.23cd0c7d837263b9eaeb96ee2d9ccfa2969daa3fa00fa1c1fe8701a9b87251a1.js integrity="sha256-I80MfYNyY7nq65buLZzPopadqj+gD6HB/ocBqbhyUaE=" crossorigin=anonymous></script></head><body><header><nav><ul><li><a class=text_secondary href=https://daxpy.xyz/>DAXPY</a></li><li>|</li><li><a href=https://daxpy.xyz/posts/>Posts</a></li><li><a href=https://daxpy.xyz/notes/>Notes</a></li><li><a href=https://daxpy.xyz/collections/>Collections</a></li><li><a href=https://daxpy.xyz/links/>Links</a></li><li><a href=https://daxpy.xyz/stories/>Stories</a></li><li><a href=/about>About</a></li></ul></nav></header><main><a name=top></a><div style=text-align:right;height:5px;margin-top:10px class=italic>this page
⟩ <a href=/collections/>collections</a>
⟩ <a href=/>home</a></div><h1>Collection: ML</h1><p><b>Table of Content</b></p><ul><li><a href=#02a><span>02a Kitchen sink optimiser model</span></a></li><li><a href=#03a><span>03a Implementing Self Attention</span></a></li></ul><hr><div class=note id=02a><div style=text-align:right;height:5px><a href=#top class="xxs grey monospace">[top]</a><a name=02a href=/collections/ml_notes/02a/ class="xxs grey monospace">[02a]</a></div><h2 id=kitchen-sink-optimiser-model>Kitchen sink optimiser model</h2><p>A typical loss function of a model is comprised of loss from data (E) and loss from weight or a regulariser (R)</p><p>$$ loss = E(w_t) + \gamma R(w_t) $$</p><p>gradient</p><p>$$ g_t = \nabla E(w_t) + \gamma \nabla R(w_t) $$</p><p>velocity</p><p>$$ v_{t+1} = f\left(\beta_1 v_t + (1 - \beta_1)(g_t)^2\right) $$</p><p>momentum</p><p>$$ m_{t+1} = h\left(\beta_2 m_t +(1 - \beta_2) g_t\right) $$</p><p>weight update</p><p>$$ w_{t+1} = w_t - \alpha \left( \frac{m_{t+1} }{\sqrt{v_{t+1}} + \epsilon} \right) - \lambda w_t $$</p><p>The final term is weight decay. We can derive all the optimisers based on the value of constants and the nature of $f$ and $h$</p><span class=tag>optimisers</span></div><div class=note id=03a><div style=text-align:right;height:5px><a href=#top class="xxs grey monospace">[top]</a><a name=03a href=/collections/ml_notes/03a/ class="xxs grey monospace">[03a]</a></div><h2 id=implementing-self-attention>Implementing Self Attention</h2><p>b is batch, t is tokens and d is token embedding size.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>scaled_dot_attention</span>(
</span></span><span style=display:flex><span>    Q: Tensor[b, t, d],
</span></span><span style=display:flex><span>    K: Tensor[b, t, d],
</span></span><span style=display:flex><span>    V: Tensor[b, t, d]
</span></span><span style=display:flex><span>) <span style=color:#f92672>-&gt;</span> Tensor[b, t, d]:
</span></span><span style=display:flex><span>    dot: Tensor[b, t, t] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#39;b i d , b j d -&gt; b i j&#39;</span>, Q, K) <span style=color:#f92672>*</span> sqrt(d)
</span></span><span style=display:flex><span>    attention: Tensor[b, t, t] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>softmax(dot, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    out: Tensor[b, t, d] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#39;t i j , t j d -&gt; t i d&#39;</span>, attention, V)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> out
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>self_attention</span>(X: Tensor[b, t, c]) <span style=color:#f92672>-&gt;</span> Tensor[b, t, d]:
</span></span><span style=display:flex><span>    Wq, Wk, Wv <span style=color:#f92672>=</span> <span style=color:#f92672>...</span> <span style=color:#75715e># define weight matrices</span>
</span></span><span style=display:flex><span>    Q: Tensor[b, t, d] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#39;b i c, c d -&gt; b i d&#39;</span>, X, Wq)
</span></span><span style=display:flex><span>    K: Tensor[b, t, d] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#39;b i c, c d -&gt; b i d&#39;</span>, X, Wk)
</span></span><span style=display:flex><span>    V: Tensor[b, t, d] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#39;b i c, c d -&gt; b i d&#39;</span>, X, Wv)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> scaled_dot_attention(Q, K, V)
</span></span></code></pre></div><span class=tag>self attention</span>
<span class=tag>transformers</span></div></main><footer><p>Found something useful here? pass it on; or tell me about it!<nav><ul><li><a href="https://drive.google.com/drive/folders/18LU6PKnxy8rDrAXhpiPXV2yOYlO_I8cF?usp=sharing">Resume</a></li><li><a href=https://daxpy.xyz/collections/>Collections</a></li><li><a href=https://www.linkedin.com/in/ndivakar/>Linkedin</a></li></ul></nav><small><a href=#top>↑ Top of page</a></small></p></footer></body></html>