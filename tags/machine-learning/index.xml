<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine-Learning on DAXPY</title><link>https://daxpy.xyz/tags/machine-learning/</link><description>Recent content in Machine-Learning on DAXPY</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 28 Apr 2024 21:30:00 +0530</lastBuildDate><atom:link href="https://daxpy.xyz/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>XGBoost</title><link>https://daxpy.xyz/posts/xgboost/</link><pubDate>Sun, 28 Apr 2024 21:30:00 +0530</pubDate><guid>https://daxpy.xyz/posts/xgboost/</guid><description>&lt;h1 id="xgboost">XGBoost&lt;/h1>
&lt;p>&lt;strong>Extreme Gradient Boosting&lt;/strong> or XGBoost is a technique that has become quite useful for solving prediction problems. XGBoost is also quite interesting academically; for it combines quite few techniques together to give us one robust method. The technique is composed from gradient boosting, decision trees, matching pursuit and gradient descent in function space among others. In this post, we will explore and derive the inner workings of XGBoost.&lt;/p>
&lt;h2 id="the-regression-problem">The Regression Problem&lt;/h2>
&lt;p>We are given a set of samples from population $\{(y_i,x_i)\}_{i=1}^{N}$ which we call a dataset. $y$&amp;rsquo;s are scalars and $x$ are vectors.&lt;/p></description></item><item><title>Normalisation Layers</title><link>https://daxpy.xyz/posts/normalisation/</link><pubDate>Tue, 24 Oct 2023 18:33:10 +0530</pubDate><guid>https://daxpy.xyz/posts/normalisation/</guid><description>&lt;h1 id="normalisation">Normalisation&lt;/h1>
&lt;p>Regulating the magnitude of activations inside a neural network is crucial for an effective training regime. We may get stuck in local minima or worse yet, the training may diverge otherwise. For this, we make use of normalisation.&lt;/p>
&lt;p>Normalisation comes in two flavours. Weight normalisation and Layer normalisation. We briefly touch on some fundamental techniques in both.&lt;/p>
&lt;h2 id="weight-normalisation">Weight normalisation&lt;/h2>
&lt;p>In weight normalisation, we focus on the magnitude of the parameters of the network; preventing them from uncontrollable growth or collapse. The basic technique appeared in &lt;a href="https://arxiv.org/pdf/1602.07868.pdf" title="Salimans, Tim and Kingma, Durk P &amp;quot;Weight normalization: A simple reparameterization to accelerate training of deep neural networks&amp;quot; In Advances in neural information processing systems 29, (2016)">Salimans et al. (2016)&lt;/a> which describes a simple scenario.&lt;/p></description></item><item><title>Variational Inference</title><link>https://daxpy.xyz/posts/variational-inference/</link><pubDate>Sun, 16 Apr 2023 12:15:00 +0530</pubDate><guid>https://daxpy.xyz/posts/variational-inference/</guid><description>&lt;h1 id="variational-inference">Variational Inference&lt;/h1>
&lt;p>We have some data from a population and we suspect that the it is generated by some underlying process. Estimating the process which generates the data allows us to understand its fundamental properties.&lt;/p>
&lt;p>Concretely, $p(x)$ is the distribution of the data and $z$ are its latent variables, the process which generates the data is $p(x|z)$.
Estimating the generation process is computing the true posterior $p(z|x)$. This is the (posterior) inference problem.&lt;/p></description></item><item><title>Gradient Boosting</title><link>https://daxpy.xyz/notes/gradient-boosting/</link><pubDate>Thu, 14 Apr 2022 04:00:00 +0530</pubDate><guid>https://daxpy.xyz/notes/gradient-boosting/</guid><description>&lt;h1 id="gradient-boosting">Gradient Boosting&lt;/h1>
&lt;p>The general framework of Boosting is learners are added in greedy manner to minimise loss.
$$F_t(x) = F_{t-1}(x) + f_t(x)$$
At the $t^{th}$ step, we are interested in learning the function $f$ which minimised the loss. The value of loss function at this point is given by&lt;/p>
&lt;p>$$\begin{aligned}
L &amp;amp;= l(y,p+f(x))
\\
&amp;amp;=l(y,p) + \nabla_{p} l(y,p)f(x)
\end{aligned}$$&lt;/p>
&lt;p>The last step is from first order taylor series approximation of $l$.
$$f(x) = f(a) + (x-a) f^{\prime}(a)$$&lt;/p></description></item><item><title>Attribute Selection in Decision Trees</title><link>https://daxpy.xyz/notes/attribute-selection-in-decision-trees/</link><pubDate>Thu, 14 Apr 2022 00:15:30 +0530</pubDate><guid>https://daxpy.xyz/notes/attribute-selection-in-decision-trees/</guid><description>&lt;h1 id="attribute-selection-in-decision-trees">Attribute Selection in Decision Trees&lt;/h1>
&lt;p>For constructing a new node in decision tree, choosing which attribute to partition the data on is important. Choosing a less desirable attribute to split the data on may result in lower performance. Lets look into a few important measures which helps us find the best attribute.&lt;/p>
&lt;h2 id="information-gain">Information Gain&lt;/h2>
&lt;p>Information Gain is defined as amount of information gained about a random variable (outcome) from observing another (attribute).
We can quantify information gain as difference in entropy when random variable is observed.&lt;/p></description></item><item><title>Bias and Variance</title><link>https://daxpy.xyz/notes/bias-and-variance/</link><pubDate>Thu, 10 Mar 2022 10:15:30 +0530</pubDate><guid>https://daxpy.xyz/notes/bias-and-variance/</guid><description>&lt;h1 id="bias-and-variance">Bias and Variance&lt;/h1>
&lt;p>A training set is only a subset of the population of data. Bias-variance trade-off talks about characteristics of predictions from the same algorithm if we use different subsets of the population as training set.&lt;/p>
&lt;p>&lt;strong>Bias&lt;/strong> is difference between true value and average predictions from model trained on different training set.&lt;/p>
&lt;p>&lt;strong>Variance&lt;/strong> is an estimate of how much the average prediction varies when we change the training set.&lt;/p></description></item><item><title>Common Optimisers</title><link>https://daxpy.xyz/notes/common-optimisers/</link><pubDate>Sun, 24 Oct 2021 05:04:51 +0530</pubDate><guid>https://daxpy.xyz/notes/common-optimisers/</guid><description>&lt;h1 id="common-optimisers">Common Optimisers&lt;/h1>
&lt;p>$E$ is the loss function and $w$ is the model parameters;&lt;/p>
&lt;h2 id="stochastic-gradient-descent">Stochastic Gradient Descent&lt;/h2>
&lt;p>$$\begin{aligned}
w_{t+1}&amp;amp;= w_t - \alpha \nabla E(w_t)\end{aligned}$$&lt;/p>
&lt;h2 id="sgd-with-momentum">SGD with Momentum&lt;/h2>
&lt;p>&lt;em>Use gradient to update velocity/direction of a particle instead of only updating its position&lt;/em>&lt;/p>
&lt;p>$$\begin{aligned}
m_{t+1} &amp;amp;= \eta m_t + \alpha \nabla E(w_t)
\\
w_{t+1}&amp;amp;= w_t - m_{t+1}
\end{aligned}$$
This results in equivalent single update as
$$w_{t+1}= w_t - \alpha \nabla E(w_t) - \eta m_{t}$$&lt;/p></description></item><item><title>Focal Loss</title><link>https://daxpy.xyz/notes/2021-10-10-focal-loss/</link><pubDate>Sun, 10 Oct 2021 05:04:51 +0530</pubDate><guid>https://daxpy.xyz/notes/2021-10-10-focal-loss/</guid><description>&lt;h1 id="focal-loss">Focal Loss&lt;/h1>
&lt;p>For binary classification problem, the standard cross entropy loss is given by&lt;/p>
&lt;p>$$CE(p,y_t)=\begin{cases}-\log(p)&amp;amp;y_t=1
\\
-\log(1-p)&amp;amp;else\end{cases}$$&lt;/p>
&lt;p>We can simplify this to $CE(p_t) = -\log(p_t)$ if we define
$$p_t \mathop{\mathrm{\triangleq}}
\begin{cases}p&amp;amp;y_t=1
\\
1-p&amp;amp;else\end{cases}$$&lt;/p>
&lt;p>What if there is a huge imbalance between no of positive and negative samples? The standard way of fixing this would be to add a balancing term $\alpha$ which is derived from inverse class frequencies. Let&lt;/p>
&lt;p>$$\alpha_t \mathop{\mathrm{\triangleq}}\begin{cases}\alpha&amp;amp;y_t=1\\1-\alpha&amp;amp;else\end{cases}$$&lt;/p></description></item><item><title>Softmax Classifier</title><link>https://daxpy.xyz/notes/softmax_classifier/</link><pubDate>Mon, 13 Jan 2020 00:00:51 +0530</pubDate><guid>https://daxpy.xyz/notes/softmax_classifier/</guid><description>&lt;h1 id="softmax-classifier">Softmax Classifier&lt;/h1>
&lt;p>Imagine we have a dataset $\{x,y\}_{i=0}^m$ where $x$ is a data point
and $y$ indicates the class $x$ belongs to. For deriving LDA classifier,
we had modeled the class conditional density $P(x|y)$ as a Gaussian and
derived the posterior probabilities $P(y|x)$. Here, we will directly
model the posterior with a linear function. Since the posterior directly
models what class a data point belongs to, we don&amp;rsquo;t have much to do
after to get a classifier.&lt;/p></description></item><item><title>Linear Classifiers</title><link>https://daxpy.xyz/notes/linear_classifiers/</link><pubDate>Sun, 18 Feb 2018 05:04:51 +0530</pubDate><guid>https://daxpy.xyz/notes/linear_classifiers/</guid><description>&lt;h1 id="linear-classifiers">Linear Classifiers&lt;/h1>
&lt;p>In the post on bayes error, we discussed what is the best classifier if
the features are not enough to tell the class apart. We also derived
that in such situation, the best classifier is
$$h(x) = sign \left( n(x) - \frac{1}{2} \right)$$&lt;/p>
&lt;p>This formulation cannot be used in general situations as there is no easy way
to estimate $n(x) = P(y=+1|x)$ for a general distribution. But what if
$x$ has a simple distribution?&lt;/p></description></item><item><title>Bayes Error</title><link>https://daxpy.xyz/notes/bayes_error/</link><pubDate>Sun, 11 Feb 2018 05:04:51 +0530</pubDate><guid>https://daxpy.xyz/notes/bayes_error/</guid><description>&lt;h1 id="bayes-error">Bayes Error&lt;/h1>
&lt;p>In an ideal world, everything has reason. Every question has a unambiguous answer. The data in sufficient to explain its behaviours, like the class it belongs to.&lt;/p>
&lt;p>$$\begin{aligned}
g(x) = y \end{aligned}$$&lt;/p>
&lt;p>In the non ideal world, however, there is always something missing that stops us from knowing the entire truth. $g$ is beyond reach. In such cases we resort to probability.&lt;/p>
&lt;p>$$\begin{aligned}
n(x) = P(y=1|x)\end{aligned}$$&lt;/p>
&lt;p>It simply tells us how probable is the data belonging to a class($y=1$) if my observations are $x$.&lt;/p></description></item></channel></rss>