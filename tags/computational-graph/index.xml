<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Computational-Graph on DAXPY</title><link>https://daxpy.xyz/tags/computational-graph/</link><description>Recent content in Computational-Graph on DAXPY</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 02 Oct 2023 05:04:51 +0530</lastBuildDate><atom:link href="https://daxpy.xyz/tags/computational-graph/index.xml" rel="self" type="application/rss+xml"/><item><title>Gradient Through Concatenation</title><link>https://daxpy.xyz/notes/gradient-through-concatenation/</link><pubDate>Mon, 02 Oct 2023 05:04:51 +0530</pubDate><guid>https://daxpy.xyz/notes/gradient-through-concatenation/</guid><description>&lt;h1 id="gradient-through-concatenation">Gradient Through Concatenation&lt;/h1>
&lt;p>Concatenation of vectors is a common operation in Deep Learning Networks. How can we compute derivative of the
output in the computational graph?&lt;/p>
&lt;p>We can write the operation as&lt;/p>
&lt;p>$$z = x|y$$&lt;/p>
&lt;p>Where $|$ is concat operator. We are interested in computing ${\partial z}/{\partial x}$ and ${\partial z}/{\partial y}$&lt;/p>
&lt;p>Assuming $x\in \mathbb{R}^m$ and $x\in \mathbb{R}^n$. We can rewrite the concat operation as&lt;/p>
&lt;p>$$z = \begin{bmatrix}I_m &amp;amp; 0\end{bmatrix}x+\begin{bmatrix}0 &amp;amp; I_n\end{bmatrix}y$$&lt;/p></description></item><item><title>Gradient Through Addition with Broadcasting</title><link>https://daxpy.xyz/notes/2018-09-18-gradient-through-addition-with-bradcasting/</link><pubDate>Tue, 18 Sep 2018 05:04:51 +0530</pubDate><guid>https://daxpy.xyz/notes/2018-09-18-gradient-through-addition-with-bradcasting/</guid><description>&lt;h1 id="gradient-through-addition-with-broadcasting">Gradient Through Addition with Broadcasting&lt;/h1>
&lt;p>Calculating gradient across an addition is a simple
algebra trick. But this gets complicated when we look at addition of tensors which allows
broadcasting. In this post, we examine how to compute gradients in such situations.&lt;/p>
&lt;h2 id="gradient-of-addition">Gradient of Addition&lt;/h2>
&lt;p>Consider a simple sequence of operations. $A$ and $B$ are inputs which
ultimately leads to computation of a scalar loss/error term $l$.&lt;/p>
&lt;p>$$ z = A+B $$
$$ l \twoheadleftarrow z $$&lt;/p></description></item><item><title>Differentiable Computations</title><link>https://daxpy.xyz/posts/differentiable-computations/</link><pubDate>Sun, 18 Feb 2018 00:00:00 +0000</pubDate><guid>https://daxpy.xyz/posts/differentiable-computations/</guid><description>&lt;h1 id="differentiable-computations">Differentiable Computations&lt;/h1>
&lt;p>Auto-grad or automatic gradient computation is a nice feature found in many computational frameworks. Specify the computation in forward direction and the framework computes backward gradients. Let&amp;rsquo;s talk about the generic method to do this.&lt;/p>
&lt;p>Let&amp;rsquo;s say we have to compute the result of &amp;lsquo;something&amp;rsquo;. It may be a
nasty heat equation or some logic driven steps to get from input to
output. Abstracting the steps involved gives us a sequence of equations
$$\begin{aligned}
z_i = f_i(z_{a(i)})\end{aligned}$$&lt;/p></description></item></channel></rss>