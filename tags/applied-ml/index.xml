<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Applied-Ml on DAXPY</title><link>https://daxpy.xyz/tags/applied-ml/</link><description>Recent content in Applied-Ml on DAXPY</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 13 Apr 2021 05:04:51 +0530</lastBuildDate><atom:link href="https://daxpy.xyz/tags/applied-ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Movie Reviews and Gradient Descent</title><link>https://daxpy.xyz/notes/2021-04-13-014-movie-reviews-and-gradient-descent/</link><pubDate>Tue, 13 Apr 2021 05:04:51 +0530</pubDate><guid>https://daxpy.xyz/notes/2021-04-13-014-movie-reviews-and-gradient-descent/</guid><description>&lt;h1 id="movie-reviews-and-gradient-descent">Movie Reviews and Gradient Descent&lt;/h1>
&lt;p>The problem is simple; $m$ movies, $n$ users and we have the data of rating of movies by users. Now in reality, each user might have rated a few movies while total number of users and movies are huge. We can consider the whole ratings data as a matrix $n\times m$ matrix $R$ where $R_{ij}$ is the rating of $i^{th}$ movie by $j^{th}$ user.&lt;/p>
&lt;p>We want a model which can predict the rating a user would have given a movie. Now the modelling part is easy as a matrix factorisation problem. We can assume every user is represented by a embedding $u_i$ and similarly, every movie has a embedding $m_j$. We simply want $$R_{ij} \sim { u_i}^\intercal m_j$$ If we simply model the learning problem as
$$\mathop{\mathrm{arg,min}}_{U,M} \| {U}^\intercal M - R \|_2$$
then we are implicitly assuming that all unavailable ratings are $0$. Since we have only a few ratings compared to all possible combinations of movies and users, the model will get biased towards $0$ ratings.&lt;/p></description></item></channel></rss>