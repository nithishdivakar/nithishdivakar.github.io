<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Optimisation on DAXPY</title><link>https://daxpy.xyz/tags/optimisation/</link><description>Recent content in Optimisation on DAXPY</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 16 Apr 2023 12:15:00 +0530</lastBuildDate><atom:link href="https://daxpy.xyz/tags/optimisation/index.xml" rel="self" type="application/rss+xml"/><item><title>Variational Inference</title><link>https://daxpy.xyz/posts/variational-inference/</link><pubDate>Sun, 16 Apr 2023 12:15:00 +0530</pubDate><guid>https://daxpy.xyz/posts/variational-inference/</guid><description>&lt;h1 id="variational-inference"&gt;Variational Inference&lt;/h1&gt;
&lt;p&gt;We have some data from a population and we suspect that the it is generated by some underlying process. Estimating the process which generates the data allows us to understand its fundamental properties.&lt;/p&gt;
&lt;p&gt;Concretely, $p(x)$ is the distribution of the data and $z$ are its latent variables, the process which generates the data is $p(x|z)$.
Estimating the generation process is computing the true posterior $p(z|x)$. This is the (posterior) inference problem.&lt;/p&gt;</description></item><item><title>KKT conditions and Lagrange multipliers</title><link>https://daxpy.xyz/notes/kkt-conditions/</link><pubDate>Fri, 11 Mar 2022 06:15:30 +0530</pubDate><guid>https://daxpy.xyz/notes/kkt-conditions/</guid><description>&lt;h1 id="karush-kuhn-tucker-conditions"&gt;Karush-Kuhn-Tucker conditions&lt;/h1&gt;
&lt;p&gt;A typical constrained optimisation problem is as follows.
$$\begin{aligned}
\min_{x\in\mathbb{R}^n}&amp;amp;f(x)
\\
s.t.~h_i(x) &amp;amp;= 0
\\
g_j(x) &amp;amp;\leq 0
\end{aligned}$$&lt;/p&gt;
&lt;h2 id="karush-kuhn-tucker-conditions-1"&gt;Karush-Kuhn-Tucker conditions&lt;/h2&gt;
&lt;p&gt;If the negative of the gradient (of $f$) has any component along an equality constraint $h(x)=0$, then we can take small steps along this surface to reduce $f(x)$.&lt;/p&gt;
&lt;p&gt;Since $\nabla h(x)$, the gradient of the equality constraint is always perpendicular to the constraint surface $h(x)=0$, at optimum, $-\nabla f(x)$ should be either parallel or anti-parallel to $\nabla h(x)$
$$-\nabla f(x) = \mu \nabla h(x)$$
A similar argument can be made for inequality constraints. These form KKT conditions. So at an optimum point $x^\ast$ we have,
$$\begin{aligned}
h_i(x^\ast)&amp;amp;=0
&amp;amp;
g_j(x^\ast) &amp;amp;\leq 0
\\
\lambda_j g_j(x^\ast) &amp;amp;= 0
&amp;amp;
\lambda_j &amp;amp;\geq 0
\end{aligned}$$
$$\nabla f(x^\ast) +\sum_{i} \mu_i\nabla h(x^\ast) + \sum_j \lambda_j \nabla g_j(x^\ast)= 0$$
These are the KKT conditions for constrained optimisation.&lt;/p&gt;</description></item><item><title>BFGS</title><link>https://daxpy.xyz/notes/bfgs/</link><pubDate>Fri, 11 Mar 2022 00:15:30 +0530</pubDate><guid>https://daxpy.xyz/notes/bfgs/</guid><description>&lt;h1 id="bfgs"&gt;BFGS&lt;/h1&gt;
&lt;h2 id="newtons-method"&gt;Newton&amp;rsquo;s Method&lt;/h2&gt;
&lt;p&gt;$$\begin{aligned}
x_{k+1} &amp;amp;= x_k - [H(x_k)]^{-1}\nabla f(x_k)^\intercal
\end{aligned}$$&lt;/p&gt;
&lt;h2 id="quasi-newtons-method"&gt;Quasi Newton&amp;rsquo;s Method&lt;/h2&gt;
&lt;p&gt;$$\begin{aligned}
x_{k+1} &amp;amp;= x_k - \alpha_kS_k {\nabla f(x_k)}^{T}
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;If $S_k$ is inverse of Hessian, then method is Newton&amp;rsquo;s iteration; if $S_k=I$, then it is steepest descent&lt;/p&gt;
&lt;h2 id="bfgs-1"&gt;BFGS&lt;/h2&gt;
&lt;p&gt;BFGS is a quasi newtons method where we approximate inverse of Hessian by $B_k$. The search direction $p_k$ is determined by solving
$$B_kp_k = -\nabla f(x_k)$$
A line search is performed in this search direction to find next point $x_{k+1}$ by minimising $f(x_k+\gamma p_k)$. The approximation to hessian is then updated as
$$\begin{aligned}
B_{k+1} &amp;amp;= B_k + \alpha_k u_ku_k^\intercal + \beta_k v_kv_k^\intercal
\\
u_k &amp;amp;= \nabla f(x_{k+1})-\nabla f(x_k)
\\
\alpha_k &amp;amp;= \frac{1}{\alpha u_k^\intercal p_k}
\\
v_k &amp;amp;= B_kp_k
\\
\beta_k &amp;amp;= \frac{-1}{p_k^\intercal B_kp_k}
\end{aligned}$$&lt;/p&gt;</description></item><item><title>Newton's Method</title><link>https://daxpy.xyz/notes/2022-01-04-newtons-method/</link><pubDate>Tue, 04 Jan 2022 05:04:51 +0530</pubDate><guid>https://daxpy.xyz/notes/2022-01-04-newtons-method/</guid><description>&lt;h1 id="newtons-method"&gt;Newton&amp;rsquo;s Method&lt;/h1&gt;
&lt;p&gt;To derive newton&amp;rsquo;s method, we simply have to find the optimum point from second order Taylor series expansion of $f(x)$
$$\begin{aligned}
x_{k+1} &amp;amp;= x_k - [H(x_k)]^{-1}\nabla f(x_k)^\intercal
\end{aligned}$$
&lt;em&gt;Derivation&lt;/em&gt;: From a point $x_k$, we want to compute the best possible move $x_k+s$ to minimise $f$. Using taylor series expansion, we have
$$f(x_k+s) = f(x_k) + s\nabla f(x_k) + \frac{s^2}{2!} H(x_k) = g(s)$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
0 &amp;amp;= \nabla_s g(s) = \nabla f(x_k) + s H(x_k)
\\
s &amp;amp;= - H(x_k)^{-1} {\nabla f(x_k)}^\intercal
\end{aligned}$$&lt;/p&gt;</description></item></channel></rss>