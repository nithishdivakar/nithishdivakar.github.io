<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>EE on daxpy</title><link>https://daxpy.xyz/ee/</link><description>Recent content in EE on daxpy</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 01 Jan 1990 00:00:00 +0000</lastBuildDate><atom:link href="https://daxpy.xyz/ee/index.xml" rel="self" type="application/rss+xml"/><item><title>XGBoost</title><link>https://daxpy.xyz/posts/xgboost/</link><pubDate>Sun, 28 Apr 2024 21:30:00 +0530</pubDate><guid>https://daxpy.xyz/posts/xgboost/</guid><description>&lt;h1 id="xgboost">XGBoost&lt;/h1>
&lt;p>&lt;strong>Extreme Gradient Boosting&lt;/strong> or XGBoost is a technique that has become quite useful for solving prediction problems. XGBoost is also quite interesting academically; for it combines quite few techniques together to give us one robust method. The technique is composed from gradient boosting, decision trees, matching pursuit and gradient descent in function space among others. In this post, we will explore and derive the inner workings of XGBoost.&lt;/p>
&lt;h2 id="the-regression-problem">The Regression Problem&lt;/h2>
&lt;p>We are given a set of samples from population $\{(y_i,x_i)\}_{i=1}^{N}$ which we call a dataset. $y$&amp;rsquo;s are scalars and $x$ are vectors.&lt;/p>
&lt;p>There is also some function $F^{\ast}$ on the population which perfectly determines the entire population i.e. $y = F^{\ast}(x)$. However, this function is unknown and all we have is the dataset.&lt;/p>
&lt;p>Regression problem is computing some $\hat{F}$ which is the best approximation of $F^{\ast}$ that minimises some loss function $L(y,F(x))$ over all values of $(y,x)$.&lt;/p>
&lt;p>Different choices of $L$ and restriction on the structrue of $F$ leads to different algorithms. For example; mean squared error and linear function on $x$ gives &lt;a href="https://en.wikipedia.org/wiki/Linear_regression" title="Linear Regression">linear regession&lt;/a>.&lt;/p>
&lt;h2 id="the-family-of-functions">T.H.E family of functions&lt;/h2>
&lt;p>Let assume (to make things easier) that $F$ has a simpler structure. The all powerful $F$ is from the family of functions which are weighted combination of simpler functions.&lt;/p>
&lt;p>$$F(x) = \sum \beta_m h_m(x)$$&lt;/p>
&lt;p>We do such approximations all the time. See &lt;a href="https://www.cs.cmu.edu/~epxing/Class/10708-17/notes-17/10708-scribe-lecture13.pdf">mean field approximation for variational inference&lt;/a> for a family which is &lt;em>product&lt;/em> of simpler functions.&lt;/p>
&lt;p>This makes our &lt;em>learning&lt;/em> easier. It&amp;rsquo;s easier to see if we look at the explict structure of $F$.&lt;/p>
&lt;p>$$F(x;\{\beta_t,\alpha_t\}) = \sum_t \beta_t h_t(x;\alpha_t)$$&lt;/p>
&lt;p>All we need to do now is compute values of parameters $\beta$&amp;rsquo;s and $\alpha$&amp;rsquo;s which minimised $L$.&lt;/p>
&lt;p>By the way, did we just sneak in a constraint on structure of $F$ ??? Are all the $h$&amp;rsquo;s same or different?&lt;/p>
&lt;p>Well the structure allows everything. B.U.T, for XGBoost, all $h$&amp;rsquo;s are &lt;a href="https://en.wikipedia.org/wiki/Decision_tree_learning" title="Decision Tree">decision trees&lt;/a>.&lt;/p>
&lt;h2 id="gradient-boosting">Gradient Boosting&lt;/h2>
&lt;blockquote>
&lt;p>All we need to do is compute $\beta$&amp;rsquo;s and $\alpha$&amp;rsquo;s&lt;/p>
&lt;/blockquote>
&lt;p>This is easier said than done.&lt;/p>
&lt;p>Joint optimisation will lead to a situation where adjusting $\alpha$ of one tree would require us to adjust $\alpha$ of another. And $\alpha$ of a decision tree determines where to add a split. This is not productive at all as we will have to throw away the existing tree and construct a whole new tree. In optimisation land this is classif case of a dis-continous objective.&lt;/p>
&lt;p>So lets fix that problem by fixing things. We first find the best parameters for $h_1$ and then never change it. Then find best parameters for $h_2$ and so on. This idea is called &amp;ldquo;Gradient Boosting&amp;rdquo; where we restrict $F$ to the family of &lt;em>additive expansion&lt;/em> is from the paper &lt;a href="https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.pdf" title="Friedman, Jerome H. &amp;quot;Greedy function approximation: a gradient boosting machine&amp;quot; In Annals of statistics , (2001)">Greedy Function Approximation: A Gradient Boosting Machine&lt;/a>. This stage wise stratergy is also very similar to &lt;a href="http://www.iro.umontreal.ca/~pift6080/H09/documents/papers/sparse/mallat_zhang_matching_pursuit.pdf" title="Mallat, Stephane G and Zhang, Zhifeng &amp;quot;Matching pursuits with time-frequency dictionaries&amp;quot; In IEEE Transactions on signal processing 41, (1993)">matching pursuit algorithm&lt;/a> in signal processing.&lt;/p>
&lt;blockquote>
&lt;p>Given an loss function $l$ and parameterised objective function $f(x;\theta)$, we can find best $\theta$ which minimises $l$ using &lt;a href="https://en.wikipedia.org/wiki/Gradient_descent" title="Gradient Descent">Gradient Descent&lt;/a>. $$\theta \gets \theta - \gamma \nabla_l f$$&lt;/p>
&lt;/blockquote>
&lt;p>Let say we are following the approach of fixing things as listed above. We are in some intermediate step $m$. We have already found and fixed the parameters of $\beta_{1:m-1}$ and $\alpha_{1:m-1}$ and the current best regressor we have is&lt;/p>
&lt;p>$$F_{t-1}(x) = \sum_{i=1}^{t-1} \beta_i h_i(x; \alpha_i)$$&lt;/p>
&lt;p>We want to add to this a $p_t(x) = \beta_t h_t(x;\alpha_t)$ and reduce the error further.&lt;/p>
&lt;p>$$ l_t = \operatorname{arg min} \sum_{i=1}^{N} \operatorname{L}(y_i, F_{t-1}(x)+f_t(x))$$&lt;/p>
&lt;p>In this situation we can find best parameters for $f_t$ using gradient descent on function space. But we need $\frac{\partial l_t}{\partial f_t}$.
Let ignore a few things a write this situation simply as&lt;/p>
&lt;p>$$l = \operatorname{L}(y, p+f(x))$$&lt;/p>
&lt;p>If we squint our eyes, r.h.s looks like a function with a fixed point $p$ and a small delta $f(x)$. We can expand it around the fix point using second order &lt;a href="https://en.wikipedia.org/wiki/Taylor_series" title="Taylor's Series">taylor&amp;rsquo;s series&lt;/a> approximation.&lt;/p>
&lt;p>$$l = \operatorname{L}(y, p) + \nabla_p \operatorname{L}(y,p) f(x) + \nabla_p^2 \operatorname{L}(y,p) \frac{f(x)^2}{2} $$&lt;/p>
&lt;p>This immediatly gives us an opening to get the derivative of loss w.r.to $f(x)$.&lt;/p>
&lt;p>$$\frac{\partial l}{\partial f} =0 + \nabla_p \operatorname{L}(y,p) + \nabla_p^2 \operatorname{L}(y,p) f(x) = \nabla_p^2 \left( \frac{\nabla_p}{\nabla_p^2} + f(x)\right)$$&lt;/p>
&lt;p>Since optimum occurs at the saddle point, the optimum $f(x)$ is the one which makes the derivative zero. So at $\frac{\partial l}{\partial f} =0$ we have
$$f(x) = - \frac{\nabla_p}{\nabla_p^2}$$&lt;/p>
&lt;p>Ultimately, to find the best additive function to the model at stage $t$, we simply have to fit $h_t(x)$ to predict the residuals $\{-{\nabla_p}/{\nabla_p^2} \}$. This again is another regression problem.&lt;/p>
&lt;p>$$h_t \leftrightsquigarrow \left\{\left(x_i,-{\nabla_{F_{t-1}(x_i)}}\middle/{\nabla^2_{F_{t-1}(x_i)} }\right)\right\}_{i=1}^{n}$$&lt;/p>
&lt;p>&lt;strong>Addendum&lt;/strong>: What if we use first order taylor&amp;rsquo;s series approximation instead on the loss function? What will be the residuals in the last step?&lt;/p>
&lt;h2 id="xgboost-algorithm">XGBoost Algorithm&lt;/h2>
&lt;ul>
&lt;li>We begin by setting $h_0$ to simply predict $\mathbb{E}(y)$.&lt;/li>
&lt;li>At each step we first compute the residuals
$$ r_i = -\frac{\nabla_{\bar{y}_i}\operatorname{L}(y_i, \bar{y}_i)}{\nabla^2_{\bar{y}_i}\operatorname{L}(y_i, \bar{y}_i)} ; with~\bar{y}_i = F_{t-1}(x_i)$$
&lt;em>Note:&lt;/em> Most loss functions have easy analytical form of first and second derivatives e.g. mse loss.&lt;/li>
&lt;li>Fit $h_t$ to predict $r_i$ given $x_i$.&lt;/li>
&lt;li>Compute $\beta_t$ using line search which will optimise
$$\operatorname{arg min}_{\beta_t}\operatorname{L}(y, F_{t-1} + \beta_t h_t)$$&lt;/li>
&lt;li>Update the model as
$$F_t = F_{t-1} + \beta_t h_t$$&lt;/li>
&lt;/ul>
&lt;h2 id="addendum">Addendum&lt;/h2>
&lt;p>There is a lot more to implementing XGBoost. Most of it is centered around the &amp;ldquo;fitting a new tree to residuals&amp;rdquo; part. A few scenarios that arise in this step are&lt;/p>
&lt;ul>
&lt;li>How to prevent overfitting of the intermediate tree on residuals?&lt;/li>
&lt;li>How to let user direct/control some part of the tree construction so that the complexity vs performance tradeoff can be tunable?&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://arxiv.org/pdf/1603.02754.pdf" title="Chen, Tianqi and Guestrin, Carlos &amp;quot;Xgboost: A scalable tree boosting system&amp;quot; In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining pp.785--794, (2016)">Chen et al. (2016)&lt;/a> gives an excellet account of these issues among other, but the official &lt;a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html">XGBoost library&amp;rsquo; documentation&lt;/a> is also a great source for discussion on these topics.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;reference>
&lt;small>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.pdf" title="Friedman, Jerome H. &amp;quot;Greedy function approximation: a gradient boosting machine&amp;quot; In Annals of statistics , (2001)">Friedman (2001)&lt;/a>: Friedman, Jerome H. &amp;ldquo;&lt;em>Greedy function approximation: a gradient boosting machine&lt;/em>&amp;rdquo; In Annals of statistics , (2001)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="http://www.iro.umontreal.ca/~pift6080/H09/documents/papers/sparse/mallat_zhang_matching_pursuit.pdf" title="Mallat, Stephane G and Zhang, Zhifeng &amp;quot;Matching pursuits with time-frequency dictionaries&amp;quot; In IEEE Transactions on signal processing 41, (1993)">Mallat and Zhang (1993)&lt;/a>: Mallat, Stephane G and Zhang, Zhifeng &amp;ldquo;&lt;em>Matching pursuits with time-frequency dictionaries&lt;/em>&amp;rdquo; In IEEE Transactions on signal processing 41, (1993)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://arxiv.org/pdf/1603.02754.pdf" title="Chen, Tianqi and Guestrin, Carlos &amp;quot;Xgboost: A scalable tree boosting system&amp;quot; In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining pp.785--794, (2016)">Chen et al. (2016)&lt;/a>: Chen, Tianqi and Guestrin, Carlos &amp;ldquo;&lt;em>Xgboost: A scalable tree boosting system&lt;/em>&amp;rdquo; In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining pp. 785&amp;ndash;794, (2016)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://en.wikipedia.org/wiki/Decision_tree_learning" title="Decision Tree">Decision Tree&lt;/a>&lt;/em>&lt;br>&lt;small>&lt;em>&lt;code>https://en.wikipedia.org/wiki/Decision_tree_learning&lt;/code>&lt;/em> &lt;/small>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://en.wikipedia.org/wiki/Gradient_descent" title="Gradient Descent">Gradient Descent&lt;/a>&lt;/em>&lt;br>&lt;small>&lt;em>&lt;code>https://en.wikipedia.org/wiki/Gradient_descent&lt;/code>&lt;/em> &lt;/small>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://en.wikipedia.org/wiki/Linear_regression" title="Linear Regression">Linear Regression&lt;/a>&lt;/em>&lt;br>&lt;small>&lt;em>&lt;code>https://en.wikipedia.org/wiki/Linear_regression&lt;/code>&lt;/em> &lt;/small>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://en.wikipedia.org/wiki/Taylor_series" title="Taylor's Series">Taylor series&lt;/a>&lt;/em>&lt;br>&lt;small>&lt;em>&lt;code>https://en.wikipedia.org/wiki/Taylor_series&lt;/code>&lt;/em> &lt;/small>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/small>
&lt;/reference></description></item><item><title>Sampling in a Sphere</title><link>https://daxpy.xyz/posts/sampling-in-a-sphere/</link><pubDate>Sun, 17 Mar 2024 21:44:15 +0530</pubDate><guid>https://daxpy.xyz/posts/sampling-in-a-sphere/</guid><description>&lt;h1 id="sampling-in-a-sphere">Sampling in a Sphere&lt;/h1>
&lt;p>Understanding how to generate a uniform sample of points inside a sphere takes us through a few interesting topics. So let begin with the end in mind.&lt;/p>
&lt;p>The following algorithm generates a uniform sample of points inside sphere in n dimensions.&lt;/p>
&lt;p>$$u_1, \ldots, u_{n+2} \sim \mathcal{N}(0,1)$$
$$x_1, \ldots, x_n = \frac{(u_1,\ldots, u_n)}{\sqrt{u_1^2+\ldots+u_{n+2}^{2}}}$$&lt;/p>
&lt;p>There is a lot to unpack here.&lt;/p>
&lt;ul>
&lt;li>Why are we sampling from a normal distribution to get a uniform distribution?&lt;/li>
&lt;li>Why $n+2$ ?&lt;/li>
&lt;li>Why are we dropping 2 coordinates?&lt;/li>
&lt;/ul>
&lt;h2 id="what-is-inside-a-sphere">What is inside a Sphere?&lt;/h2>
&lt;p>A sphere is defined as a set of points which are equidistant from a point known as center. For this article, we mostly deal with unit spheres. A unit n-sphere with center at origin can be defined as
$$ S^n = \{x \in \mathbb{R}^{n} ~:~ \|x\| = 1\} $$&lt;/p>
&lt;p>This includes only the periphery of the region. Or the circumference. An n-ball however is the whole region inside the n-sphere.
$$ B^n = \{x \in \mathbb{R}^{n} ~:~ \|x\| \leq 1 \}$$
Note that the $B^n$ is a closed ball so $S^n \subset B^n$.&lt;/p>
&lt;p>Some quick facts about the unit n-ball and n-sphere&lt;/p>
&lt;ul>
&lt;li>Both can be completely inscribed in the region $[-1,1]^n$&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Volume_of_an_n-ball" title="Volume of an n ball">Volume of an n-ball&lt;/a> is given by $$\operatorname{vol}[B^n] = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2}+1)}$$
This fomula simplyfies to $\pi r^2$ and $\frac{4}{3}\pi r^2$ for a circle and 3d sphere.&lt;/li>
&lt;/ul>
&lt;h2 id="when-spheres-are-circles">When Spheres are Circles&lt;/h2>
&lt;p>In 2 dimensions, there is simple technique get uniformly sampling points within a circle. Rejection sampling. Generate unifom random samples inside $[-1,1]^2$ and reject all the samples which are outside the circle.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">while&lt;/span> &lt;span style="color:#66d9ef">True&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">~&lt;/span> U[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>,&lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y &lt;span style="color:#f92672">~&lt;/span> U[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>,&lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> x&lt;span style="color:#f92672">^&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>&lt;span style="color:#f92672">+&lt;/span> y&lt;span style="color:#f92672">^&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#f92672">&amp;lt;=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> (x,y)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>However, efficiency of rejection sampling depends on how many samples are accepted; which is ratio of area of the circle to the area of the $2 \times 2$ square. and that about
$$\frac{\pi 1^2}{4} \approx 78.5\%$$&lt;/p>
&lt;p>We can extend the technique for higher dimensions but it has a critical flaw. The curse of dimensionality. The efficiency of rejection sampling for generating uniform sampling of points for n-sphere is&lt;/p>
&lt;p>$$\frac{\operatorname{vol}[B^n]}{2^{n}}$$&lt;/p>
&lt;p>In 3 dimensions the efficiency drops to $\approx 52.33\%$ and by the time we get to 10 dimensions, its $ \frac{\pi^5/5!}{2^{10}} \approx 0.249\% $. i.e we end up rejecting $99.7\%$ of the generated points. It worse further up.&lt;/p>
&lt;h2 id="on-the-circumference">On the circumference&lt;/h2>
&lt;p>If we restrict ourselves to get uniform sampling of point on the circumference of the circle, there is an easier techqnique. The technique is an inversion of &lt;a href="https://en.wikipedia.org/wiki/Marsaglia_polar_method" title="Marsaglia's polar method">Marsaglia&amp;rsquo;s polar method&lt;/a> for generating standard normal random numbers.&lt;/p>
&lt;p>In 2D the technique is&lt;/p>
&lt;p>$$u,v \sim \mathcal{N}(0,1)$$
$$x,y = \frac{u,v}{\sqrt{u^2+v^2}}$$&lt;/p>
&lt;p>It seems a bit strange why this would generate random uniform points on the circle&amp;rsquo;s circumference. Especially since we begin sampling from a normal distribution. But the proof is simple. Let take a look at the join distribution of $(u,v)$. Since both are normally distributed and independent of each other, we have&lt;/p>
&lt;p>$$f(u,v) = \left(\frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}u^2}\right) \left(\frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}v^2}\right)$$
$$= \frac{1}{2\pi} e^{\frac{1}{2} (u^2+v^2)}$$&lt;/p>
&lt;p>The joint distribution of $(u,v)$ only depends on its distance from origin ($u^2+v^2$) and is invariant to direction. The last step in the algorithm where we normalise both point by this distance projects all $(u,v)$&amp;rsquo;s to the circle&amp;rsquo;s circumference without any rotation. So they are uniformly distributed on the circle.&lt;/p>
&lt;p>We can extend the proof to n-dimensions&lt;/p>
&lt;p>$$f(u_1,\ldots, u_n) = \prod_i \left(\frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}u_i^2}\right) = \frac{1}{(2\pi)^{n/2}} e^{\frac{1}{2}\|u\|^2} $$
and so does the technique. Generating random uniform points on n-sphere is
$$u_i \sim \mathcal{N}(0,1) $$
$$ x = \frac{u}{\|u\|} $$&lt;/p>
&lt;p>But what about generating uniformly sampled points inside the circle?&lt;/p>
&lt;h2 id="from-circumference-to-the-center">From circumference to the center&lt;/h2>
&lt;blockquote>
&lt;p>Uniformly distributed points on an $S^{n+2}$ are uniformly distributed on $B^{n}$.&lt;/p>
&lt;/blockquote>
&lt;p>This elegant results first observed by &lt;a href="https://www.sciencedirect.com/science/article/pii/S0047259X10001211" title="Harman, Radoslav and Lacko, Vladim{'\i}r &amp;quot;On decompositional algorithms for uniform sampling from n-spheres and n-balls&amp;quot; In Journal of Multivariate Analysis 101, (2010)">Harman and Lacko&lt;/a> and later proved by &lt;a href="https://compneuro.uwaterloo.ca/files/publications/voelker.2017.pdf" title="Voelker, Aaron R and Gosmann, Jan and Stewart, Terrence C &amp;quot;Efficiently sampling vectors and coordinates from the n-sphere and n-ball&amp;quot; In Centre for Theoretical Neuroscience-Technical Report 1, (2017)">Voelker et al. (2017)&lt;/a>. Following this result, we can generate uniform sampling of points in $B^n$ by first generating uniform sampling of points in $S^{n+2}$ and dropping (any) 2 coordinates.&lt;/p>
&lt;p>$$u_1, \ldots, u_{n+2} \sim \mathcal{N}(0,1)$$
$$x_1, \ldots, x_n = \frac{(u_1,\ldots, u_n)}{\sqrt{u_1^2+\ldots+u_{n+2}^{2}}}$$&lt;/p>
&lt;h2 id="-but-one-last-thing">&amp;hellip; but one last thing&lt;/h2>
&lt;p>There is one more technique which uses projection from n-sphere to get uniform sampling on points on n-ball. The key obsevation is that an n-ball is a collection of n-spheres of different radius
i.e $$ B^n[1] = \bigcup_{x=0}^{1} S^n[x]$$&lt;/p>
&lt;p>So first we sample a point in $S^n[1]$ and then move the point somewhere between origin and circumference. But there is a catch. The area of $S^n[x]$ is proportional to $x^n$, so we have to normalize accordingly. So&lt;/p>
&lt;p>$$u_1, \ldots, u_{n} \sim \mathcal{N}(0,1)$$
$$r \sim \operatorname{U}[0,1]$$
$$x = r^{1/n} \frac{u}{\|u\|}$$&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;reference>
&lt;small>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://www.sciencedirect.com/science/article/pii/S0047259X10001211" title="Harman, Radoslav and Lacko, Vladim{'\i}r &amp;quot;On decompositional algorithms for uniform sampling from n-spheres and n-balls&amp;quot; In Journal of Multivariate Analysis 101, (2010)">Harman and Lacko (2010)&lt;/a>: Harman, Radoslav and Lacko, Vladim{'\i}r &amp;ldquo;&lt;em>On decompositional algorithms for uniform sampling from n-spheres and n-balls&lt;/em>&amp;rdquo; In Journal of Multivariate Analysis 101, (2010)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://compneuro.uwaterloo.ca/files/publications/voelker.2017.pdf" title="Voelker, Aaron R and Gosmann, Jan and Stewart, Terrence C &amp;quot;Efficiently sampling vectors and coordinates from the n-sphere and n-ball&amp;quot; In Centre for Theoretical Neuroscience-Technical Report 1, (2017)">Voelker et al. (2017)&lt;/a>: Voelker, Aaron R and Gosmann, Jan and Stewart, Terrence C &amp;ldquo;&lt;em>Efficiently sampling vectors and coordinates from the n-sphere and n-ball&lt;/em>&amp;rdquo; In Centre for Theoretical Neuroscience-Technical Report 1, (2017)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://en.wikipedia.org/wiki/Marsaglia_polar_method" title="Marsaglia's polar method">Marsaglia polar method&lt;/a>&lt;/em>&lt;br>&lt;small>&lt;em>&lt;code>https://en.wikipedia.org/wiki/Marsaglia_polar_method&lt;/code>&lt;/em> &lt;/small>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://en.wikipedia.org/wiki/Volume_of_an_n-ball" title="Volume of an n ball">Volume of an n ball&lt;/a>&lt;/em>&lt;br>&lt;small>&lt;em>&lt;code>https://en.wikipedia.org/wiki/Volume_of_an_n-ball&lt;/code>&lt;/em> &lt;/small>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/small>
&lt;/reference></description></item><item><title>Normalisation Layers</title><link>https://daxpy.xyz/posts/normalisation/</link><pubDate>Tue, 24 Oct 2023 18:33:10 +0530</pubDate><guid>https://daxpy.xyz/posts/normalisation/</guid><description>&lt;h1 id="normalisation">Normalisation&lt;/h1>
&lt;p>Regulating the magnitude of activations inside a neural network is crucial for an effective training regime. We may get stuck in local minima or worse yet, the training may diverge otherwise. For this, we make use of normalisation.&lt;/p>
&lt;p>Normalisation comes in two flavours. Weight normalisation and Layer normalisation. We briefly touch on some fundamental techniques in both.&lt;/p>
&lt;h2 id="weight-normalisation">Weight normalisation&lt;/h2>
&lt;p>In weight normalisation, we focus on the magnitude of the parameters of the network; preventing them from uncontrollable growth or collapse. The basic technique appeared in &lt;a href="https://arxiv.org/pdf/1602.07868.pdf" title="Salimans, Tim and Kingma, Durk P &amp;quot;Weight normalization: A simple reparameterization to accelerate training of deep neural networks&amp;quot; In Advances in neural information processing systems 29, (2016)">Salimans et al. (2016)&lt;/a> which describes a simple scenario.&lt;/p>
&lt;p>Given a network layer&lt;/p>
&lt;p>$$y = \phi(w \cdot x+b)$$&lt;/p>
&lt;p>We constrain the weight by reparameterising it as&lt;/p>
&lt;p>$$w = g\frac{v}{\|v\|}$$&lt;/p>
&lt;p>This constraints the norm of $w$ to be always $g$, the growth parameter.&lt;/p>
&lt;h3 id="spectral-normalization">Spectral Normalization&lt;/h3>
&lt;p>If we consider the parameter of the layer/network as matrix, then we can device normalisation schemes using matrix norms. One such technique is spectral norm. It was first described in &lt;a href="https://arxiv.org/pdf/1802.05957.pdf" title="Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi &amp;quot;Spectral normalization for generative adversarial networks&amp;quot; In arXiv preprint arXiv:1802.05957 , (2018)">Miyato et al. (2018)&lt;/a>.&lt;/p>
&lt;p>Given the weight matrix, we normalise it by its spectral norm&lt;/p>
&lt;p>$$W \gets \frac{W}{\sigma(W)}$$&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Matrix_norm" title="Matrix Norm">Spectral norm&lt;/a> is an induced vector norm defined as&lt;/p>
&lt;p>$$\sigma(A) = max_{h \neq 0}\frac{\|Ah\|_ 2}{\|h\|_ 2}$$&lt;/p>
&lt;p>To recall, this is the formulation of the largest singular value of $A$ and hence, spectral norm stabilizes the &lt;a href="https://en.wikipedia.org/wiki/Lipschitz_continuity" title="Lipschitz Continuity">Lipschitz constant&lt;/a> of the weight matrix.&lt;/p>
&lt;p>Spectral norm had found its initial use in &lt;a href="https://arxiv.org/pdf/1802.05957.pdf" title="Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi &amp;quot;Spectral normalization for generative adversarial networks&amp;quot; In arXiv preprint arXiv:1802.05957 , (2018)">Generative Adversarial Networks (GANs)&lt;/a> for stabilising training by preventing mode collapse.&lt;/p>
&lt;h2 id="normalisation-layers">Normalisation Layers&lt;/h2>
&lt;p>Normalisation layers are like any other layers in a neural network which takes applied transformations on its input. But their primary purpose is to stablise the activations. They are places in the computational graph such that the further layers recieve a more stable inputs due to their action.&lt;/p>
&lt;h3 id="batch-normalisation">Batch Normalisation&lt;/h3>
&lt;p>The key idea behind batch norm is to normalise each feature of a sample. Ideally, if $x$ is the input feature, then the best normalisation is achieved if we do
$$x_f = \frac{x_f - \mathbb{E}[x_f]}{\sqrt{\operatorname{Var}[x_f]}}$$&lt;/p>
&lt;p>But we don&amp;rsquo;t have access to the exact population mean and variance. So we approximate it at run time with the mean and variance of the features in the mini-batch. Hence the name &amp;ldquo;batch norm&amp;rdquo;.&lt;/p>
&lt;p>Batch norm was introduced by &lt;a href="http://proceedings.mlr.press/v37/ioffe15.pdf" title="Ioffe, Sergey and Szegedy, Christian &amp;quot;Batch normalization: Accelerating deep network training by reducing internal covariate shift&amp;quot; In , (2015)">Ioffe et al. (2015)&lt;/a> and it has has 4 learnable parameters, each of size $F$, the number of features.&lt;/p>
&lt;p>$$y \gets \operatorname{BatchNorm}(x; \{\bar{\mu}, \bar{\sigma}^2, \gamma, \beta \})$$
After (and during) training, the layer converges its output to $\mathcal{N}(\beta, \gamma^2I)$. The layer does the following transforms during the forward pass of the training.&lt;/p>
&lt;p>$$y_{b,f} = \gamma_{f}\left( \frac{x_{b,f}-\mu_{f}}{\sqrt{\sigma^2_{f}-\epsilon}} \right) + \beta_{f}$$
$$\mu_{f} \gets \frac{1}{B}\sum_b x_{b,f}$$
$$\sigma^2_{f} \gets \frac{1}{B} \sum_t (x_{b,f}-\mu_{f})^2$$&lt;/p>
&lt;p>During inference batch statistics are not available. So batch norm keeps track of running average of the batch statistics during training in $\bar{\mu}$ and $\bar{\sigma}^2$. These are used during inference as mean and variance.&lt;/p>
&lt;p>$$\bar{\mu}_{f} \gets \operatorname{running-mean}(\mu_{f})$$
$$\bar{\sigma}_{f}^2 \gets \operatorname{running-mean}(\sigma^2_{f})$$&lt;/p>
&lt;p>Each of the features are normalised independently using batch statistics. The $\epsilon$ is constant added for numerical stability.&lt;/p>
&lt;p>&lt;strong>Why does batch norm work?&lt;/strong>&lt;/p>
&lt;p>&lt;em>Internal co-variate shift&lt;/em> was the original reasoning for batch norm discussed in &lt;a href="http://proceedings.mlr.press/v37/ioffe15.pdf" title="Ioffe, Sergey and Szegedy, Christian &amp;quot;Batch normalization: Accelerating deep network training by reducing internal covariate shift&amp;quot; In , (2015)">Ioffe et al. (2015)&lt;/a>. During training, when the lower layer&amp;rsquo;s parameter changes, the output distribution changes. This leads to the following layers needing to constantly readjust its parameters.&lt;/p>
&lt;p>There has been some evidence against the internal covariate shift theory. See &lt;a href="https://arxiv.org/pdf/1805.11604.pdf" title="Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander &amp;quot;How does batch normalization help optimization?&amp;quot; In Advances in neural information processing systems 31, (2018)">Santurkar et al. (2018)&lt;/a>&lt;/p>
&lt;h3 id="layer-normalisation">Layer Normalisation&lt;/h3>
&lt;p>While batch norm normalises each features independently, Layer norm normalises the total activation of a sample. Since we do not need to keep track of batch statistics, it only has 2 scalar parameters.&lt;/p>
&lt;p>$$y \gets \operatorname{LayerNorm}(x; \{\gamma, \beta \})$$&lt;/p>
&lt;p>Layer norm was introduces in &lt;a href="https://arxiv.org/abs/1607.06450" title="Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E &amp;quot;Layer normalization&amp;quot; In arXiv preprint arXiv:1607.06450 , (2016)">Lei Ba et al. (2016)&lt;/a> with the following definitions.&lt;/p>
&lt;p>$$y_{b,f} = \gamma \left(\frac{x_{b,f}-\mu_{b}}{\sqrt{\sigma^2_{b}-\epsilon}}\right) + \beta$$&lt;/p>
&lt;p>$$\mu_{b} = \frac{1}{F}\sum_f x_{b,f}$$
$$\sigma^2_{b} = \frac{1}{F} \sum_f (x_{b,f}-\mu_f)^2$$&lt;/p>
&lt;p>Layer normalization is used in recurrent neural networks (RNNs) like architectures to address vanishing gradient problems.&lt;/p>
&lt;blockquote>
&lt;p>In a standard RNN, there is a tendency for the average magnitude of the summed inputs to the recurrent units to either grow or shrink at every time-step, leading to exploding or vanishing gradients. In a layer normalized RNN, the normalization terms make it invariant to re-scaling all of the summed inputs to a layer, which results in much more stable hidden-to-hidden dynamics. ~ &lt;em>from &lt;a href="https://arxiv.org/abs/1607.06450" title="Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E &amp;quot;Layer normalization&amp;quot; In arXiv preprint arXiv:1607.06450 , (2016)">Lei Ba et al. (2016)&lt;/a>&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>Since layer norm does not have any contribution from different samples of a same batch, it does the same computation during inference as well. For the same reason, it is also distributed training friendly as gradients depend only on the single sample.&lt;/p>
&lt;h2 id="rmsnorm">RMSNorm&lt;/h2>
&lt;p>&lt;a href="https://arxiv.org/pdf/1910.07467.pdf" title="Zhang, Biao and Sennrich, Rico &amp;quot;Root mean square layer normalization&amp;quot; In Advances in Neural Information Processing Systems 32, (2019)">Root Mean Square Layer Normalization&lt;/a> is a variant of Layer norm. The difference is that instead of normalising the data by mean centering and dividing by variance, the data is simply divided by RMS.&lt;/p>
&lt;p>$$y \gets \operatorname{RMSNorm}(x; \{\gamma\})$$&lt;/p>
&lt;p>RMS measures the quadratic mean of inputs.&lt;/p>
&lt;p>$$\operatorname{RMS}(x_b) = \sqrt{\frac{1}{F}\sum_f x_{b,f}^2}$$&lt;/p>
&lt;p>Then RMS norm is computed as&lt;/p>
&lt;p>$$y_{b,f} = \gamma_f \frac{x_{b,f}}{\operatorname{RMS}(x_b)}$$&lt;/p>
&lt;p>The main difference to LayerNorm is that RMSNorm is not re-centered and thus does not show similar linearity property for variable shifting.&lt;/p>
&lt;p>RMSNorm forces the summed inputs into a âˆšn-scaled unit sphere. By doing so, the output distribution remains same regardless of the scaling of input. RMSNorm is invariant to the scaling of its inputs. It is not invariant to all re-centering operations.&lt;/p>
&lt;h2 id="normalisation-layers-for-data-with-channels">Normalisation Layers for data with channels&lt;/h2>
&lt;h3 id="instance-normalization">Instance Normalization&lt;/h3>
&lt;p>For data with multiple channels, applying a normalisation technique like layer normalisation might result in one of the channels being saturated. For such scenario, we use &lt;a href="https://arxiv.org/abs/1607.08022" title="Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor &amp;quot;Instance normalization: The missing ingredient for fast stylization&amp;quot; In arXiv preprint arXiv:1607.08022 , (2016)">Instance Normalization&lt;/a>.&lt;/p>
&lt;p>The technique is similar to layer normalization, but the mean and variance statistics are calculated for each channels and instance independently. Instance normalization is commonly used in style transfer and image generation tasks.&lt;/p>
&lt;h3 id="group-normalization">Group Normalization&lt;/h3>
&lt;p>&lt;a href="https://arxiv.org/abs/1803.08494" title="Wu, Yuxin and He, Kaiming &amp;quot;Group normalization&amp;quot; In Proceedings of the European conference on computer vision (ECCV) pp.3--19, (2018)">Group Normalization&lt;/a> is a variant of Instance normalisation which attempts to strike a balance between Layer norm and Instance norm. The channels of a layer are divided into groups, and mean and variance statistics are computed for each group separately.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;reference>
&lt;small>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="http://proceedings.mlr.press/v37/ioffe15.pdf" title="Ioffe, Sergey and Szegedy, Christian &amp;quot;Batch normalization: Accelerating deep network training by reducing internal covariate shift&amp;quot; In , (2015)">Ioffe et al. (2015)&lt;/a>: Ioffe, Sergey and Szegedy, Christian &amp;ldquo;&lt;em>Batch normalization: Accelerating deep network training by reducing internal covariate shift&lt;/em>&amp;rdquo; In , (2015)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://arxiv.org/abs/1607.06450" title="Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E &amp;quot;Layer normalization&amp;quot; In arXiv preprint arXiv:1607.06450 , (2016)">Lei Ba et al. (2016)&lt;/a>: Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E &amp;ldquo;&lt;em>Layer normalization&lt;/em>&amp;rdquo; In arXiv preprint arXiv:1607.06450 , (2016)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://arxiv.org/pdf/1802.05957.pdf" title="Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi &amp;quot;Spectral normalization for generative adversarial networks&amp;quot; In arXiv preprint arXiv:1802.05957 , (2018)">Miyato et al. (2018)&lt;/a>: Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi &amp;ldquo;&lt;em>Spectral normalization for generative adversarial networks&lt;/em>&amp;rdquo; In arXiv preprint arXiv:1802.05957 , (2018)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://arxiv.org/pdf/1602.07868.pdf" title="Salimans, Tim and Kingma, Durk P &amp;quot;Weight normalization: A simple reparameterization to accelerate training of deep neural networks&amp;quot; In Advances in neural information processing systems 29, (2016)">Salimans et al. (2016)&lt;/a>: Salimans, Tim and Kingma, Durk P &amp;ldquo;&lt;em>Weight normalization: A simple reparameterization to accelerate training of deep neural networks&lt;/em>&amp;rdquo; In Advances in neural information processing systems 29, (2016)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://arxiv.org/pdf/1805.11604.pdf" title="Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander &amp;quot;How does batch normalization help optimization?&amp;quot; In Advances in neural information processing systems 31, (2018)">Santurkar et al. (2018)&lt;/a>: Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander &amp;ldquo;&lt;em>How does batch normalization help optimization?&lt;/em>&amp;rdquo; In Advances in neural information processing systems 31, (2018)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://arxiv.org/abs/1607.08022" title="Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor &amp;quot;Instance normalization: The missing ingredient for fast stylization&amp;quot; In arXiv preprint arXiv:1607.08022 , (2016)">Ulyanov et al. (2016)&lt;/a>: Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor &amp;ldquo;&lt;em>Instance normalization: The missing ingredient for fast stylization&lt;/em>&amp;rdquo; In arXiv preprint arXiv:1607.08022 , (2016)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://arxiv.org/pdf/1902.08129" title="Yang, Greg and Pennington, Jeffrey and Rao, Vinay and Sohl-Dickstein, Jascha and Schoenholz, Samuel S &amp;quot;A mean field theory of batch normalization&amp;quot; In arXiv preprint arXiv:1902.08129 , (2019)">Yang et al. (2019)&lt;/a>: Yang, Greg and Pennington, Jeffrey and Rao, Vinay and Sohl-Dickstein, Jascha and Schoenholz, Samuel S &amp;ldquo;&lt;em>A mean field theory of batch normalization&lt;/em>&amp;rdquo; In arXiv preprint arXiv:1902.08129 , (2019)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://arxiv.org/pdf/1910.07467.pdf" title="Zhang, Biao and Sennrich, Rico &amp;quot;Root mean square layer normalization&amp;quot; In Advances in Neural Information Processing Systems 32, (2019)">Zhang et al. (2019)&lt;/a>: Zhang, Biao and Sennrich, Rico &amp;ldquo;&lt;em>Root mean square layer normalization&lt;/em>&amp;rdquo; In Advances in Neural Information Processing Systems 32, (2019)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://arxiv.org/abs/1803.08494" title="Wu, Yuxin and He, Kaiming &amp;quot;Group normalization&amp;quot; In Proceedings of the European conference on computer vision (ECCV) pp.3--19, (2018)">Wu et al. (2018)&lt;/a>: Wu, Yuxin and He, Kaiming &amp;ldquo;&lt;em>Group normalization&lt;/em>&amp;rdquo; In Proceedings of the European conference on computer vision (ECCV) pp. 3&amp;ndash;19, (2018)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://en.wikipedia.org/wiki/Lipschitz_continuity" title="Lipschitz Continuity">Lipschitz Continuity&lt;/a>&lt;/em>&lt;br>&lt;small>&lt;em>&lt;code>https://en.wikipedia.org/wiki/Lipschitz_continuity&lt;/code>&lt;/em> [accessed - Oct 2023]&lt;/small>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://en.wikipedia.org/wiki/Matrix_norm" title="Matrix Norm">Matrix Norm&lt;/a>&lt;/em>&lt;br>&lt;small>&lt;em>&lt;code>https://en.wikipedia.org/wiki/Matrix_norm&lt;/code>&lt;/em> [accessed - Oct 2023]&lt;/small>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/small>
&lt;/reference></description></item><item><title>Variational Inference</title><link>https://daxpy.xyz/posts/variational-inference/</link><pubDate>Sun, 16 Apr 2023 12:15:00 +0530</pubDate><guid>https://daxpy.xyz/posts/variational-inference/</guid><description>&lt;h1 id="variational-inference">Variational Inference&lt;/h1>
&lt;p>We have some data from a population and we suspect that the it is generated by some underlying process. Estimating the process which generates the data allows us to understand its fundamental properties.&lt;/p>
&lt;p>Concretely, $p(x)$ is the distribution of the data and $z$ are its latent variables, the process which generates the data is $p(x|z)$.
Estimating the generation process is computing the true posterior $p(z|x)$. This is the (posterior) inference problem.&lt;/p>
&lt;p>From Bayes rule, we have
$$p(z|x) = \frac{p(x,z)}{p(x)}$$
with $p(x) = \int_z p(x,z)$. But this is often intractable as the quantities on RHS are non-trivial to estimate.&lt;/p>
&lt;p>Variational inference is the technique which helps in converting this estimation problem into an optimization problem by approximating the posterior $p(z|x)$ with a family of simpler distributions $q_v(z)$. The best approximation is found by minimizing the &lt;a href="https://en.wikipedia.org/wiki/Divergence_(statistics)" title="Divergence">divergence&lt;/a> between $q$ and $p$.&lt;/p>
&lt;p>$$ \min_v D_{KL}[q_v(z), p(z|x)] $$&lt;/p>
&lt;p>$q_v$ is a family of distribution and a specific member is selected by $v$. In this context, $v$&amp;rsquo;s are called &lt;em>variational parameters&lt;/em>.&lt;/p>
&lt;p>There are many different &lt;a href="https://en.wikipedia.org/wiki/Divergence_(statistics)" title="Divergence">divergence&lt;/a> measures. But we are going to stick with &lt;a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" title="KL Divergence">KL divergence&lt;/a> as it has a straight forward definition. Note that it is asymmetric.&lt;/p>
&lt;p>$$D_{KL}(p,q) = \int_{\Omega} p \log\frac{p}{q}$$&lt;/p>
&lt;p>Although this sounds promising and simple in theory, in practise we run into some difficulties. Optimising &lt;a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" title="KL Divergence">KL divergence&lt;/a> is practically difficult. The gradient is readily available for simpler distributions and in practise $p$ can be very complex. This will force us to use very simple $q$ and have a bad approximations.&lt;/p>
&lt;p>Let see if we can optimise KL divergence indirectly.&lt;/p>
&lt;p>Consider this. $\log p(x)$ is called &lt;em>evidence&lt;/em> or &lt;em>log-evidence&lt;/em> of $x$ and is considered a constant. Since $\int_z q_v(z)=1$ we can write&lt;/p>
&lt;p>$$\log p(x) = \log p(x) \int_z q_v(z)$$&lt;/p>
&lt;p>Since $p(x)$ is invariant to the w.r.to $z$, we have,&lt;/p>
&lt;p>$$\log p(x) = \int_z q_v(z) \log p(x)$$&lt;/p>
&lt;p>A bit of clever substitutions and wrangling later,&lt;/p>
&lt;p>$$ \log p(x) =\int_z q_v(z) \log \frac{p(x,z)}{q_v(z)} - \int_z q_v(z) \log \frac{p(z|x)}{q_v(z)}$$&lt;/p>
&lt;p>Thus we have the following system.&lt;/p>
&lt;p>$$\log p(x) = L(q,p) - D_{KL}(q,p)$$&lt;/p>
&lt;p>$\log p(x)$ is constant and KL divergence is always $\geq 0$. The $L(q,p)$ is thus lower bound on the approximation between $p$ and $q$. It is hence called &lt;em>&lt;a href="https://en.wikipedia.org/wiki/Evidence_lower_bound" title="Evidence Lower Bound">Evidence Lower Bound&lt;/a>&lt;/em> or ELBO.&lt;/p>
&lt;p>Since a constant is equal to difference between 2 variables, maximising ELBO decreases the KL divergence making approximation of $q$ to $p$ better. Now we have an alternate way to find the best approximation.&lt;/p>
&lt;h2 id="evidence-lower-bound">Evidence Lower Bound&lt;/h2>
&lt;p>One easy way to understand effect of optimising evidence lower bound is to think of it as a combination of 2 entropies.
$$L(q,p) = H(q) - H(q,p)$$
While maximizing $L$, the entropy term is pushing $q$ to spread everywhere while the negative cross entropy term is pushing $q$ to concentrate on regions where $p$ has high density.
However, for using as a training objective, we need its gradient.&lt;/p>
&lt;h2 id="black-box-variational-inference">Black Box Variational Inference&lt;/h2>
&lt;p>ELBO has the following equivalent form.&lt;/p>
&lt;p>$$L(q,p) = \mathbb{E}_{q_v(z)} \left [ \log p(x,z) - \log q_v(z) \right ]$$&lt;/p>
&lt;p>&lt;a href="http://www.cs.columbia.edu/~blei/papers/RanganathGerrishBlei2014.pdf" title="Rajesh Ranganath, Sean Gerrish, and David Blei. &amp;quot;Black box variational inference.&amp;quot; Artificial intelligence and statistics. PMLR, 2014.">Ranganath et al. (2014)&lt;/a> gives the following form of gradient.
$$\nabla_v L(v) = \mathbb{E}_{q_v(z)} \left [ \nabla_v \log q_v(z)\, (\log p(x,z) - \log q_v(z) )\right ]$$&lt;/p>
&lt;p>Gradient of the log of probability distribution is called score function and this gradient is known as &lt;strong>score gradient&lt;/strong>. When the gradient is in form of an expectation of a random variable, we take monte-carlo samples and to get &lt;a href="https://en.wikipedia.org/wiki/Stochastic_approximation" title="Stochastic Approximation">approximate gradient&lt;/a>.&lt;/p>
&lt;p>Also note that the gradient assumes no knowledge of the model except that we can evaluate the quantities in the equation. More of less, a &lt;em>Black Box Variational Inference&lt;/em>.&lt;/p>
&lt;p>Unfortunately, the approximation of the gradient has high variance. Fortunately, &lt;a href="http://www.cs.columbia.edu/~blei/papers/RanganathGerrishBlei2014.pdf" title="Rajesh Ranganath, Sean Gerrish, and David Blei. &amp;quot;Black box variational inference.&amp;quot; Artificial intelligence and statistics. PMLR, 2014.">Ranganath et al. (2014)&lt;/a> also describes a few variance reduction techniques.&lt;/p>
&lt;h2 id="reprameterisation-gradient">Reprameterisation gradient&lt;/h2>
&lt;p>Score gradient allows us to use complex distributions to approximate posterior distribution. But it is difficult to sample from complex distributions. &lt;a href="https://arxiv.org/abs/1312.6114" title="Kingma, Diederik P., and Max Welling. &amp;quot;Auto-encoding variational bayes.&amp;quot; arXiv preprint arXiv:1312.6114 (2013).">Reparametrisation trick&lt;/a> allows us to create complex distributions from simple ones.&lt;/p>
&lt;p>Say we are able to write $z = t(\epsilon, v)$ with
$\epsilon \sim s(\epsilon)$, a simple distribution which we can sample from.
What we have done is to bound all &amp;ldquo;randomness&amp;rdquo; to $s(\epsilon)$ and made $q_v(z)$ &amp;ldquo;non-random&amp;rdquo;.&lt;/p>
&lt;p>With this, we have a simpler gradient for elbo.&lt;/p>
&lt;p>$$\nabla_v L(v) = \mathbb{E}_{s(\epsilon)}\big[ \nabla_z \left[ \log p(x,z) - \log q_v(z) \right] \nabla_v t(\epsilon, v) \big]$$&lt;/p>
&lt;p>To compute approximate gradient, take samples $\epsilon \sim s(\epsilon)$, compute $z = t(\epsilon, v)$ and then evaluate the reparameterization gradient.
$\log p(x,z) - \log q_v(z)$ is the model and $\nabla_z(\cdots)$ can be evaluated using auto-differentiation. See &lt;a href="http://www.cs.columbia.edu/~blei/papers/RuizTitsiasBlei2016b.pdf" title="Ruiz, Francisco R., Titsias RC AUEB, and David Blei. &amp;quot;The generalized reparameterization gradient.&amp;quot; Advances in neural information processing systems 29 (2016).">Ruiz et al. (2016)&lt;/a> for more discussion on reprameterisation gradient.&lt;/p>
&lt;h2 id="amortised-variational-inference">Amortised Variational Inference&lt;/h2>
&lt;p>Variational Inference is still not scalable. We still have to fit the variational parameter for each observations essentially minimising KL divergence between each $q_v(z)$ and $p(z|x_i)$. This is not scalable and will lead to over fitting.&lt;/p>
&lt;p>We can get around this by constrianing the variational parameters to be a function of the observations; a learnable function.&lt;/p>
&lt;p>$$v = g_\phi(x)$$&lt;/p>
&lt;p>In &lt;a href="https://www.tuananhle.co.uk/notes/amortized-inference.html" title="Amortized Inference by Tuan Anh Le">Amortised Inference&lt;/a>, the variational parameters are output of a network which takes the samples from true distribution as input. The parameters of the network can be learned by optimising reparameterisation gradient.&lt;/p>
&lt;p>$$\nabla_\phi L(\phi) = \mathbb{E}_{s(\epsilon)}\big[ \nabla_z \left[ \log p(x,z) - \log q_v(z) \right] \nabla_\phi t(\epsilon, g_\phi(x)) \big]$$&lt;/p>
&lt;h2 id="variational-auto-encoders">Variational Auto Encoders&lt;/h2>
&lt;p>&lt;a href="https://arxiv.org/abs/1312.6114" title="Kingma, Diederik P., and Max Welling. &amp;quot;Auto-encoding variational bayes.&amp;quot; arXiv preprint arXiv:1312.6114 (2013).">Variational auto encoders&lt;/a> are a generative model which learns to generate data from its true distribution. It has an architecture simlar to a &lt;a href="https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf" title="Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P. A., &amp;amp; Bottou, L. (2010). &amp;quot;Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion&amp;quot;. Journal of machine learning research, 11(12).">denoising auto encoder&lt;/a> and uses variational inference to learn the distribution.&lt;/p>
&lt;p>Variational auto encoders make a constraint that the posterior is approximated by an gaussian distribution with diagonal covariances. As a result, the latent representation will have linearly independent dimensions.&lt;/p>
&lt;p>The encoder is a differentiable network which is used to approximate posterior distribution $p(z|x)$. The network is trained to predict the parameters of the approximating distribution from a data point.&lt;/p>
&lt;p>$$\mu, \sigma = g_{\phi}(x)$$&lt;/p>
&lt;p>then, the posterior is approximated by the distribution
$$ q_{\mu, \sigma}(z)= \mathcal{N}(z; \mu, diag(\sigma^2))$$&lt;/p>
&lt;p>The latent vector is obtained by sampling from $q_{\mu, \sigma}(z)$.&lt;/p>
&lt;p>The decoder is also a differentiable network which is trained to predict a sample from the latent vector.
$$\hat{x} = f_{\theta}(z)$$&lt;/p>
&lt;p>Different samples generate different predictions thus generating samples from $p(x|z)$. Some designs explicitly adds &amp;ldquo;intelligent noise&amp;rdquo; to aid in directed distribution. $\hat{x} = f_{\theta}(z+\epsilon)$&lt;/p>
&lt;p>The forward inference of variational auto encoder defined in the &lt;a href="https://arxiv.org/abs/1312.6114" title="Kingma, Diederik P., and Max Welling. &amp;quot;Auto-encoding variational bayes.&amp;quot; arXiv preprint arXiv:1312.6114 (2013).">Kingma et al. (2013)&lt;/a> has the following form.&lt;/p>
&lt;p>$$(\mu, \log \sigma) = g_\phi(x) $$
$$q_{\mu,\sigma}(z|x) = \mathcal{N} (z; \mu, diag(\sigma^2))$$
$$z \sim q_{\mu,\sigma}(z|x)$$
$$ \hat{x} = f_{\theta}(z)$$&lt;/p>
&lt;p>Both the encoder and decoder are neural networks and $\phi$ and $\theta$ are their parameters. Both the networks are trained end to end to miniminse the following loss.
$$L
= C\|x - f(z) \|^2 + D_{KL}(\mathcal{N}(\mu, diag(\sigma^2), \mathcal{N}(0,I))$$&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;reference>
&lt;small>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://arxiv.org/abs/1601.00670" title="Blei, David M., Alp Kucukelbir, and Jon D. McAuliffe. &amp;quot;Variational inference: A review for statisticians.&amp;quot; Journal of the American statistical Association 112.518 (2017): 859-877.">Blei et al. (2017)&lt;/a>: Blei, David M., Alp Kucukelbir, and Jon D. McAuliffe. &amp;ldquo;&lt;em>Variational inference: A review for statisticians.&lt;/em>&amp;rdquo; Journal of the American statistical Association 112.518 (2017): 859-877.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://arxiv.org/abs/1312.6114" title="Kingma, Diederik P., and Max Welling. &amp;quot;Auto-encoding variational bayes.&amp;quot; arXiv preprint arXiv:1312.6114 (2013).">Kingma et al. (2013)&lt;/a>: Kingma, Diederik P., and Max Welling. &amp;ldquo;&lt;em>Auto-encoding variational bayes.&lt;/em>&amp;rdquo; arXiv preprint arXiv:1312.6114 (2013).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://arxiv.org/pdf/1906.02691.pdf" title="Diederik P. Kingma and Max Welling (2019), &amp;quot;An Introduction to Variational Autoencoders&amp;quot;, Foundations and Trends in Machine Learning: Vol. xx, No.xx, pp 1â€“18. DOI: 10.1561/XXXXXXXXX.">Kingma et al. (2019)&lt;/a>: Diederik P. Kingma and Max Welling (2019), &amp;ldquo;&lt;em>An Introduction to Variational Autoencoders&lt;/em>&amp;rdquo;, Foundations and Trends in Machine Learning: Vol. xx, No.xx, pp 1â€“18. DOI: 10.1561/XXXXXXXXX.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="http://www.cs.columbia.edu/~blei/papers/RanganathGerrishBlei2014.pdf" title="Rajesh Ranganath, Sean Gerrish, and David Blei. &amp;quot;Black box variational inference.&amp;quot; Artificial intelligence and statistics. PMLR, 2014.">Ranganath et al. (2014)&lt;/a>: Rajesh Ranganath, Sean Gerrish, and David Blei. &amp;ldquo;&lt;em>Black box variational inference.&lt;/em>&amp;rdquo; Artificial intelligence and statistics. PMLR, 2014.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://arxiv.org/pdf/1401.4082.pdf" title="Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. &amp;quot;Stochastic backpropagation and approximate inference in deep generative models.&amp;quot; International conference on machine learning. PMLR, 2014.">Rezende et al. (2014)&lt;/a>: Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. &amp;ldquo;&lt;em>Stochastic backpropagation and approximate inference in deep generative models.&lt;/em>&amp;rdquo; International conference on machine learning. PMLR, 2014.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="http://www.cs.columbia.edu/~blei/papers/RuizTitsiasBlei2016b.pdf" title="Ruiz, Francisco R., Titsias RC AUEB, and David Blei. &amp;quot;The generalized reparameterization gradient.&amp;quot; Advances in neural information processing systems 29 (2016).">Ruiz et al. (2016)&lt;/a>: Ruiz, Francisco R., Titsias RC AUEB, and David Blei. &amp;ldquo;&lt;em>The generalized reparameterization gradient.&lt;/em>&amp;rdquo; Advances in neural information processing systems 29 (2016).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf" title="Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P. A., &amp;amp; Bottou, L. (2010). &amp;quot;Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion&amp;quot;. Journal of machine learning research, 11(12).">Vincent et al. (2010)&lt;/a>: Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P. A., &amp;amp; Bottou, L. (2010). &amp;ldquo;&lt;em>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion&lt;/em>&amp;rdquo;. Journal of machine learning research, 11(12).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://www.tuananhle.co.uk/notes/amortized-inference.html" title="Amortized Inference by Tuan Anh Le">Amortized Inference&lt;/a>&lt;/em> by Tuan Anh Le&lt;br>&lt;small>&lt;em>&lt;code>https://www.tuananhle.co.uk/notes/amortized-inference.html&lt;/code>&lt;/em>&lt;/small>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://erdogdu.github.io/csc412/notes/lec11-1.pdf" title="Amortized Inference and Variational Auto Encoders">Amortized Inference and Variational Auto Encoders&lt;/a>&lt;/em>&lt;br>&lt;small>&lt;em>&lt;code>https://erdogdu.github.io/csc412/notes/lec11-1.pdf&lt;/code>&lt;/em>&lt;/small>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://en.wikipedia.org/wiki/Divergence_(statistics)" title="Divergence">Divergence&lt;/a>&lt;/em>&lt;br>&lt;small>&lt;em>&lt;code>https://en.wikipedia.org/wiki/Divergence_(statistics)&lt;/code>&lt;/em>&lt;/small>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://en.wikipedia.org/wiki/Evidence_lower_bound" title="Evidence Lower Bound">Evidence Lower Bound&lt;/a>&lt;/em>&lt;br>&lt;small>&lt;em>&lt;code>https://en.wikipedia.org/wiki/Evidence_lower_bound&lt;/code>&lt;/em>&lt;/small>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" title="KL Divergence">KL Divergence&lt;/a>&lt;/em>&lt;br>&lt;small>&lt;em>&lt;code>https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&lt;/code>&lt;/em>&lt;/small>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://en.wikipedia.org/wiki/Stochastic_approximation" title="Stochastic Approximation">Stochastic Approximation&lt;/a>&lt;/em>&lt;br>&lt;small>&lt;em>&lt;code>https://en.wikipedia.org/wiki/Stochastic_approximation&lt;/code>&lt;/em>&lt;/small>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://ermongroup.github.io/cs228-notes/extras/vae" title="The variational auto-encoder">The variational auto-encoder&lt;/a>&lt;/em>&lt;br>&lt;small>&lt;em>&lt;code>https://ermongroup.github.io/cs228-notes/extras/vae&lt;/code>&lt;/em>&lt;/small>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://www.depthfirstlearning.com/2021/VI-with-NFs" title="Variational Inference with Normalizing Flows">Variational Inference with Normalizing Flows&lt;/a>&lt;/em>&lt;br>&lt;small>&lt;em>&lt;code>https://www.depthfirstlearning.com/2021/VI-with-NFs&lt;/code>&lt;/em>&lt;/small>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://ermongroup.github.io/cs228-notes/inference/variational" title="Variational inference">Variational inference&lt;/a>&lt;/em>&lt;br>&lt;small>&lt;em>&lt;code>https://ermongroup.github.io/cs228-notes/inference/variational&lt;/code>&lt;/em>&lt;/small>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>&lt;a href="https://www.youtube.com/watch?v=Dv86zdWjJKQ" title="Variational Inference: Foundations and Innovations by David Blei">Variational Inference: Foundations and Innovations by David Blei&lt;/a>&lt;/em>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/small>
&lt;/reference></description></item><item><title>Finding Median</title><link>https://daxpy.xyz/posts/finding-median/</link><pubDate>Tue, 01 Mar 2022 06:59:51 +0530</pubDate><guid>https://daxpy.xyz/posts/finding-median/</guid><description>&lt;h1 id="finding-median">Finding Median&lt;/h1>
&lt;p>&lt;em>Here is my &lt;a href="https://www.dictionary.com/e/slang/eli5/">ELI5&lt;/a> definition of a median.&lt;/em>&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;Median is the middle number when numbers are sorted&amp;rdquo;.&lt;/p>
&lt;/blockquote>
&lt;p>There is only a single middle number when the size list is odd. But if the size is even, there are 2 middle numbers. Then we take an average of those 2 numbers to be the median.&lt;/p>
&lt;p>Median is useful when your data doesn&amp;rsquo;t behave. Medians are part of &amp;ldquo;robust statistics&amp;rdquo; because they are not affected by outliers. Both $[1,2,100]$ and $[1,2,3]$ have 2 as their median while their means differ widely. You can see why medians are not affected by noise.&lt;/p>
&lt;p>This post is all about computing medians from a list of numbers. The straightforward approach is to sort everything first and the middle element is the median. But can we do better?&lt;/p>
&lt;h2 id="quick-select">Quick Select&lt;/h2>
&lt;p>Quick select is similar to quicksort except that it doesn&amp;rsquo;t sort the array. It semi sorts the array.&lt;/p>
&lt;p>Given an element &amp;lsquo;pivot&amp;rsquo;, quick select partitions the array such all the smaller elements are moved to the left of the pivot and all the larger elements are moved to the right; essentially partition the array into two. The partition procedure can be implemented using either &lt;a href="https://en.wikipedia.org/wiki/Quicksort#Lomuto_partition_scheme">Lomuto&lt;/a> or &lt;a href="https://en.wikipedia.org/wiki/Quicksort#Hoare_partition_scheme">Hoare&lt;/a> scheme, either run in linear time.&lt;/p>
&lt;p>Quick select takes a number $k$ as a parameter and finds the $k^{th}$ smallest number in the array. As a side effect, the first $k$ locations of the array contains the $k$ smallest elements after the procedure.&lt;/p>
&lt;p>For finding the median, we can use quick selection by setting $k$ to the middle location.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">quick_select&lt;/span>(A, k):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> low, high &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>, len(A)&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> &lt;span style="color:#66d9ef">True&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pivot &lt;span style="color:#f92672">=&lt;/span> random_index(low, high)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> index &lt;span style="color:#f92672">=&lt;/span> partition(A, low, high, A[pivot])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> index &lt;span style="color:#f92672">==&lt;/span> k:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> A[:k]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> index &lt;span style="color:#f92672">&amp;lt;&lt;/span> k:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> left &lt;span style="color:#f92672">=&lt;/span> index &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> right &lt;span style="color:#f92672">=&lt;/span> index &lt;span style="color:#f92672">-&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>size &lt;span style="color:#f92672">=&lt;/span> len(A) &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>m1 &lt;span style="color:#f92672">=&lt;/span> quick_select(A, size&lt;span style="color:#f92672">//&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>m2 &lt;span style="color:#f92672">=&lt;/span> quick_select(A, size &lt;span style="color:#f92672">-&lt;/span> size&lt;span style="color:#f92672">//&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>median &lt;span style="color:#f92672">=&lt;/span> (m1&lt;span style="color:#f92672">+&lt;/span>m2)&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">2.0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The best-case and average-case complexity for quick select is both $O(n)$ but in the worst case, it takes $O(n^2)$ time. The worst case occurs when all the pivots we select results in an &lt;em>unfavourable&lt;/em> partition of the array.&lt;/p>
&lt;p>Is there any way to guarantee that the pivot we select results in a &lt;em>favourable&lt;/em> split?&lt;/p>
&lt;h2 id="median-of-medians">Median of Medians&lt;/h2>
&lt;p>The algorithm is easier to describe. Split the large list into smaller lists. Find medians of the smaller lists(recursively) and then find the median of those medians. Use this number as the pivot in the partition function of quick select.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">median_small&lt;/span>(A):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> size &lt;span style="color:#f92672">=&lt;/span> len(A) &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> B &lt;span style="color:#f92672">=&lt;/span> sorted(A)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> m1 &lt;span style="color:#f92672">=&lt;/span> B[size&lt;span style="color:#f92672">//&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> m2 &lt;span style="color:#f92672">=&lt;/span> B[size &lt;span style="color:#f92672">-&lt;/span> size&lt;span style="color:#f92672">//&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> (m1&lt;span style="color:#f92672">+&lt;/span>m2)&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">2.0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">median_of_medians&lt;/span>(A, d &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">5&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> n &lt;span style="color:#f92672">=&lt;/span> len(A)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> n &lt;span style="color:#f92672">&amp;lt;=&lt;/span> d:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> median_small(A)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> medians &lt;span style="color:#f92672">=&lt;/span> [median_small(A[i:i&lt;span style="color:#f92672">+&lt;/span>d]) &lt;span style="color:#66d9ef">for&lt;/span> i &lt;span style="color:#f92672">in&lt;/span> range(&lt;span style="color:#ae81ff">0&lt;/span>, n, d)]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pivot &lt;span style="color:#f92672">=&lt;/span> median_of_medians(medians, d)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> pivot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">quick_select&lt;/span>(A, k):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> low, high &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>, len(A)&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> &lt;span style="color:#66d9ef">True&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pivot &lt;span style="color:#f92672">=&lt;/span> median_of_medians(A[low:high])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> index &lt;span style="color:#f92672">=&lt;/span> partition(A, low, high, pivot)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> index &lt;span style="color:#f92672">==&lt;/span> k:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> A[:k]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> index &lt;span style="color:#f92672">&amp;lt;&lt;/span> k:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> left &lt;span style="color:#f92672">=&lt;/span> index &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> right &lt;span style="color:#f92672">=&lt;/span> index &lt;span style="color:#f92672">-&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>size &lt;span style="color:#f92672">=&lt;/span> len(A) &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>m1 &lt;span style="color:#f92672">=&lt;/span> quick_select(A, size&lt;span style="color:#f92672">//&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>m2 &lt;span style="color:#f92672">=&lt;/span> quick_select(A, size &lt;span style="color:#f92672">-&lt;/span> size&lt;span style="color:#f92672">//&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>median &lt;span style="color:#f92672">=&lt;/span> (m1&lt;span style="color:#f92672">+&lt;/span>m2)&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">2.0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Median of medians does not compute the real median. But it does give us something close. A guarantee that the selected pivot will always partition the array &lt;em>favourably&lt;/em>.&lt;/p>
&lt;p>A randomly selected element may end up partitioning the array &lt;em>unfavourably&lt;/em> (i.e very little or no elements in one of the partitions). The recursion does not help reduce the running time in such a case. But what about the median of medians?&lt;/p>
&lt;p>If $S_i$ is the $i^{th}$ segment of array $A$ with $|A|=n$, $|S_i|\geq 5$, $m_i=median(S_i)$, $m=median(m_1,m_2,\dots)$, we can infer that&lt;/p>
&lt;ol>
&lt;li>Atleast 2 element in $S_i$ are smaller than $m_i$&lt;/li>
&lt;li>Atleast half of $m_i$&amp;rsquo;s are smaller than $m$&lt;/li>
&lt;/ol>
&lt;p>Which implies at least $\frac{n/5}{2}\cdot 2=\frac{n}{5}$ elements of $A$ are smaller than $m$. Similarly, at least $\frac{n}{5}$ elements in $A$ which are greater than $m$. So splitting the array with $m$ as pivot always results in a &lt;em>favourable&lt;/em> split of the array.&lt;/p>
&lt;p>The run time of median of median procedure is also linear.
$$T(n) = T(n/5)+ n/5 = O(n)$$&lt;/p>
&lt;p>So if we use the median of medians for selecting pivot, the worst-case performance of quick select is improved to $O(n \log n)$.&lt;/p>
&lt;p>But then why a magical segment size of $5$? Why not 3? Would a larger number work? Read more &lt;a href="https://en.wikipedia.org/wiki/Median_of_medians">here&lt;/a>&lt;/p>
&lt;h2 id="median-of-sorted-arrays">Median of sorted arrays&lt;/h2>
&lt;p>What if the numbers are in smaller sorted arrays and we need to find the median of the full list?&lt;/p>
&lt;p>Here is the strategy. Given a pivot, we can count numbers smaller than the pivot in each of the sorted arrays using binary search. The smallest number for which this total count is $k$ is the $k^{th}$ smallest number in the whole collection. To find such a number, we can use binary search on the total range of numbers.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">find_kth&lt;/span>(A:List[List[int]], k:int) &lt;span style="color:#f92672">-&amp;gt;&lt;/span> float:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> len(A)&lt;span style="color:#f92672">==&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>: &lt;span style="color:#66d9ef">return&lt;/span> A[&lt;span style="color:#ae81ff">0&lt;/span>][k&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lo &lt;span style="color:#f92672">=&lt;/span> min(a[&lt;span style="color:#ae81ff">0&lt;/span>] &lt;span style="color:#66d9ef">for&lt;/span> a &lt;span style="color:#f92672">in&lt;/span> A)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hi &lt;span style="color:#f92672">=&lt;/span> max(a[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>] &lt;span style="color:#66d9ef">for&lt;/span> a &lt;span style="color:#f92672">in&lt;/span> A)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> lo &lt;span style="color:#f92672">&amp;lt;&lt;/span> hi:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mid &lt;span style="color:#f92672">=&lt;/span> lo &lt;span style="color:#f92672">+&lt;/span> (hi &lt;span style="color:#f92672">-&lt;/span> lo)&lt;span style="color:#f92672">//&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> count &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> a &lt;span style="color:#f92672">in&lt;/span> A:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> count &lt;span style="color:#f92672">+=&lt;/span> bisect&lt;span style="color:#f92672">.&lt;/span>bisect_right(a, mid)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> count &lt;span style="color:#f92672">&amp;lt;&lt;/span> k:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lo &lt;span style="color:#f92672">=&lt;/span> mid &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hi &lt;span style="color:#f92672">=&lt;/span> mid
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> hi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">median_sorted&lt;/span>(A:List[List[int]]) &lt;span style="color:#f92672">-&amp;gt;&lt;/span> float:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> A &lt;span style="color:#f92672">=&lt;/span> [a &lt;span style="color:#66d9ef">for&lt;/span> a &lt;span style="color:#f92672">in&lt;/span> A &lt;span style="color:#66d9ef">if&lt;/span> a] &lt;span style="color:#75715e"># remove all empty arrays&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> size &lt;span style="color:#f92672">=&lt;/span> sum(len(a) &lt;span style="color:#66d9ef">for&lt;/span> a &lt;span style="color:#f92672">in&lt;/span> A) &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> m1 &lt;span style="color:#f92672">=&lt;/span> find_kth(A, size&lt;span style="color:#f92672">//&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> m2 &lt;span style="color:#f92672">=&lt;/span> find_kth(A, size &lt;span style="color:#f92672">-&lt;/span> size&lt;span style="color:#f92672">//&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> (m1&lt;span style="color:#f92672">+&lt;/span>m2)&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">2.0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The time complexity of this approach is $O(m\log n)$ where $m$ is no of arrays and $n$ is the size of the largest array.&lt;/p>
&lt;p>&lt;em>To be precise, the time complexity is $O(b\cdot m \log n)$ where $b$ is the total number of bits used to represent the numbers. The difference between &lt;code>hi&lt;/code> and &lt;code>lo&lt;/code> can be as large as $2^{b}$ and we are doing a binary search in that range. So, the number of steps the while loop runs is bounded by $O(b)$. In a realistic scenario, $b$ is always a constant like 32 or 64.&lt;/em>&lt;/p>
&lt;h2 id="median-with-updates">Median with updates&lt;/h2>
&lt;p>So far we have been talking about arrays of fixed sizes. What if the array grows over time with insertions? How do you find the median instantaneously?&lt;/p>
&lt;p>Essentially, we have to support 2 operations. &lt;code>insert&lt;/code> and &lt;code>find_median&lt;/code>. If insertion is a simple append operation, then finding the median would take $O(n)$ steps. If we always insert while maintaining sorted order which takes $O(n)$ steps, then finding the median would just be a lookup. Can we do better?&lt;/p>
&lt;p>We maintain 2 heaps; a max heap and a min-heap. The max heap always maintains the smaller half of elements while the min-heap maintains the larger half. The roots of both the heaps would be the middle element(s).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> heapq
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>L, R &lt;span style="color:#f92672">=&lt;/span> [], []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">insert&lt;/span>(num) &lt;span style="color:#f92672">-&amp;gt;&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> heapq&lt;span style="color:#f92672">.&lt;/span>heappush(R, num)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> itm &lt;span style="color:#f92672">=&lt;/span> heapq&lt;span style="color:#f92672">.&lt;/span>heappop(R)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> heapq&lt;span style="color:#f92672">.&lt;/span>heappush(L, &lt;span style="color:#f92672">-&lt;/span>itm)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> len(R) &lt;span style="color:#f92672">&amp;lt;&lt;/span> len(L):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># invariant: |R|-|L| = 0/1 &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> itm &lt;span style="color:#f92672">=&lt;/span> heapq&lt;span style="color:#f92672">.&lt;/span>heappop(L)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> heapq&lt;span style="color:#f92672">.&lt;/span>heappush(R, &lt;span style="color:#f92672">-&lt;/span>itm)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">find_median&lt;/span>() &lt;span style="color:#f92672">-&amp;gt;&lt;/span> float:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> len(L) &lt;span style="color:#f92672">==&lt;/span> len(R):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> (&lt;span style="color:#f92672">-&lt;/span>L[&lt;span style="color:#ae81ff">0&lt;/span>] &lt;span style="color:#f92672">+&lt;/span> R[&lt;span style="color:#ae81ff">0&lt;/span>])&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> R[&lt;span style="color:#ae81ff">0&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This scheme incurs a $O(\log n)$ steps for &lt;code>insert&lt;/code> and $O(1)$ for &lt;code>find_median&lt;/code>.
&lt;em>We do a bit of hack by inserting the negative of an element to simulate a max heap from a min-heap&lt;/em>.&lt;/p>
&lt;h2 id="median-of-a-distribution">Median of a distribution&lt;/h2>
&lt;p>Let&amp;rsquo;s say we have lots of numbers. But most of them are in a small range. How do you compute the median then?&lt;/p>
&lt;p>We can store the statistics of the numbers by maintaining their frequencies in the valid range. For the numbers beyond the range, we store only the total counts (one for each end). This allows fast inserts and fast median finding operations. Let&amp;rsquo;s see a sample implementation.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>freq &lt;span style="color:#f92672">=&lt;/span> Counter()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>low, high &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">?&lt;/span>, &lt;span style="color:#960050;background-color:#1e0010">?&lt;/span> &lt;span style="color:#75715e"># set the limits here&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>low_counter, count, high_counter &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">insert&lt;/span>(num) &lt;span style="color:#f92672">-&amp;gt;&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> num &lt;span style="color:#f92672">&amp;lt;&lt;/span> low:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> low_counter &lt;span style="color:#f92672">+=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">elif&lt;/span> num &lt;span style="color:#f92672">&amp;gt;&lt;/span> high:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> high_counter &lt;span style="color:#f92672">+=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> freq[num] &lt;span style="color:#f92672">+=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> count &lt;span style="color:#f92672">+=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">find_kth&lt;/span>(k) &lt;span style="color:#f92672">-&amp;gt;&lt;/span> float:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> count &lt;span style="color:#f92672">=&lt;/span> low_counter
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> n &lt;span style="color:#f92672">in&lt;/span> range(low, high&lt;span style="color:#f92672">+&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> count &lt;span style="color:#f92672">&amp;lt;=&lt;/span> k &lt;span style="color:#f92672">&amp;lt;=&lt;/span> (count &lt;span style="color:#f92672">+&lt;/span> freq[n]):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> n
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> count &lt;span style="color:#f92672">+=&lt;/span> freq[n]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> float(&lt;span style="color:#e6db74">&amp;#39;nan&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">find_median&lt;/span>() &lt;span style="color:#f92672">-&amp;gt;&lt;/span> float:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> size &lt;span style="color:#f92672">=&lt;/span> low_counter &lt;span style="color:#f92672">+&lt;/span> count &lt;span style="color:#f92672">+&lt;/span> high_counter &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> m1 &lt;span style="color:#f92672">=&lt;/span> find_kth(size&lt;span style="color:#f92672">//&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> m2 &lt;span style="color:#f92672">=&lt;/span> find_kth(size &lt;span style="color:#f92672">-&lt;/span> size&lt;span style="color:#f92672">//&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> (m1 &lt;span style="color:#f92672">+&lt;/span> m2)&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">2.0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Do you know any other weird scenarios where you need median computed? Let me know.&lt;/p></description></item><item><title>Binary Search</title><link>https://daxpy.xyz/posts/binary-search-problems/</link><pubDate>Fri, 28 Jan 2022 06:59:51 +0530</pubDate><guid>https://daxpy.xyz/posts/binary-search-problems/</guid><description>&lt;h1 id="binary-search">Binary Search&lt;/h1>
&lt;p>Binary search is more than just a search algorithm for sorted arrays. It&amp;rsquo;s an algorithm which keeps showing up as optimal solutions in unlikely places. This note is a very limited exploration of what binary search can do.&lt;/p>
&lt;p>Let&amp;rsquo;s begin by talking about vanilla binary search.&lt;/p>
&lt;h2 id="binary-search-1">Binary Search&lt;/h2>
&lt;p>We are given a sorted array of numbers and a target. Binary search is the most optimal way of finding position of target in the array if present.&lt;/p>
&lt;p>Binary search starts by having the entire array as a search space. It then progressively compares the middle element with the target, eliminating half pf search space as not needing further exploration w.r.to the relativeness of target and middle element.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">binary_search&lt;/span>(nums, target):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> low, high &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>, len(nums)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> low &lt;span style="color:#f92672">&amp;lt;&lt;/span> high:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mid &lt;span style="color:#f92672">=&lt;/span> low &lt;span style="color:#f92672">+&lt;/span> (high&lt;span style="color:#f92672">-&lt;/span>low)&lt;span style="color:#f92672">//&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> nums[mid] &lt;span style="color:#f92672">==&lt;/span> target:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> mid
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> nums[mid] &lt;span style="color:#f92672">&amp;lt;&lt;/span> target:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> low &lt;span style="color:#f92672">=&lt;/span> mid&lt;span style="color:#f92672">+&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> high &lt;span style="color:#f92672">=&lt;/span> mid
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The basic structure of binary search can be used to solve many other seemingly different problems. A one line abstraction of such problems is&lt;/p>
&lt;blockquote>
&lt;p>Find a lowest value in a range which is feasible&lt;/p>
&lt;/blockquote>
&lt;p>Lets describe the search in sorted array problem in this framework.&lt;/p>
&lt;p>Instead of trying to find the location of target, let us recast the problem as the smallest index in the which contains elements which are larger than or equal to target. Note that this is no longer solving the search problem exactly. The difference in when target is not present in the array.&lt;/p>
&lt;p>In this description, an index in the array is feasible if the element at the index is larger than or equal to target&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">feasible&lt;/span>(index, nums, target):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> (nums[index]&lt;span style="color:#f92672">&amp;gt;=&lt;/span>target)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">binary_search&lt;/span>(nums, target):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> low, high &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>, len(nums)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> low &lt;span style="color:#f92672">&amp;lt;&lt;/span> high:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mid &lt;span style="color:#f92672">=&lt;/span> low &lt;span style="color:#f92672">+&lt;/span> (high&lt;span style="color:#f92672">-&lt;/span>low)&lt;span style="color:#f92672">//&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> feasible(mid, nums, target):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> high &lt;span style="color:#f92672">=&lt;/span> mid
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> low &lt;span style="color:#f92672">=&lt;/span> mid&lt;span style="color:#f92672">+&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> low
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This template can be quickly extended to solve a few other problems.&lt;/p>
&lt;h2 id="split-array-largest-sum">Split array largest sum&lt;/h2>
&lt;p>Given an array which consists of non-negative integers, split array into M non-empty contiguous sub-arrays such that the largest sum of the segments is minimum.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">feasible&lt;/span>(threshold, M) &lt;span style="color:#f92672">-&amp;gt;&lt;/span> bool:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> count, total &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> num &lt;span style="color:#f92672">in&lt;/span> nums:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> total &lt;span style="color:#f92672">+=&lt;/span> num
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> total &lt;span style="color:#f92672">&amp;gt;&lt;/span> threshold:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> total &lt;span style="color:#f92672">=&lt;/span> num
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> count &lt;span style="color:#f92672">+=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> count &lt;span style="color:#f92672">&amp;gt;&lt;/span> M:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#66d9ef">False&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#66d9ef">True&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">binary_search&lt;/span>(nums) &lt;span style="color:#f92672">-&amp;gt;&lt;/span> int:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> low, high &lt;span style="color:#f92672">=&lt;/span> max(nums), sum(nums)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> low &lt;span style="color:#f92672">&amp;lt;&lt;/span> high:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mid &lt;span style="color:#f92672">=&lt;/span> low &lt;span style="color:#f92672">+&lt;/span> (high &lt;span style="color:#f92672">-&lt;/span> low)&lt;span style="color:#f92672">//&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> feasible(mid, M):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> high &lt;span style="color:#f92672">=&lt;/span> mid
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> low &lt;span style="color:#f92672">=&lt;/span> mid &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> low
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now lets look at another problem with similar structure.&lt;/p>
&lt;h2 id="median-in-a-row-wise-sorted-matrix">Median in a row wise sorted Matrix&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">binary_median&lt;/span>(A):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> m, n &lt;span style="color:#f92672">=&lt;/span> len(A),len(A[&lt;span style="color:#ae81ff">0&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> low &lt;span style="color:#f92672">=&lt;/span> min(A[i][ &lt;span style="color:#ae81ff">0&lt;/span>] &lt;span style="color:#66d9ef">for&lt;/span> i &lt;span style="color:#f92672">in&lt;/span> range(m))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> high &lt;span style="color:#f92672">=&lt;/span> max(A[i][&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>] &lt;span style="color:#66d9ef">for&lt;/span> i &lt;span style="color:#f92672">in&lt;/span> range(m))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> median_loc &lt;span style="color:#f92672">=&lt;/span> (m &lt;span style="color:#f92672">*&lt;/span> n &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>) &lt;span style="color:#f92672">//&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> low &lt;span style="color:#f92672">&amp;lt;&lt;/span> high:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mid &lt;span style="color:#f92672">=&lt;/span> low &lt;span style="color:#f92672">+&lt;/span> (high &lt;span style="color:#f92672">-&lt;/span> low) &lt;span style="color:#f92672">//&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> count &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> i &lt;span style="color:#f92672">in&lt;/span> range(m):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> count &lt;span style="color:#f92672">+=&lt;/span> upper_bound(A[i], mid)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> count &lt;span style="color:#f92672">&amp;lt;&lt;/span> median_loc:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> low &lt;span style="color:#f92672">=&lt;/span> mid &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> high &lt;span style="color:#f92672">=&lt;/span> mid
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> high &lt;span style="color:#75715e"># is median&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="square-root-of-a-number">Square root of a number&lt;/h2>
&lt;p>Binary search can also used to find roots of an equations. Let us demonstrate how it is used to find square root of a number.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">square_root&lt;/span>(x, tolerance&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1e-4&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> low, high &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>,x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> (high&lt;span style="color:#f92672">-&lt;/span>low) &lt;span style="color:#f92672">&amp;gt;&lt;/span> tolerance:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mid &lt;span style="color:#f92672">=&lt;/span> low &lt;span style="color:#f92672">+&lt;/span> (high &lt;span style="color:#f92672">-&lt;/span> low)&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">2.0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> mid &lt;span style="color:#f92672">*&lt;/span> mid &lt;span style="color:#f92672">&amp;lt;=&lt;/span> x:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> low &lt;span style="color:#f92672">=&lt;/span> mid
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> high &lt;span style="color:#f92672">=&lt;/span> mid
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> low
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="find-minimum-in-rotated-sorted-array-with-no-duplicates">Find Minimum in Rotated Sorted Array With No Duplicates&lt;/h2>
&lt;p>\url{https://www.topcoder.com/thrive/articles/Binary%20Search}&lt;/p>
&lt;h2 id="median-of-2-sorted-arrays">Median of 2 sorted arrays&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">Solution&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">findMedianSortedArrays&lt;/span>(self, A: List[int], B: List[int]) &lt;span style="color:#f92672">-&amp;gt;&lt;/span> float:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> m, n &lt;span style="color:#f92672">=&lt;/span> len(A), len(B)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> length, mid &lt;span style="color:#f92672">=&lt;/span> (m&lt;span style="color:#f92672">+&lt;/span>n&lt;span style="color:#f92672">+&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>), (m&lt;span style="color:#f92672">+&lt;/span>n&lt;span style="color:#f92672">+&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>)&lt;span style="color:#f92672">//&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> m1 &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>find_kth(A,B, mid)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> m2 &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>find_kth(A,B, length &lt;span style="color:#f92672">-&lt;/span> mid)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> (m1&lt;span style="color:#f92672">+&lt;/span>m2)&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">2.0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">find_kth&lt;/span>(self, A, B, k):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#f92672">not&lt;/span> A: &lt;span style="color:#66d9ef">return&lt;/span> B[k&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#f92672">not&lt;/span> B: &lt;span style="color:#66d9ef">return&lt;/span> A[k&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lo, hi &lt;span style="color:#f92672">=&lt;/span> min(A[&lt;span style="color:#ae81ff">0&lt;/span>], B[&lt;span style="color:#ae81ff">0&lt;/span>]), max(A[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>], B[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> lo &lt;span style="color:#f92672">&amp;lt;&lt;/span> hi:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mid &lt;span style="color:#f92672">=&lt;/span> lo &lt;span style="color:#f92672">+&lt;/span> (hi &lt;span style="color:#f92672">-&lt;/span> lo)&lt;span style="color:#f92672">//&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> a_md &lt;span style="color:#f92672">=&lt;/span> bisect&lt;span style="color:#f92672">.&lt;/span>bisect_right(A, mid)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> b_md &lt;span style="color:#f92672">=&lt;/span> bisect&lt;span style="color:#f92672">.&lt;/span>bisect_right(B, mid)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> a_md &lt;span style="color:#f92672">+&lt;/span> b_md &lt;span style="color:#f92672">&amp;lt;&lt;/span> k:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lo &lt;span style="color:#f92672">=&lt;/span> mid &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hi &lt;span style="color:#f92672">=&lt;/span> mid
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> hi
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Differentiable Computations</title><link>https://daxpy.xyz/posts/differentiable-computations/</link><pubDate>Sun, 18 Feb 2018 00:00:00 +0000</pubDate><guid>https://daxpy.xyz/posts/differentiable-computations/</guid><description>&lt;h1 id="differentiable-computations">Differentiable Computations&lt;/h1>
&lt;p>Auto-grad or automatic gradient computation is a nice feature found in many computational frameworks. Specify the computation in forward direction and the framework computes backward gradients. Let&amp;rsquo;s talk about the generic method to do this.&lt;/p>
&lt;p>Let&amp;rsquo;s say we have to compute the result of &amp;lsquo;something&amp;rsquo;. It may be a
nasty heat equation or some logic driven steps to get from input to
output. Abstracting the steps involved gives us a sequence of equations
$$\begin{aligned}
z_i = f_i(z_{a(i)})\end{aligned}$$&lt;/p>
&lt;p>The $z$&amp;rsquo;s are intermediate variables of the computation steps or they may be parameters. The selections $z_{a(i)}$ are inputs to $f_i$.&lt;/p>
&lt;p>&lt;strong>What does gradient of this sequence of computation mean?&lt;/strong> If is the
final step of the computation, then computing gradients of the sequence
means $\frac{\partial z_n}{\partial z_i}$ are the gradients if $z_n=f_n(z_{a(n)})$ is the final step. Computing all those gradients gives us how parameters change w.r.to the output.&lt;/p>
&lt;h2 id="handling-branches-and-loops">Handling Branches and loops&lt;/h2>
&lt;p>For any general computation to be included, we need to talk about
branches and loops. How are these handled in our model?&lt;/p>
&lt;p>Loops could be unrolled in to a sequence of functions. All of them would
simply share a same parameters, but inputs will be output of the
function representing previous iteration. For example&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>begin loop 1:3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x = x + c
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>end
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>can be unrolled as
$$\begin{aligned}
x_1 &amp;amp;= x + c
\\
x_2 &amp;amp;= x_1 + c
\\
x_3 &amp;amp;= x_2 + c
\end{aligned}$$&lt;/p>
&lt;p>Unrolling doesn&amp;rsquo;t work for infinite loops because the unrolling will never end. Infinite loops has no business in real world computation. If a loop cannot be unrolled even after applying the &amp;ldquo;reality of the universe&amp;rdquo;, we are not talking about a computational system. It might be an event loop or a queue. Neither needs gradients!&lt;/p>
&lt;h2 id="forward-computation-as-a-constrained-optimisation-problem">Forward computation as a Constrained optimisation problem&lt;/h2>
&lt;p>Without loss of generality, we can say that all this hoopla of computing gradient is to minimise the final value. Even if this is not the case, like for example, if maximising the final result was the intent, then append a negating function at the end of the sequence. There are many other techniques out there to convert different problems to a minimisation problem.&lt;/p>
&lt;p>Now we have &lt;em>that&lt;/em> out of the way, lets look at the following problem.&lt;/p>
&lt;p>$$\begin{aligned}
&amp;amp;\min{z_n}
\\
s.t~z_i &amp;amp;= f_i(z_{a(i)})
\end{aligned}$$&lt;/p>
&lt;p>The formulation if a little bit weird. All it is saying is, minimise $z_n$ such that, outputs of computations ($f_i$) are inputs to some other computation (all $f$&amp;rsquo;s which has $z_i$ as input). Constraints are maintaining integrity of the sequence. So we managed to represent same thing is two ways, each saying the same thing. Great!&lt;/p>
&lt;h2 id="how-do-you-solve-a-constrained-optimisation-problem">How do you solve a constrained optimisation problem?&lt;/h2>
&lt;p>Using the method of Lagrange multipliers. It basically says that once we define Lagrange&amp;rsquo;s function&lt;/p>
&lt;p>$$\begin{aligned}
L(z,\lambda) = z_n - \sum_i\lambda_i(z_i - f_i(z_{a(i)}))
\end{aligned}$$&lt;/p>
&lt;p>These $L$&amp;rsquo;s gradient w.r.to its parameters vanishes at optimum points of original function as well. So we get&lt;/p>
&lt;p>$$\begin{aligned}
\nabla_{\lambda_i}=0 &amp;amp;\implies z_i = f_i(z_{a(i)})
\\
\nabla_{z_n}=0 &amp;amp;\implies \lambda_n = 1
\\
\nabla_{z_i}=0 &amp;amp;\implies \lambda_i = \sum_{k\in b(i)}\lambda_k \frac{\partial f_k}{\partial z_i}
\end{aligned}$$&lt;/p>
&lt;p>Final expression of $\lambda_i$&amp;rsquo;s will give $\frac{\partial z_n}{\partial z_i}$ and hence all the gradients of our original computation. $b(\cdot)$ is like inverse of $a(\cdot)$. $a(i)$ gives which $z$&amp;rsquo;s are arguments of $f_i$ while $b(i)$ simply gives which
$f$ has $z_i$ as an argument. $b=a^{-1}$ ??. Anyway, these equations
fits nicely as a linear system&lt;/p>
&lt;p>$$\begin{aligned}
A \lambda &amp;amp;= \begin{bmatrix}
0
\\
\vdots
\\
0
\\
-1
\end{bmatrix}
\quad
&amp;amp; A_{k,i} &amp;amp;=
\begin{cases}
\frac{\partial f_k}{\partial z_i} &amp;amp; k\in b(i)
\\
-1 &amp;amp; k=i
\\
0 &amp;amp; otherwise
\end{cases}
\end{aligned}$$&lt;/p>
&lt;p>$A$ is an upper triangular matrix with $1$&amp;rsquo;s on
the diagonal. Otherwise, we are looking at sequence of computation which
needs result of a future. That is too complicated for now(example of
explicit systems).&lt;/p>
&lt;p>This linear system of equations opens up myriad of possibilities of
computing gradients faster. The simplest of which is back substitution
since $A$ is triangular. If the computation we are dealing with is a
forward pass of a neural network, what we get out of the back
substitution is &amp;ldquo;backprop&amp;rdquo; algorithm!!&lt;/p>
&lt;h2 id="deriving-backprop-in-a-weird-way">Deriving backprop, in a weird way&lt;/h2>
&lt;p>Lets look at a very simple Neural network&lt;/p>
&lt;p>$$\begin{aligned}
a_1 &amp;amp;= \sigma(x w_1)
\\
a_2 &amp;amp;= \operatorname{sofmax}(a_1 w_2)
\\
l &amp;amp;= \operatorname{loss}(a_2,y)
\end{aligned}$$&lt;/p>
&lt;p>If we simplify (ahem!) it up according to our problem, we get&lt;/p>
&lt;p>$$\begin{aligned}
z_1&amp;amp;=x,~ z_2=y, z_3=w_1, z_4=w_2
\\
z_5 &amp;amp;= z_1z_3
\\
z_6 &amp;amp;= \sigma(z_5)
\\
z_7 &amp;amp;= z_6z_4
\\
z_8 &amp;amp;= \operatorname{softmax}(z_7)
\\
z_9 &amp;amp;= \operatorname{loss}(z_8,z_2)
\end{aligned}$$&lt;/p>
&lt;p>This gives us the linear system&lt;/p>
&lt;p>$$\begin{aligned}
\begin{bmatrix}
\\-1 &amp;amp; &amp;amp; &amp;amp; &amp;amp; \frac{\partial f_{5}}{\partial z_{1}} &amp;amp; &amp;amp; &amp;amp; &amp;amp;&lt;br>
\\ &amp;amp;-1 &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; \frac{\partial f_{9}}{\partial z_{2}}
\\ &amp;amp; &amp;amp;-1 &amp;amp; &amp;amp; \frac{\partial f_{5}}{\partial z_{3}} &amp;amp; &amp;amp; &amp;amp; &amp;amp;&lt;br>
\\ &amp;amp; &amp;amp; &amp;amp;-1 &amp;amp; &amp;amp; &amp;amp; \frac{\partial f_{7}}{\partial z_{4}} &amp;amp; &amp;amp;&lt;br>
\\ &amp;amp; &amp;amp; &amp;amp; &amp;amp;-1 &amp;amp; \frac{\partial f_{6}}{\partial z_{5}} &amp;amp; &amp;amp; &amp;amp;&lt;br>
\\ &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp;-1 &amp;amp; \frac{\partial f_{7}}{\partial z_{6}} &amp;amp; &amp;amp;&lt;br>
\\ &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp;-1 &amp;amp; \frac{\partial f_{8}}{\partial z_{7}} &amp;amp;&lt;br>
\\ &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp;-1 &amp;amp; \frac{\partial f_{9}}{\partial z_{8}}
\\ &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; -1 &amp;amp;
\end{bmatrix}
\begin{bmatrix}
\lambda_{1}\\
\lambda_{2}\\
\lambda_{3}\\
\lambda_{4}\\
\lambda_{5}\\
\lambda_{6}\\
\lambda_{7}\\
\lambda_{8}\\
\lambda_{9}\\
\end{bmatrix} = \begin{bmatrix}
0\\
0\\
0\\
0\\
0\\
0\\
0\\
0\\
-1
\end{bmatrix}
\end{aligned}$$&lt;/p>
&lt;p>Apply back substitution and we get&lt;/p>
&lt;p>$$\begin{aligned}
\lambda_3 &amp;amp;= \lambda_5 \frac{\partial f_5}{\partial z_3}\quad
\lambda_4 = \lambda_7 \frac{\partial f_7}{\partial z_4}\\
\lambda_5 &amp;amp;= \lambda_6 \frac{\partial f_6}{\partial z_6}\quad
\lambda_6 = \lambda_7 \frac{\partial f_7}{\partial z_6}\\
\lambda_7 &amp;amp;= \lambda_8 \frac{\partial f_8}{\partial z_7}\quad
\lambda_8 = \lambda_9 \frac{\partial f_9}{\partial z_8}\\
\lambda_3 &amp;amp;= \frac{\partial l}{\partial z_8} \frac{\partial z_8}{\partial z_7} \frac{\partial z_7}{\partial z_6} \frac{\partial z_6}{\partial z_6} \frac{\partial z_5}{\partial w_1}\\
\lambda_4 &amp;amp;= \frac{\partial l}{\partial z_8} \frac{\partial z_8}{\partial z_7} \frac{\partial z_7}{\partial w_2}
\end{aligned}$$&lt;/p>
&lt;p>and there it is!! $\lambda_3$ is the gradient for parameter $w_1$ and
$\lambda_4$ represent the gradient of $w_2$.&lt;/p>
&lt;p>Now the structure of matrix $A$ for this problem isn&amp;rsquo;t that interesting.
The example network is very simple. Almost too simple. The computational
graph is almost a line graph. But with more interesting cases, like for
example, &lt;a href="https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf">inception&lt;/a>
architecture, the matrix will have very nice structure. A very
particular example is dense block from
&lt;a href="https://arxiv.org/abs/1608.06993">DenseNet&lt;/a>. The matrix will have a
fully filled upper triangular.&lt;/p>
&lt;p>&lt;strong>Attribution:&lt;/strong>&lt;/p>
&lt;p>&lt;em>I had my first encounter with the constrained optimisation view of
computation in Yann LeCunn&amp;rsquo;s 1988 paper&lt;/em>&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;&lt;em>A Theoretical Framework for
back propagation&lt;/em>&amp;rdquo;
&lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-88.pdf">http://yann.lecun.com/exdb/publis/pdf/lecun-88.pdf&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;em>Incidentally, this is the first paper made me understand the connection between optimisation theory and deep learning.&lt;/em>&lt;/p></description></item></channel></rss>