<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><title>KKT conditions and Lagrange multipliers | DAXPY</title><link rel=stylesheet href=/css/main.min.893c9340c957513598581780adb1395d937fd4885954ab6c67536bd4c77301a6.css integrity="sha256-iTyTQMlXUTWYWBeArbE5XZN/1IhZVKtsZ1Nr1MdzAaY=" crossorigin=anonymous><script src=/js/main.23cd0c7d837263b9eaeb96ee2d9ccfa2969daa3fa00fa1c1fe8701a9b87251a1.js integrity="sha256-I80MfYNyY7nq65buLZzPopadqj+gD6HB/ocBqbhyUaE=" crossorigin=anonymous></script></head><body><header><nav><ul><li><a class=text_secondary href=https://daxpy.xyz/>DAXPY</a></li><li>|</li><li><a href=https://daxpy.xyz/posts/>Posts</a></li><li><a href=https://daxpy.xyz/notes/>Notes</a></li><li><a href=https://daxpy.xyz/links/>Links</a></li><li><a href=https://daxpy.xyz/stories/>Stories</a></li><li><a href=/about>About</a></li></ul></nav></header><main><section><article><h1 id=karush-kuhn-tucker-conditions>Karush-Kuhn-Tucker conditions</h1><p>A typical constrained optimisation problem is as follows.
$$\begin{aligned}
\min_{x\in\mathbb{R}^n}&amp;f(x)
\\
s.t.~h_i(x) &= 0
\\
g_j(x) &\leq 0
\end{aligned}$$</p><h2 id=karush-kuhn-tucker-conditions-1>Karush-Kuhn-Tucker conditions</h2><p>If the negative of the gradient (of $f$) has any component along an equality constraint $h(x)=0$, then we can take small steps along this surface to reduce $f(x)$.</p><p>Since $\nabla h(x)$, the gradient of the equality constraint is always perpendicular to the constraint surface $h(x)=0$, at optimum, $-\nabla f(x)$ should be either parallel or anti-parallel to $\nabla h(x)$
$$-\nabla f(x) = \mu \nabla h(x)$$
A similar argument can be made for inequality constraints. These form KKT conditions. So at an optimum point $x^\ast$ we have,
$$\begin{aligned}
h_i(x^\ast)&=0
&
g_j(x^\ast) &\leq 0
\\
\lambda_j g_j(x^\ast) &= 0
&
\lambda_j &\geq 0
\end{aligned}$$
$$\nabla f(x^\ast) +\sum_{i} \mu_i\nabla h(x^\ast) + \sum_j \lambda_j \nabla g_j(x^\ast)= 0$$
These are the KKT conditions for constrained optimisation.</p><h2 id=lagrange-multipliers>Lagrange multipliers</h2><p>The method of Lagrange multipliers relies on KKT conditions. For a constrained optimisation problem, we introduce a Lagrange function
$$\begin{aligned}
\mathcal{L}(x,\mu,\lambda) = f(x) + {\mu}^{T} h(x) + {\lambda}^{T} g(x)
\end{aligned}$$
<em>Stationary points are points where derivative of the function is zero.</em></p><p>The sationary points of Lagrange function satisfies all of the KKT conditions. Hence we can solve for $\nabla_x\mathcal{L} =0$ to find the optimum point of $f(x)$. $\nabla_\mu \mathcal{L}=0$ and $\nabla_\lambda \mathcal{L} =0$ gives us the constraints.</p></article><br><br><br><i>tags:</i>
<a class=gray style=padding-right:5px href=/tags#optimisation><i>#optimisation</i></a></section></main><footer><p>Found something useful here? pass it on; or tell me about it!<nav><ul><li><a href="https://drive.google.com/drive/folders/18LU6PKnxy8rDrAXhpiPXV2yOYlO_I8cF?usp=sharing">Resume</a></li><li><a href=https://daxpy.xyz/omnia_mea>Coding notes</a></li><li><a href=https://www.linkedin.com/in/ndivakar/>Linkedin</a></li></ul></nav><small><a href=#top>â†‘ Top of page</a></small></p></footer></body></html>