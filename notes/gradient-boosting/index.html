<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><title>Gradient Boosting | DAXPY</title><link rel=stylesheet href=/css/main.min.893c9340c957513598581780adb1395d937fd4885954ab6c67536bd4c77301a6.css integrity="sha256-iTyTQMlXUTWYWBeArbE5XZN/1IhZVKtsZ1Nr1MdzAaY=" crossorigin=anonymous><script src=/js/main.23cd0c7d837263b9eaeb96ee2d9ccfa2969daa3fa00fa1c1fe8701a9b87251a1.js integrity="sha256-I80MfYNyY7nq65buLZzPopadqj+gD6HB/ocBqbhyUaE=" crossorigin=anonymous></script></head><body><header><nav><ul><li><a class=text_secondary href=https://daxpy.xyz/>DAXPY</a></li><li>|</li><li><a href=https://daxpy.xyz/posts/>Posts</a></li><li><a href=https://daxpy.xyz/notes/>Notes</a></li><li><a href=https://daxpy.xyz/links/>Links</a></li><li><a href=https://daxpy.xyz/stories/>Stories</a></li><li><a href=/about>About</a></li></ul></nav></header><main><section><article><h1 id=gradient-boosting>Gradient Boosting</h1><p>The general framework of Boosting is learners are added in greedy manner to minimise loss.
$$F_t(x) = F_{t-1}(x) + f_t(x)$$
At the $t^{th}$ step, we are interested in learning the function $f$ which minimised the loss. The value of loss function at this point is given by</p><p>$$\begin{aligned}
L &= l(y,p+f(x))
\\
&=l(y,p) + \nabla_{p} l(y,p)f(x)
\end{aligned}$$</p><p>The last step is from first order taylor series approximation of $l$.
$$f(x) = f(a) + (x-a) f^{\prime}(a)$$</p><p>We have trying find $f$ which minimises the loss $L$. So, we should move in negative gradient direction in function space.
\begin{align*}
F_t & \gets F_{t-1}-\gamma_t \frac{\partial L}{\partial f}
\\
\frac{\partial L}{\partial f}&= \nabla_{p} l(y,p)=-r
\end{align*}</p><p>Following this, all we need to do is at step $t$, we find the learner which best approximates the pseudo-residuals $r$</p><p><strong>Algorithm</strong>: Gradient Boosting</p><ul><li>[<strong>input</strong>] Dataset $D=\{(x_i,y_i)\}$, loss function $L$</li><li>[<strong>initialise</strong>] the model with a constant (typically mean $\mathbb{E} y_i$)
$$F_0(x) = \operatorname*{arg\,min}_{\gamma}\sum_i L(y_i,\gamma)$$</li><li>For $t \in 1 \ldots T$<ul><li>[<strong>Compute pseudo residuals</strong>]
$$r_{it} = -\frac{\partial L(y_i,F_{t-1}(x_i))}{\partial F_{t-1}(x_i)}$$</li><li>[<strong>Fit a weak learner on residuals</strong>]
$$f_t \leftrightsquigarrow \{(x_i,r_{it})\}$$</li><li>[<strong>Compute multiplier</strong>] by solving the 1D optimisation problem
$$\gamma_t = \operatorname*{arg,min}_{\gamma}\, L(y,F_{t-1}(x)+\gamma f_t(x))$$</li><li>[<strong>Update model</strong>]
$$F_t = F_{t-1}+\gamma_t f_t$$</li></ul></li></ul></article><br><br><br><i>tags:</i>
<a class=gray style=padding-right:5px href=/tags#machine-learning><i>#machine-learning</i></a></section></main><footer><p>Found something useful here? pass it on; or tell me about it!<nav><ul><li><a href="https://drive.google.com/drive/folders/18LU6PKnxy8rDrAXhpiPXV2yOYlO_I8cF?usp=sharing">Resume</a></li><li><a href=https://daxpy.xyz/omnia_mea>Coding notes</a></li><li><a href=https://www.linkedin.com/in/ndivakar/>Linkedin</a></li></ul></nav><small><a href=#top>â†‘ Top of page</a></small></p></footer></body></html>