<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><title>Common Optimisers | DAXPY</title><link rel=stylesheet href=/css/main.min.09b566cf287e6fb5c0477b301eeb6144854b92718d35549d0fa0f150a67136eb.css integrity="sha256-CbVmzyh+b7XAR3swHuthRIVLknGNNVSdD6DxUKZxNus=" crossorigin=anonymous><script src=/js/main.23cd0c7d837263b9eaeb96ee2d9ccfa2969daa3fa00fa1c1fe8701a9b87251a1.js integrity="sha256-I80MfYNyY7nq65buLZzPopadqj+gD6HB/ocBqbhyUaE=" crossorigin=anonymous></script></head><body><header><nav><ul><li><a class=text_secondary href=https://daxpy.xyz/>DAXPY</a></li><li>|</li><li><a href=https://daxpy.xyz/posts/>Posts</a></li><li><a href=https://daxpy.xyz/notes/>Notes</a></li><li><a href=https://daxpy.xyz/links/>Links</a></li><li><a href=https://daxpy.xyz/stories/>Stories</a></li><li><a href=/about>About</a></li></ul></nav></header><main><section><article><h1 id=common-optimisers>Common Optimisers</h1><p>$E$ is the loss function and $w$ is the model parameters;</p><h2 id=stochastic-gradient-descent>Stochastic Gradient Descent</h2><p>$$\begin{aligned}
w_{t+1}&= w_t - \alpha \nabla E(w_t)\end{aligned}$$</p><h2 id=sgd-with-momentum>SGD with Momentum</h2><p><em>Use gradient to update velocity/direction of a particle instead of only updating its position</em></p><p>$$\begin{aligned}
m_{t+1} &= \eta m_t + \alpha \nabla E(w_t)
\\
w_{t+1}&= w_t - m_{t+1}
\end{aligned}$$
This results in equivalent single update as
$$w_{t+1}= w_t - \alpha \nabla E(w_t) - \eta m_{t}$$</p><p>$\eta$ is the exponential decay factor in $[0,1]$ which determines
contribution of previous gradients to the weight change.</p><h2 id=nesterov-accelerated-gradient>Nesterov Accelerated Gradient</h2><p>The observation behind nesterov momentum is that we will update the
parameters by the momentum term anyway, why not calculate gradient at
the updated step instead?</p><p>$$\begin{aligned}
m_{t+1} &= \eta m_t + \alpha \boldsymbol{\nabla E(w_t-\eta m_t)}
\\
w_{t+1}&= w_t - m_{t+1}
\end{aligned}$$</p><p>Since we calculate the gradient at the new location, if there is a difference in direction, the
update will be able to correct for the difference. It increases responsiveness of the optimiser.</p><h2 id=rmsprop>RMSProp</h2><p>Root Mean Square Propagation. Divide learning rate of each weight by
running average of magnitudes of recent gradients for that weight.
$$\begin{aligned}
v_{t+1} &= \gamma v_{t} + (1-\gamma)(\nabla E(w_t))^2
\\
w_{t+1}&=w_{t} - \frac{\alpha}{\sqrt{v_{t+1}}} \nabla E(w_t)
\end{aligned}$$</p><p><em>Note</em>: $\nabla E(w_t)^2=|\nabla E(w_t)|_F^2$</p><h2 id=adam>Adam</h2><p>Adaptive Moment Estimation is an update to RMSProp. It uses a running
average for the gradient as well
$$\begin{aligned}
v_{t+1} &= \beta_1 v_{t} + (1-\beta_1)(\nabla E(w_t))^2
\\
m_{t+1} &= \beta_2 m_{t} + (1-\beta_2)\nabla E(w_t)
\\
m &= \frac{m_{t+1}}{1-\beta_2} \quad v = \frac{v_{t+1}}{1-\beta_1}
\\
w_{t+1}&=w_{t} - \alpha \frac{m}{\sqrt{v}+\epsilon}
\end{aligned}$$</p></article><br><br><br><i>tags:</i>
<a class=gray style=padding-right:5px href=/tags#machine-learning><i>#machine-learning</i></a>
<a class=gray style=padding-right:5px href=/tags#deep-learning><i>#deep-learning</i></a></section></main><footer><p>Found something useful here? pass it on; or tell me about it!<nav><ul><li><a href="https://drive.google.com/drive/folders/18LU6PKnxy8rDrAXhpiPXV2yOYlO_I8cF?usp=sharing">Resume</a></li><li><a href=https://daxpy.xyz/omnia_mea>Coding notes</a></li><li><a href=https://www.linkedin.com/in/ndivakar/>Linkedin</a></li></ul></nav><small><a href=#top>â†‘ Top of page</a></small></p></footer></body></html>