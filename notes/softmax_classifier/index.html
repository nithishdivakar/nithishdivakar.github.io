<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><title>Softmax Classifier | DAXPY</title><link rel=stylesheet href=/css/main.min.893c9340c957513598581780adb1395d937fd4885954ab6c67536bd4c77301a6.css integrity="sha256-iTyTQMlXUTWYWBeArbE5XZN/1IhZVKtsZ1Nr1MdzAaY=" crossorigin=anonymous><script src=/js/main.23cd0c7d837263b9eaeb96ee2d9ccfa2969daa3fa00fa1c1fe8701a9b87251a1.js integrity="sha256-I80MfYNyY7nq65buLZzPopadqj+gD6HB/ocBqbhyUaE=" crossorigin=anonymous></script></head><body><header><nav><ul><li><a class=text_secondary href=https://daxpy.xyz/>DAXPY</a></li><li>|</li><li><a href=https://daxpy.xyz/posts/>Posts</a></li><li><a href=https://daxpy.xyz/notes/>Notes</a></li><li><a href=https://daxpy.xyz/links/>Links</a></li><li><a href=https://daxpy.xyz/stories/>Stories</a></li><li><a href=/about>About</a></li></ul></nav></header><main><section><article><h1 id=softmax-classifier>Softmax Classifier</h1><p>Imagine we have a dataset $\{x,y\}_{i=0}^m$ where $x$ is a data point
and $y$ indicates the class $x$ belongs to. For deriving LDA classifier,
we had modeled the class conditional density $P(x|y)$ as a Gaussian and
derived the posterior probabilities $P(y|x)$. Here, we will directly
model the posterior with a linear function. Since the posterior directly
models what class a data point belongs to, we don&rsquo;t have much to do
after to get a classifier.</p><p>But modeling $P(y|x)$ with only a linear projection $w^Tx$ has some
problems. There is no easy way to restrict $w^Tx$ to always fall in
$[0,1]$ nor assure that $\sum_k P(y=k|x) = 1$.</p><p>We want a projection of the data such that it forms a clear probability
distribution over the classes. Since this is near impossible with only a
linear function, we stack a non parametric transformation following the
linear transformation.</p><p><strong>Softmax</strong> is a vector valued function defined over a sequence $(z_k)$
as</p><p>$$\operatorname{softmax}(z)_k = \frac{\operatorname{exp}[z_k]}{\sum_j\operatorname{exp}[z_j]}$$</p><p>Softmax preserves the relative magnitude of its input i.e the larger
input coordinate get the larger output value. Softmax also squashes the
values to lie in range $[0,1]$ and makes their sum equal to $1$.</p><p>$$\sum_k \frac{\operatorname{exp}[z_k]}{\sum_j\operatorname{exp}[z_j]} = \frac{\sum_k \operatorname{exp}[z_k]}{\sum_j\operatorname{exp}[z_j]} = 1$$</p><p>So our classifier is</p><p>$$P(y|x) = \operatorname{softmax}(w^Tx)$$</p><h2 id=derivative-of-the-softmax-function class=unnumbered>Derivative of the Softmax function</h2><p>We will need the derivative of softmax function later on. So let&rsquo;s
figure out what it is. We can begin by writing softmax in a concise
form. $$s_k = \frac{e_k}{\Sigma}$$ where $e_k = \operatorname{exp}[z_k]$
and $\Sigma = \sum_j\operatorname{exp}[z_j]$. With
$\frac{\partial e_k}{\partial z_k} = e_k$ and
$\frac{\partial \Sigma}{\partial z_p} = e_p$, we can easily derive the
derivative for softmax function as follows.
$$\cfrac{\partial s_k}{\partial x_p} = \begin{cases}
e_k\left[ \frac{-1}{\Sigma^2} e_p\right] = -s_ks_p & p \neq k
\\
\cfrac{ e_k \Sigma- e_p e_k}{\Sigma^2} = s_k-s_ps_k & p=k
\end{cases}$$</p><p>in general
$$\frac{\partial s_k}{\partial x_p} = s_k(\delta_{kp} - s_p)$$
$\delta_{kp}$ is dirac delta function which is $1$ only when $k=p$ and
$0$ otherwise.</p><h2 id=estimating-model-parameter-using-likelihood class=unnumbered>Estimating Model Parameter using Likelihood</h2><p>Now that we have a complete model of the classifier,
$P(y|x) = \operatorname{softmax}(w^Tx)$, all that is remaining is to
estimate the model&rsquo;s parameters $w$ from the dataset. We can begin by
using likelihood of the model explaining the training data.
$$L(w) = \prod_x \prod_k P(k|x;w)^{y_k} = \prod_x \prod_k \operatorname{softmax}(w_k^Tx)^{y_k}$$
Likelihood gives a measure of how much the model explains the data
${x,y}$ for a given parameter $w$. To get the optimum value for the
parameter, all we have to do find the value of $w$ which maximizes the
likelihood.</p><p>The likelihood function is a bit difficult to work with on its own. But
we take negative of the log of likelihood function so that all products
gets converted to sum and all exponentials gets converted to products.
$$E(w) = -\log L(w) = -\sum_x \sum_k y_k \log s_k$$ All we have to do to
get the best parameter is to minimize the negative log likelihood
(thereby maximizing likelihood)
$$w_{opt} = \operatorname{argmin}_w E(w)$$ <strong>Note</strong>: If we think of $y$
as a probability distribution of data belonging to each of the classes
and $s$ as our model&rsquo;s prediction of the same, then $E(w)$ is cross
entropy between the two distributions.</p><p>For computing $w_{opt}$, we simply have to find derivative of $E(w)$
w.r.to $w$ and equate it to 0. But finding derivative over then entire
$w$ is difficult and non-intuitive. So let&rsquo;s break it down and find
derivatives over each the columns $w_p$ separately.
$$ \nabla_{w_p} E(w) = -\sum_x \sum_k y_k \frac{1}{s_k} \frac{\partial s_k}{\partial w_p} $$
$$ = -\sum_x \sum_k y_k \frac{1}{s_k} \frac{\partial s_k}{\partial z_p} \frac{\partial z_p}{\partial w_p} $$
$$ = -\sum_x \sum_k y_k \frac{1}{s_k} s_k(\delta_{kp} - s_p) x $$
$$ = -\sum_x \sum_k y_k (\delta_{kp} - s_p) x $$</p><p>$\sum_k y_k (\delta_{kp} - s_p)$ can be expanded as
$\sum_k y_k \delta_{kp} - s_p\sum_k y_k$. Only $\delta_{pp}=1$ and
$\sum y_k =1$. Thus the original term evaluates to $y_p - s_p$.</p><p>$$\nabla_{w_p} E(w) = \sum_x (s_p - y_p) x$$
$$\nabla_{w} E(w) = \sum_x (s - y) x$$</p><p>Now we have a problem
here. Setting $\nabla_w E(w)=0$ does not give any information about $w$.
However, what the gradient do tells is that in space of $w$, which is
the direction to move (change $w$) so that the change in $E$ is the
largest. So if we move in the exact opposite direction (negative of
gradient), we will get maximum reduction in $E$. Since $E$ is measuring
the difference between model&rsquo;s prediction and the true labels,
decreasing $E$ would mean our model is getting better. Enter
**Gradient Descent**.</p><h2 id=gradient-descent-algorithm class=unnumbered>Gradient Descent Algorithm</h2><p>Gradient descent is exactly that; gradient-descent. If you want to
minimize a function, keep moving in negative direction of its gradient
(thereby descending).</p><p>Gradient descent is an optimization algorithm for cases where there is
no analytic or easy solution for the parameters, but gradients of the
models can be computed at each point. The algorithm simply says, if $L$
is some loss function which measures how good the model is with
parameter $\theta$, then we can update $\theta$ as to make the model
better. $$\theta_{new} = \theta - \alpha \nabla_\theta L$$ Now repeat
the same with new $\theta_{new}$ and the model with keep getting better.
There are some caveats however. The behaviour of gradient descent
entirely depends on choice of step size $\alpha$ and even then,
convergence to global optimum is not guaranteed.</p><p>In practise however, gradient descent performs well. We do have some
tricks to pick the (seemingly) best step size and some other ways to
ensure model improves. In our case, the update step is simply,
$$w_{new} = w - \alpha \nabla_{w}E(w)$$ To compute the true gradient
direction of the model, we need to evaluate the model over every
possible data. This is impossible because (1) intractable amount of
computation and (2) we don&rsquo;t have labels for all possible data (if we
did, what is the point of building a classifier).</p><p>So, we are computing an approximate model gradient over the data that we
have. But even this is a lot of work for getting an approximate gradient
and that is not end of the story. We have to keep iterating. Since we
are approximating, why not approximate further. Pick a sample at random,
compute gradient over it and update the model. This is Stochastic
Gradient Descent.</p><p>Stochastic Gradient Descent is not that stable. Gradient of each samples
do not agree with each other and hence, the frequent updates simply
results in random motion in the parameter space. To make more stable, we
compute gradient over a small set of samples or a batch. This is batched
stochastic gradient descent. Batched SGD updates the model more
frequently than pure gradient descent and is more stable than vanilla
SGD. In fact, batched SGD is so commonly used that now it has become the
vanilla. SGD now refers to batched SGD.</p><p>This is how you implement SGD.
$$ idx \gets 1 $$
$$ W = \text{initialise_weights()} $$
$$ x_b,y_b = \text{next(get batch)} $$
$$ s_b = model(x_b;W) $$
$$ g_b = \nabla_W s_b $$
$$ W \gets W - \alpha g_b $$</p><h2 id=implementing-softmax-classifier-and-its-gradients class=unnumbered>Implementing Softmax classifier and its gradients</h2><p>Implementing the forward prediction of the classifier is pretty straight
forward. First we have to do a matrix vector multiplication to implement
$z=w^Tx$ and then point-wise exponentiate all the terms in $z$.</p><p>However, we have to sum all of these exponentiated terms to get the
denominator in the softmax step. Since exponentiation creates huge
numbers if components of $z$ are greater than $1$, this creates some
numerical errors.</p><p>To get rid of the numerical errors we use the following trick.
$$\operatorname{softmax}(z)_k = \frac{e^{z_k}}{\sum_i e^{z_i}}= \frac{e^{-M}e^{z_k}}{e^{-M}\sum_i e^{z_i}}= \frac{e^{z_k-M}}{\sum_i e^{z_i-M}}$$
If we choose $M$ large enough such that all the terms in the powers are
negative or $0$, all the exponentiated terms will be small. So we set
$M = \max z_i$.</p><p>Following code sample shows how the model&rsquo;s prediction is implemented.
The code has been vectorized so that it can predict for a batch of $x$
at once.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_predictions</span>(x,W):
</span></span><span style=display:flex><span>  z <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>matmul(x,W<span style=color:#f92672>.</span>T)
</span></span><span style=display:flex><span>  M <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>max(
</span></span><span style=display:flex><span>        z,
</span></span><span style=display:flex><span>        axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>      )
</span></span><span style=display:flex><span>  e <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>exp(z<span style=color:#f92672>-</span>M)
</span></span><span style=display:flex><span>  <span style=color:#75715e># normalisation trick so</span>
</span></span><span style=display:flex><span>  <span style=color:#75715e># that largest of z is 0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  sigma <span style=color:#f92672>=</span> e<span style=color:#f92672>.</span>sum(
</span></span><span style=display:flex><span>            axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>            keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>          )
</span></span><span style=display:flex><span>  s <span style=color:#f92672>=</span> e<span style=color:#f92672>/</span>sigma
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> s
</span></span></code></pre></div><p>Unlike forward pass, implementing gradient is very simple. It&rsquo;s only an
outer product between two vectors $s-y$ and $x$. But, when we implement
it for a batch of samples and it&rsquo;s predictions, the outer product can be
implemented as a matrix multiplication. See the code sample below.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_batch_gradient</span>(
</span></span><span style=display:flex><span>  x_b, <span style=color:#75715e># input</span>
</span></span><span style=display:flex><span>  y_b, <span style=color:#75715e># target</span>
</span></span><span style=display:flex><span>  s_b  <span style=color:#75715e># prediction</span>
</span></span><span style=display:flex><span>):
</span></span><span style=display:flex><span>  g_b <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>matmul((s_b<span style=color:#f92672>-</span>y_b)<span style=color:#f92672>.</span>T, x_b)
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> g_b<span style=color:#f92672>/</span>batch_size
</span></span></code></pre></div><h2 id=model-performance class=unnumbered>Model Performance</h2><p>Cifar 10 is an image dataset having 10 image classes. In this section,
we test our simple softmax classifier&rsquo;s performance on it.</p><p>To tune the model to get optimum performance, we first need to find the
best hyper-parameters (batch size and learning rate). For this, we first
split the training set into two. A smaller set for validation and the
rest to be used solely for training. The model is trained only on this
new training set and validation set will be like our proxy test set
while we find the best hyper parameter.</p><p>So we train the model for relative shorter duration (10000 gradient
updates) and see its performance on validation set for different choices
of hyper parameters. The following table list model performances for
different hyper parameter combinations.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>               4      8      16     32
</span></span><span style=display:flex><span>  ---------- ------ ------ ------ ------
</span></span><span style=display:flex><span>  $10^{-1}$   0.20   0.16   0.28   0.31
</span></span><span style=display:flex><span>  $10^{-2}$   0.22   0.29   0.35   0.38
</span></span><span style=display:flex><span>  $10^{-3}$   0.33   0.34   0.36   0.36
</span></span><span style=display:flex><span>  $10^{-4}$   0.29   0.30   0.30   0.30
</span></span><span style=display:flex><span>  $10^{-5}$   0.18   0.19   0.18   0.19
</span></span><span style=display:flex><span>                                  
</span></span><span style=display:flex><span>               64    128    256    512
</span></span><span style=display:flex><span>  $10^{-1}$   0.26   0.29   0.22   0.28
</span></span><span style=display:flex><span>  $10^{-2}$   0.38   0.39   0.40   0.40
</span></span><span style=display:flex><span>  $10^{-3}$   0.37   0.37   0.37   0.37
</span></span><span style=display:flex><span>  $10^{-4}$   0.31   0.30   0.30   0.30
</span></span><span style=display:flex><span>  $10^{-5}$   0.19   0.19   0.19   0.17
</span></span></code></pre></div><p>Batch size $256$ and learning rate $0.01$ gave the best performance. So
we will train the model for longer duration(1000000 gradient updates)
with these parameters. The following plot shows validation loss during
training progress. We get a final test accuracy of 38.05</p><h2 id=conclusion class=unnumbered>Conclusion</h2><p>LDA model gave us 37.85% accuracy on Cifar 10 dataset. The softmax
classifier is giving us 38% accuracy. It appears to be a close tie
between both the models, but one important distinction is that LDA
distinctly modeled the data as gaussian while we made no such assumption
while designing the softmax classifier.</p><p>Our simple linear classifier appear useless when compared to bigger and
complex models(CNNs) that achieves near perfect accuracy on cifar 10.
But there is some values in learning these simple ones first. They do
teach some very valuable lessons about data modeling. They are also very
good to test implementing optimizer algorithms like SGD we have
implemented for this post. Do test them out on some other problems.</p><h2 id=code>Code</h2><p>The code is <a href=https://github.com/nithishdivakar/blog-post-codes/tree/master/softmax-classifiers>here</a>.</p></article><br><br><br><i>tags:</i>
<a class=gray style=padding-right:5px href=/tags#machine-learning><i>#machine-learning</i></a></section></main><footer><p>Found something useful here? pass it on; or tell me about it!<nav><ul><li><a href="https://drive.google.com/drive/folders/18LU6PKnxy8rDrAXhpiPXV2yOYlO_I8cF?usp=sharing">Resume</a></li><li><a href=https://daxpy.xyz/omnia_mea>Coding notes</a></li><li><a href=https://www.linkedin.com/in/ndivakar/>Linkedin</a></li></ul></nav><small><a href=#top>↑ Top of page</a></small></p></footer></body></html>