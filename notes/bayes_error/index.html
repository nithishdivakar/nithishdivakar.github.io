<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><title>Bayes Error | DAXPY</title><link rel=stylesheet href=/css/main.min.893c9340c957513598581780adb1395d937fd4885954ab6c67536bd4c77301a6.css integrity="sha256-iTyTQMlXUTWYWBeArbE5XZN/1IhZVKtsZ1Nr1MdzAaY=" crossorigin=anonymous><script src=/js/main.23cd0c7d837263b9eaeb96ee2d9ccfa2969daa3fa00fa1c1fe8701a9b87251a1.js integrity="sha256-I80MfYNyY7nq65buLZzPopadqj+gD6HB/ocBqbhyUaE=" crossorigin=anonymous></script></head><body><header><nav><ul><li><a class=text_secondary href=https://daxpy.xyz/>DAXPY</a></li><li>|</li><li><a href=https://daxpy.xyz/posts/>Posts</a></li><li><a href=https://daxpy.xyz/notes/>Notes</a></li><li><a href=https://daxpy.xyz/links/>Links</a></li><li><a href=https://daxpy.xyz/stories/>Stories</a></li><li><a href=/about>About</a></li></ul></nav></header><main><section><article><h1 id=bayes-error>Bayes Error</h1><p>In an ideal world, everything has reason. Every question has a unambiguous answer. The data in sufficient to explain its behaviours, like the class it belongs to.</p><p>$$\begin{aligned}
g(x) = y \end{aligned}$$</p><p>In the non ideal world, however, there is always something missing that stops us from knowing the entire truth. $g$ is beyond reach. In such cases we resort to probability.</p><p>$$\begin{aligned}
n(x) = P(y=1|x)\end{aligned}$$</p><p>It simply tells us how probable is the data belonging to a class($y=1$) if my observations are $x$.</p><p><em>If we build a classifier on this data, how good will it be?</em> This is the question Bayes error answers.</p><p>Lets say I&rsquo;ve built a classifier $h$ to predict the class of data. $h(x)=\hat{y}$ is the predicted class and $y$ is the true class. Even ambiguous data needs to come from somewhere, So we assume $D$ is the joint distribution of $x$ and $y$.</p><p>$$\begin{aligned}
er_D[h] = P_D[h(x) \neq y]\end{aligned}$$</p><p>Using an old trick to convert probability to expectation, $P[A] = E[1(A)]$, we have</p><p>$$\begin{aligned}
er_D[h] = E_{x,y}[1(h(x)\neq y)] = E_x E_{y|x}[1(h(x)\neq y)]\end{aligned}$$</p><p>The inner expectation is easier to solve when expanded.</p><p>$$\begin{aligned}
E_{y|x}[1(h(x)\neq y)] = 1(h(x)\neq +1) P(y=+1|x) + 1(h(x)\neq -1)P(y=-1|x)\end{aligned}$$</p><p>Which give the final error to be</p><p>$$\begin{aligned}
er_D[h] = E_x[1(h(x)\neq +1) n(x) + 1(h(x)\neq -1)(1-n(x))]\end{aligned}$$</p><p>The last equation means, if the classifier predicts $+1$ for the data, it will contribute $n(x)$ to the error. On the other hand if it predicts $-1$ for the data, the contribution will be $1-n(x)$.</p><p>The best classifier would predict $+1$ when $n(x)$ is small and $-1$ when $n(x)$ is large. The minimum achievable error is then</p><p>$$\begin{aligned}
er_D = E_x [\min(n(x),1-n(x))]\end{aligned}$$</p><p>This error is called <strong>Bayes Error</strong>.</p><h2 id=references>References</h2><ul><li><a href=http://drona.csa.iisc.ernet.in/~e0270/Jan-2015/>Shivani Agarwal&rsquo;s lectures</a></li></ul></article><br><br><br><i>tags:</i>
<a class=gray style=padding-right:5px href=/tags#machine-learning><i>#machine-learning</i></a></section></main><footer><p>Found something useful here? pass it on; or tell me about it!<nav><ul><li><a href="https://drive.google.com/drive/folders/18LU6PKnxy8rDrAXhpiPXV2yOYlO_I8cF?usp=sharing">Resume</a></li><li><a href=https://daxpy.xyz/omnia_mea>Coding notes</a></li><li><a href=https://www.linkedin.com/in/ndivakar/>Linkedin</a></li></ul></nav><small><a href=#top>â†‘ Top of page</a></small></p></footer></body></html>