<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><title>BFGS | DAXPY</title><link rel=stylesheet href=/css/main.min.d700ad2998f190f21436ef39c94129166ea316da7e0014005c9c2ced3b288a04.css integrity="sha256-1wCtKZjxkPIUNu85yUEpFm6jFtp+ABQAXJws7TsoigQ=" crossorigin=anonymous><script src=/js/main.23cd0c7d837263b9eaeb96ee2d9ccfa2969daa3fa00fa1c1fe8701a9b87251a1.js integrity="sha256-I80MfYNyY7nq65buLZzPopadqj+gD6HB/ocBqbhyUaE=" crossorigin=anonymous></script></head><body><header><nav><ul><li><a class=text_secondary href=https://daxpy.xyz/>DAXPY</a></li><li>|</li><li><a href=https://daxpy.xyz/posts/>Posts</a></li><li><a href=https://daxpy.xyz/notes/>Notes</a></li><li><a href=https://daxpy.xyz/collections/>Collections</a></li><li><a href=https://daxpy.xyz/links/>Links</a></li><li><a href=https://daxpy.xyz/stories/>Stories</a></li><li><a href=/about>About</a></li></ul></nav></header><main><section><div style=text-align:right;height:5px;margin-top:10px class=italic>this page
⟩ <a href=/notes/>notes</a>
⟩ <a href=/>home</a></div><article><h1 id=bfgs>BFGS</h1><h2 id=newtons-method>Newton&rsquo;s Method</h2><p>$$\begin{aligned}
x_{k+1} &= x_k - [H(x_k)]^{-1}\nabla f(x_k)^\intercal
\end{aligned}$$</p><h2 id=quasi-newtons-method>Quasi Newton&rsquo;s Method</h2><p>$$\begin{aligned}
x_{k+1} &= x_k - \alpha_kS_k {\nabla f(x_k)}^{T}
\end{aligned}$$</p><p>If $S_k$ is inverse of Hessian, then method is Newton&rsquo;s iteration; if $S_k=I$, then it is steepest descent</p><h2 id=bfgs-1>BFGS</h2><p>BFGS is a quasi newtons method where we approximate inverse of Hessian by $B_k$. The search direction $p_k$ is determined by solving
$$B_kp_k = -\nabla f(x_k)$$
A line search is performed in this search direction to find next point $x_{k+1}$ by minimising $f(x_k+\gamma p_k)$. The approximation to hessian is then updated as
$$\begin{aligned}
B_{k+1} &= B_k + \alpha_k u_ku_k^\intercal + \beta_k v_kv_k^\intercal
\\
u_k &= \nabla f(x_{k+1})-\nabla f(x_k)
\\
\alpha_k &= \frac{1}{\alpha u_k^\intercal p_k}
\\
v_k &= B_kp_k
\\
\beta_k &= \frac{-1}{p_k^\intercal B_kp_k}
\end{aligned}$$</p></article><br><br><br><i>tags:</i>
<a class=gray style=padding-right:5px href=/tags#optimisation><i>#optimisation</i></a></section></main><footer><p>Found something useful here? pass it on; or tell me about it!<nav><ul><li><a href="https://drive.google.com/drive/folders/18LU6PKnxy8rDrAXhpiPXV2yOYlO_I8cF?usp=sharing">Resume</a></li><li><a href=https://daxpy.xyz/collections/>Collections</a></li><li><a href=https://www.linkedin.com/in/ndivakar/>Linkedin</a></li></ul></nav><small><a href=#top>↑ Top of page</a></small></p></footer></body></html>