<!doctype html><html style=height:100%><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://daxpy.xyz/favicon.ico><link rel=stylesheet href=/css/style.min.css><link href="//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic" rel=stylesheet type=text/css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><title>RankNet and LambdaRank</title></head><body style=height:100%><div class=header><header id=banner style=display:flex;flex-direction:row;flex-wrap:nowrap;justify-content:flex-start;align-items:baseline><big class=h2 style="flex:0 0 80px"><b><a href=https://daxpy.xyz/ class=black>daxpy</a></b></big><nav style=flex:0px><ul style="display:flex;flex-direction:row;gap:1rem;justify-content:safe flex-end"><li><a href=https://daxpy.xyz/posts/>Posts</a></li><li><a href=https://daxpy.xyz/notes/>Notes</a></li><li><a href=https://daxpy.xyz/links/>Links</a></li><li><a href=https://daxpy.xyz/stories/>Stories</a></li><li><a href=/about>about</a></li></ul></nav></header><hr style=margin:0></div><div class="container justify" style=display:flex;flex-direction:column;min-height:100vh><div class=main style=flex:1><main id=content><section><article><h1 id=ranknet-and-lambdarank>RankNet and LambdaRank</h1><p>The ranking problem is about ordering a collection of documents according to their relevance to the given query.</p><p>Their are multiple approaches to the problem, but in pairwise approach, we simply care about predicting order of document pairs for the query. Given 2 documents $d_i$ and $d_j$ the true relative ordering is specified as
$$h_{ij} = \begin{cases}1& d_i>d_j\\0& d_i=d_j\\-1& d_i&lt;d_j\\\end{cases}$$</p><p>In terms of modelling, we assume there is a base model takes in features $x_i$ corresponds to document $d_i$ and predict a score depicting relevance to the given query. $$s_i = f(x_i)$$</p><p>A comparator model is feed these scores which predicts
$\mathop{\mathrm{\operatorname{P}}}(d_i>d_j)$.</p><p>The comparator model can be a binary classifier by setting
$$y_{ij} \mathop{\mathrm{\triangleq}}\frac{1+h_{ij}}{2}$$ as target variable. The a comparator is trained to predict 1 for verifying the order and 0 for negating it.</p><h2 id=ranknet>RankNet</h2><p>RankNet uses a logistic regression as comparator which is feed the difference of scores.
$$\hat{y}_{ij} = \mathop{\mathrm{\operatorname{P}}}(d_i>d_j) = \frac{1}{1+e^{-\alpha(s_i-s_j)}}$$
$\alpha$ is a parameter which controls the slope of sigmoid function.</p><p>We can define binary cross entropy loss on this model as</p><p>$$C_{ij} = -(y_{ij}\log \hat{y}_{ij}+(1-y_{ij})\log (1- \hat{y}_{ij}))$$</p><p>Now consider a mini-batch of document ${d_1,\ldots,d_n}$ corresponding to a particular query. The documents have some perfect ordering to answer the query which is specified by values of $h_{ij}$.</p><p>For gradient update, we are interested in computing the gradients generated by each documents.
$$\begin{aligned}
\frac{\partial C}{\partial w} &= \frac{1}{n} \sum_{i=1}^{n} \frac{\partial C_i}{\partial w}
\\
w&\gets w-\eta \frac{\partial C}{\partial w}
\end{aligned}$$</p><p>Now, if we assume the documents follow the order $d_b>d_i>d_a$, then the loss incurred by the $d_i$ can be written as</p><p>$$\begin{aligned}
C_i &= -\sum_{a: d_i>d_a}y_{ia}\log \hat{y}_{ia} - \sum_{b:d_b>d_i}(1-y_{ib})\log (1-\hat{y}_{ib})
\end{aligned}$$</p><p>Note that the ground truth labels $y_{ia}=1$ and $y_{ib}=0$. The gradient from $d_i$ can also be simplified as</p><p>$$\begin{aligned}
\frac{\partial C_i}{\partial w}&=\sum_{a}\frac{\partial C_i}{\partial s_a}\frac{\partial s_a}{\partial w}+ \sum_{b}\frac{\partial C_i}{\partial s_b}\frac{\partial s_b}{\partial w}
\end{aligned}$$</p><p>The ${\partial s_\square}/{\partial w}$ part of the gradient only depends on the score prediction network. For computing the gradient of the comparator, we have
$$\begin{aligned}
\frac{\partial C_i}{\partial s_a}&=-\frac{\partial \log \hat{y}_{ia}}{\partial s_a} = \frac{-\alpha}{1+e^{\alpha(s_i-s_a)}}=-\alpha(1-\hat{y}_{ia})
\\
\frac{\partial C_i}{\partial s_b}&=-\frac{\partial \log (1- \hat{y}_{ib})}{\partial s_b} = \frac{-\alpha}{1+e^{-\alpha(s_i-s_b)}}=-\alpha\hat{y}_{ib}
\end{aligned}$$</p><p>If we randomly select a document pair $(d_i,d_j)$, the gradients would
be</p><p>$$\begin{aligned}
\frac{\partial C_i}{\partial s_j} &=
\begin{cases} \alpha(\hat{y}_{ij}-1) &amp;d_i>d_j~or~h_{ij}=1
\\
-\alpha\hat{y}_{ij}&amp;d_j>d_i~or~h_{ij}=-1\end{cases}
\end{aligned}$$</p><p>Now if we define the quantities
$$\lambda_{ij} \mathop{\mathrm{\triangleq}}\alpha\left[\frac{(1-h_{ij})}{2}-(1-\hat{y}_{ij})\right]$$
we can write the individual gradient as $$\begin{aligned}
\frac{\partial C_i}{\partial w}&= \sum_a\lambda_{ia}\frac{\partial s_a}{\partial w} - \sum_b\lambda_{ib}\frac{\partial s_b}{\partial w}
\\
\frac{\partial C}{\partial w}&= \frac{1}{n}\sum_{i=1}^{n}\lambda_{i}\frac{\partial s_i}{\partial w}
\\
\lambda_i &= \sum_{d_i > d_j} \lambda_{ij} - \sum_{d_i &lt; d_j} \lambda_{ij}
\end{aligned}$$</p><p>So for each document in the batch, we can simply accumulate $\lambda$ and then apply it to the gradient thus not requiring $n^2$ gradient computation.</p><p>Each $\lambda_i$ can also be thought of as the strength of gradient, getting larger for every inversions in the the ordering and getting smaller for correct orderings.</p><h2 id=lambdarank>LambdaRank</h2><p>At this point explaining lambda rank is very simple. Its exactly same as RankNet, but we modify computation of $\lambda$&rsquo;s as follows.
$$\lambda_{ij} \mathop{\mathrm{\triangleq}}-\alpha(1-\hat{y}_{ij})|\Delta_{NDCG}|$$</p><p>$\Delta_{NDCG}$ is the change in $NDCG$ measure if we swap $d_i$ and $d_j$ in the ordering. This results in gradient updates optimising for NDCG measure. Since in terms of NDCG, higher is better, we have to do
gradient ascent instead of gradient descent
$$w \gets w + \eta \left(\frac{1}{n}\sum_{i=1}^{n}\lambda_{i}\frac{\partial s_i}{\partial w} \right)$$</p></article><br><br><br><i>tags:</i>
<a class=gray style=padding-right:5px href=/tags#ranking><i>#ranking</i></a>
<a class=gray style=padding-right:5px href=/tags#search><i>#search</i></a></section></main></div><div class=footer><br><br><hr style=margin:0><footer id=footer style=display:flex;justify-content:space-between><div>Ping me <a target=_blank class=green href=https://twitter.com/nithishdivakar>@nithishdivakar</a></div><div><a href=/resources target=_blank>::</a></div></footer></div></div></body></html>