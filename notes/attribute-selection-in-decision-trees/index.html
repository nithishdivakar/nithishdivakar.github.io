<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><title>Attribute Selection in Decision Trees | DAXPY</title><link rel=stylesheet href=/css/main.min.893c9340c957513598581780adb1395d937fd4885954ab6c67536bd4c77301a6.css integrity="sha256-iTyTQMlXUTWYWBeArbE5XZN/1IhZVKtsZ1Nr1MdzAaY=" crossorigin=anonymous><script src=/js/main.23cd0c7d837263b9eaeb96ee2d9ccfa2969daa3fa00fa1c1fe8701a9b87251a1.js integrity="sha256-I80MfYNyY7nq65buLZzPopadqj+gD6HB/ocBqbhyUaE=" crossorigin=anonymous></script></head><body><header><nav><ul><li><a class=text_secondary href=https://daxpy.xyz/>DAXPY</a></li><li>|</li><li><a href=https://daxpy.xyz/posts/>Posts</a></li><li><a href=https://daxpy.xyz/notes/>Notes</a></li><li><a href=https://daxpy.xyz/links/>Links</a></li><li><a href=https://daxpy.xyz/stories/>Stories</a></li><li><a href=/about>About</a></li></ul></nav></header><main><section><article><h1 id=attribute-selection-in-decision-trees>Attribute Selection in Decision Trees</h1><p>For constructing a new node in decision tree, choosing which attribute to partition the data on is important. Choosing a less desirable attribute to split the data on may result in lower performance. Lets look into a few important measures which helps us find the best attribute.</p><h2 id=information-gain>Information Gain</h2><p>Information Gain is defined as amount of information gained about a random variable (outcome) from observing another (attribute).
We can quantify information gain as difference in entropy when random variable is observed.</p><p>$$\begin{aligned}
IG(T,A) &= H(T) - H(T|A)
\\
H(T) &= -\sum_{c\in C}^{}p_c\log_2 p_c
\\
H(T|A)
&= \sum_{a
\in A}p_a H(T_a)
\end{aligned}$$
Here $H(T)$ is the entropy of set $T$ and $T_a = \{t\in T: t_{A} = a\}$ is its subset of items with attribute $A=a$. Also, $p_a = \frac{\left|T_a\right|}{|T|}$.</p><h2 id=gini-impurity>GINI Impurity</h2><p>GINI Impurity is a measure of how often a randomly chosen element from a set would be incorrectly labeled if it was randomly labeled according to the distribution of an attribute in the set.</p><p>Let say we partition the input set $T$ according to the values of attribute $A$ such that $T = \bigcup_{a\in A} T_a$.
The split would be ideal if each of the partitions would have only a single class (different subset can have same class).</p><p>GINI impurity quantifies having multiple classes in same partition.
$$\begin{aligned}
G(T_a) &=
\sum_{c\in C}p_{a,c}\left(\textstyle\sum_{k \neq c} p_{a,k}\right)
\\
&=\sum_{c\in C}p_{a,c}(1-p_{a,c})
\\
&= 1 - \sum_{c\in C}p_{a,c}^2
\end{aligned}$$</p><p>Overall GINI Impurity score of partitioning $T$ according to $A$ is
$$\begin{aligned}
G(A) &=
\sum_{a\in A}p_{a}G(T_a) \quad
\\
&= \sum_{a\in A} p_{a} \left(1- \sum_{c\in C}p_{a,c}^{2}\right)
\end{aligned}$$</p><p>$p_a$ fraction of elements which has attribute $a$
$p_{a,c}$ fraction of elements in class $c$ and has attribute $a$</p><h2 id=variance-reduction>Variance Reduction</h2><p>Variance reduction is used when target variable is continuous (tree is a regression tree). If the set $T$ is being partitioned into $T_L$ and $T_R$, the reduction in variance is given by
$$\begin{aligned}
V(T) = Var(T) &- \left(\frac{|T_L|}{|T|}Var(T_L)+\frac{|T_R|}{|T|}Var(T_R)\right)<br>\end{aligned}$$
For calculating the best split point, the standard variance calculation formula would require recalculation of mean repeatedly. But we can compute variance without explicitly calculating mean as
$$Var(S) = \frac{1}{|S|^2}\sum_{i,j\in S}(y_i-y_j)^2 $$</p></article><br><br><br><i>tags:</i>
<a class=gray style=padding-right:5px href=/tags#machine-learning><i>#machine-learning</i></a></section></main><footer><p>Found something useful here? pass it on; or tell me about it!<nav><ul><li><a href="https://drive.google.com/drive/folders/18LU6PKnxy8rDrAXhpiPXV2yOYlO_I8cF?usp=sharing">Resume</a></li><li><a href=https://daxpy.xyz/omnia_mea>Coding notes</a></li><li><a href=https://www.linkedin.com/in/ndivakar/>Linkedin</a></li></ul></nav><small><a href=#top>â†‘ Top of page</a></small></p></footer></body></html>