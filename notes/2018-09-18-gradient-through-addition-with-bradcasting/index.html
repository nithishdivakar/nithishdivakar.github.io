<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><title>Gradient Through Addition with Broadcasting | DAXPY</title><link rel=stylesheet href=/css/main.min.893c9340c957513598581780adb1395d937fd4885954ab6c67536bd4c77301a6.css integrity="sha256-iTyTQMlXUTWYWBeArbE5XZN/1IhZVKtsZ1Nr1MdzAaY=" crossorigin=anonymous><script src=/js/main.23cd0c7d837263b9eaeb96ee2d9ccfa2969daa3fa00fa1c1fe8701a9b87251a1.js integrity="sha256-I80MfYNyY7nq65buLZzPopadqj+gD6HB/ocBqbhyUaE=" crossorigin=anonymous></script></head><body><header><nav><ul><li><a class=text_secondary href=https://daxpy.xyz/>DAXPY</a></li><li>|</li><li><a href=https://daxpy.xyz/posts/>Posts</a></li><li><a href=https://daxpy.xyz/notes/>Notes</a></li><li><a href=https://daxpy.xyz/links/>Links</a></li><li><a href=https://daxpy.xyz/stories/>Stories</a></li><li><a href=/about>About</a></li></ul></nav></header><main><section><article><h1 id=gradient-through-addition-with-broadcasting>Gradient Through Addition with Broadcasting</h1><p>Calculating gradient across an addition is a simple
algebra trick. But this gets complicated when we look at addition of tensors which allows
broadcasting. In this post, we examine how to compute gradients in such situations.</p><h2 id=gradient-of-addition>Gradient of Addition</h2><p>Consider a simple sequence of operations. $A$ and $B$ are inputs which
ultimately leads to computation of a scalar loss/error term $l$.</p><p>$$ z = A+B $$
$$ l \twoheadleftarrow z $$</p><p>We are interested in the gradient of both the inputs
w.r.to $l$. Apply chain rule and we get</p><p>$$ \frac{\partial l}{\partial A} = \frac{\partial l}{\partial z} $$
$$ \frac{\partial l}{\partial B} = \frac{\partial l}{\partial z} $$</p><h2 id=addition-in-neural-networks class=unnumbered>Addition in Neural Networks</h2><p>Consider a simple feed forward layer with $x$ as input. We will ignore the activation function to keep things cimple. The transformation of the layer is given by</p><p>$$ z = W^Tx+b $$
$$ l \twoheadleftarrow z $$</p><p>The gradients of the parameters w.r.to the loss terms are
$$ \frac{\partial l}{\partial b} =\frac{\partial l}{\partial z} $$
$$ \frac{\partial l}{\partial W} = x \frac{\partial l}{\partial z}^T $$</p><p>$W^Tx$ is a vector which is of same size dimensions as $b$ if $x$ is
also a vector. But in practise, networks are always trained with
min-batches of samples at a time. Which makes $x$ a 2 dimensional tensor
and suddenly the quantities $W^Tx$ and $b$ have different dimensions.</p><h2 id=addition-with-broadcasting class=unnumbered>Addition with broadcasting</h2><p>An implicit assumption we make while adding two multi dimensional
quantities is that their dimensions always match. But numerical
frameworks allow addition even when the dimensions of the operands are
not the same. This is called addition with broadcasting.</p><p>Addition is allowed if the arrays are broadcast compatible with each
other. Numpy&rsquo;s docs describes two arrays are broadcast compatible if
their dimensions are compatible. Two dimensions are compatible if they
are</p><ol><li>Equal</li><li>One of them is 1</li></ol><p>When arrays do not have same number of dimensions, the arrays are
compatible if the smaller array&rsquo;s dimensions can be stretched to both
sides by simply adding dimensions of 1 and then the dimensions of both
arrays become compatible.</p><h2 id=gradient-through-broadcasting class=unnumbered>Gradient through Broadcasting</h2><p>Lets unwrap what actually happens during a broadcast operation. For
simplicity, lets say we are trying to add two tensors $A$ and $B$. $A$
and $B$ agree on all dimensions except the last where A has a dimension
of size $3$ while $B$ is dimensionless.</p><p>In this case, $B$ would be broadcast over $A$ to facilitate the addition
as follows.
$$ A+B = \begin{bmatrix} A_{:,0} & A_{:,1} & A_{:,2} \end{bmatrix}+B $$
$$ =\begin{bmatrix} A_{:,0}+B & A_{:,1}+B & A_{:,2}+B \end{bmatrix} $$</p><p>This allows us to visualise the computations better. For the following
computation,</p><p>$$ z = A+B $$
$$ l \dashleftarrow z $$</p><p>we know that $\frac{\partial l}{\partial A}$ remains
the same. But $\frac{\partial l}{\partial B}$ is for the tensor that was broadcasted. The gradient for original B needs to be
amplified by the factor of the broadcasting. Hence $$\frac{\partial l}{\partial B} = \mathbf{n}\frac{\partial l}{\partial z}$$ A curious case to note here that, the gradient of parameter $B$ has a dimension of $A$ in it and hence is dependent on size of $A$. This might be the reason for differing behaviour of neural networks when they are trained with different batch sizes.</p></article><br><br><br><i>tags:</i>
<a class=gray style=padding-right:5px href=/tags#computational-graph><i>#computational-graph</i></a></section></main><footer><p>Found something useful here? pass it on; or tell me about it!<nav><ul><li><a href="https://drive.google.com/drive/folders/18LU6PKnxy8rDrAXhpiPXV2yOYlO_I8cF?usp=sharing">Resume</a></li><li><a href=https://daxpy.xyz/omnia_mea>Coding notes</a></li><li><a href=https://www.linkedin.com/in/ndivakar/>Linkedin</a></li></ul></nav><small><a href=#top>â†‘ Top of page</a></small></p></footer></body></html>